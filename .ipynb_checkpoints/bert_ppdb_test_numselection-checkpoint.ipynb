{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertForMaskedLM\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PPDB import Ppdb\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def convert_sentence_to_token(sentence, seq_length, tokenizer):\n",
    "  \n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "\n",
    "    if len(tokenized_text) > seq_length - 2:\n",
    "        tokenized_text = tokenized_text[0:(seq_length - 2)]\n",
    "\n",
    "    position =[]\n",
    "    special =[]\n",
    "    isSpecial = False\n",
    "\n",
    "    whole_word = ''\n",
    "    words = []\n",
    "\n",
    "    start_pos =  len(tokenized_text)  + 2\n",
    "\n",
    "    connect_sign = 0\n",
    "    for index in range(len(tokenized_text)-1):\n",
    "        \n",
    "        if(tokenized_text[index+1]==\"-\" and tokenized_text[index+2]!=\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]==\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "\n",
    "            if(tokenized_text[index-1]==\"-\"):\n",
    "                words.append(whole_word)\n",
    "                position.append(special)\n",
    "                special = []\n",
    "                whole_word = ''\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]!=\"-\" and tokenized_text[index-1]==\"-\"):\n",
    "            whole_word += tokenized_text[index]\n",
    "            words.append(whole_word)\n",
    "            whole_word = ''\n",
    "            special.append(start_pos+index)\n",
    "            position.append(special)\n",
    "            special = []\n",
    "            continue    \n",
    "\n",
    "        if(tokenized_text[index+1][0:2]==\"##\"):\n",
    "            special.append(start_pos+index)\n",
    "            whole_word += tokenized_text[index]\n",
    "            isSpecial = True\n",
    "            continue\n",
    "        else:\n",
    "            if isSpecial:\n",
    "                isSpecial = False\n",
    "                special.append(start_pos+index)\n",
    "                position.append(special)\n",
    "                whole_word += tokenized_text[index]\n",
    "                whole_word = whole_word.replace('##','')\n",
    "                words.append(whole_word)\n",
    "                whole_word = ''\n",
    "                special =  []\n",
    "            else:\n",
    "                position.append(start_pos+index)\n",
    "                words.append(tokenized_text[index])\n",
    "\n",
    "    if isSpecial:\n",
    "        isSpecial = False\n",
    "        special.append(start_pos+index+1)\n",
    "        position.append(special)\n",
    "        whole_word += tokenized_text[index+1]\n",
    "        whole_word = whole_word.replace('##','')\n",
    "        words.append(whole_word)\n",
    "    else:\n",
    "        position.append(start_pos+index+1)\n",
    "        words.append(tokenized_text[index+1])\n",
    "       \n",
    "    return tokenized_text, words, position\n",
    "\n",
    "def convert_whole_word_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"Convert_whole_word_to_feature\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "\n",
    "    class ClassName(object):\n",
    "\n",
    "\n",
    "    \t\"\"\"docstring for ClassName\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \tdef __init__(self, arg):\n",
    "\n",
    "    \t\tsuper(ClassName, self).__init__()\n",
    "\n",
    "\n",
    "    \t\tself.arg = arg\n",
    "\n",
    "    \t\t\n",
    "\n",
    "    \n",
    "    len_tokens = len(tokens_a)\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    mask_index = []\n",
    "\n",
    "    for mask_pos in mask_position:\n",
    "        mask_index.append(mask_pos-len_tokens-2)\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i in mask_index:\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    index = 0\n",
    "    count = 0\n",
    "    mask_position_length = len(mask_position)\n",
    "\n",
    "    while count in range(mask_position_length):\n",
    "        index = mask_position_length - 1 - count\n",
    "\n",
    "        pos = mask_position[index]\n",
    "        if index == 0:\n",
    "            tokens[pos] = '[MASK]'\n",
    "        else:\n",
    "            del tokens[pos]\n",
    "            del input_type_ids[pos]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def convert_token_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"----------\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    len_tokens = len(tokens_a)\n",
    "    #print(\"length of tokens: \", len_tokens)\n",
    "\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i==(mask_position-len_tokens-2):\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    true_word = tokens[mask_position]\n",
    "    tokens[mask_position] =  '[MASK]'\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def getWordmap(wordVecPath):\n",
    "    words=[]\n",
    "    We = []\n",
    "    f = open(wordVecPath,'r', encoding=\"utf-8\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for (n,line) in enumerate(lines):\n",
    "        if (n == 0) :\n",
    "            print(line)\n",
    "            continue\n",
    "        word, vect = line.rstrip().split(' ', 1)\n",
    "                    \n",
    "        vect = np.fromstring(vect, sep=' ')\n",
    "                \n",
    "        We.append(vect)\n",
    "\n",
    "        words.append(word)\n",
    "\n",
    "        #if(n==200000):\n",
    "        #    break\n",
    "    f.close()       \n",
    "    return (words, We)\n",
    "\n",
    "\n",
    "def getWordCount(word_count_path):\n",
    "    word2count = {}\n",
    "    xlsx_file = Path('',word_count_path)\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "    sheet = wb_obj.active\n",
    "\n",
    "    last_column = sheet.max_column-1\n",
    "    for i, row in enumerate(sheet.iter_rows(values_only=True)):\n",
    "        if i==0:\n",
    "            continue\n",
    "        word2count[row[0]] = round(float(row[last_column]),3)\n",
    "        \n",
    "    return word2count\n",
    "\n",
    "def read_eval_index_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            \n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "            mask_word,labels = words.strip().split('\\t',1)\n",
    "            label = labels.split('\\t')\n",
    "                \n",
    "            sentences.append(sentence)\n",
    "            mask_words.append(mask_word)\n",
    "                \n",
    "            one_labels = []\n",
    "            for la in label[1:]:\n",
    "                if la not in one_labels:\n",
    "                    la_id,la_word = la.split(':')\n",
    "                    one_labels.append(la_word)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "            mask_labels.append(one_labels)\n",
    "            \n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def read_eval_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "    id = 0\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if is_label:\n",
    "                id += 1\n",
    "                if id==1:\n",
    "                    continue\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "                mask_word,labels = words.strip().split('\\t',1)\n",
    "                label = labels.split('\\t')\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "                \n",
    "                one_labels = []\n",
    "                for la in label:\n",
    "                    if la not in one_labels:\n",
    "                        one_labels.append(la)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "                mask_labels.append(one_labels)\n",
    "            else:\n",
    "                if not line:\n",
    "                    break\n",
    "                #print(line)\n",
    "                sentence,mask_word = line.strip().split('\\t')\n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def BERT_candidate_generation(source_word, pre_tokens, pre_scores, ps, num_selection=10):\n",
    "\n",
    "    cur_tokens=[]\n",
    "   \n",
    "\n",
    "    source_stem = ps.stem(source_word)\n",
    "\n",
    "    assert num_selection<=len(pre_tokens)\n",
    "\n",
    "    for i in range(len(pre_tokens)):\n",
    "        token = pre_tokens[i]\n",
    "     \n",
    "        if token[0:2]==\"##\":\n",
    "            continue\n",
    "\n",
    "        if(token==source_word):\n",
    "            continue\n",
    "\n",
    "        token_stem = ps.stem(token)\n",
    "\n",
    "        if(token_stem == source_stem):\n",
    "            continue\n",
    "\n",
    "        if (len(token_stem)>=3) and (token_stem[:3]==source_stem[:3]):\n",
    "            continue\n",
    "\n",
    "        cur_tokens.append(token)\n",
    "        \n",
    "\n",
    "        if(len(cur_tokens)==num_selection):\n",
    "            break\n",
    "    \n",
    "    if(len(cur_tokens)==0):\n",
    "        cur_tokens = pre_tokens[0:num_selection+1]\n",
    "        \n",
    "\n",
    "    assert len(cur_tokens)>0       \n",
    "\n",
    "    return cur_tokens\n",
    "\n",
    "def cross_entropy_word(X,i,pos):\n",
    "    \n",
    "    #print(X)\n",
    "    #print(X[0,2,3])\n",
    "    X = softmax(X,axis=1)\n",
    "    loss = 0\n",
    "    loss -= np.log10(X[i,pos])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_score(sentence,tokenizer,maskedLM):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "\n",
    "    len_sen = len(tokenize_input)\n",
    "\n",
    "    START_TOKEN = '[CLS]'\n",
    "    SEPARATOR_TOKEN = '[SEP]'\n",
    "\n",
    "    tokenize_input.insert(0, START_TOKEN)\n",
    "    tokenize_input.append(SEPARATOR_TOKEN)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenize_input)\n",
    "\n",
    "    #tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    #print(\"tensor_input\")\n",
    "    #print(tensor_input)\n",
    "    #tensor_input = tensor_input.to('cuda')\n",
    "    sentence_loss = 0\n",
    "    \n",
    "    for i,word in enumerate(tokenize_input):\n",
    "\n",
    "        if(word == START_TOKEN or word==SEPARATOR_TOKEN):\n",
    "            continue\n",
    "\n",
    "        orignial_word = tokenize_input[i]\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        #print(tokenize_input)\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        #print(mask_input)\n",
    "        mask_input = mask_input.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            att, pre_word =maskedLM(mask_input)\n",
    "        word_loss = cross_entropy_word(pre_word[0].cpu().numpy(),i,input_ids[i])\n",
    "        sentence_loss += word_loss\n",
    "        #print(word_loss)\n",
    "        tokenize_input[i] = orignial_word\n",
    "        \n",
    "    return np.exp(sentence_loss/len_sen)\n",
    "\n",
    "\n",
    "def LM_score(source_word,source_context,substitution_selection,tokenizer,maskedLM):\n",
    "    #source_index = source_context.index(source_word)\n",
    "\n",
    "    source_sentence = ''\n",
    "\n",
    "    for context in source_context:\n",
    "        source_sentence += context + \" \"\n",
    "    \n",
    "    source_sentence = source_sentence.strip()\n",
    "    #print(source_sentence)\n",
    "    LM = []\n",
    "\n",
    "    source_loss = get_score(source_sentence,tokenizer,maskedLM)\n",
    "\n",
    "    for substibution in substitution_selection:\n",
    "        \n",
    "        sub_sentence = source_sentence.replace(source_word,substibution)\n",
    "\n",
    "        \n",
    "        #print(sub_sentence)\n",
    "        score = get_score(sub_sentence,tokenizer,maskedLM)\n",
    "\n",
    "        #print(score)\n",
    "        LM.append(score)\n",
    "\n",
    "    return LM,source_loss\n",
    "\n",
    "\n",
    "def preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count):\n",
    "    ss = []\n",
    "    ##ss_score=[]\n",
    "    sis_scores=[]\n",
    "    count_scores=[]\n",
    "\n",
    "    isFast = True\n",
    "\n",
    "    if(source_word not in fasttext_dico):\n",
    "        isFast = False\n",
    "    else:\n",
    "        source_emb = fasttext_emb[fasttext_dico.index(source_word)].reshape(1,-1)\n",
    "\n",
    "    #ss.append(source_word)\n",
    "\n",
    "    for sub in substitution_selection:\n",
    "\n",
    "        if sub not in word_count:\n",
    "            continue\n",
    "        else:\n",
    "            sub_count = word_count[sub]\n",
    "\n",
    "        if(sub_count<=3):\n",
    "            continue\n",
    "\n",
    "        #if sub_count<source_count:\n",
    "         #   continue\n",
    "        if isFast:\n",
    "            if sub not in fasttext_dico:\n",
    "                continue\n",
    "\n",
    "            token_index_fast = fasttext_dico.index(sub)\n",
    "            sis = cosine(source_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            #if sis<0.35:\n",
    "            #    continue\n",
    "            sis_scores.append(sis)\n",
    "\n",
    "        ss.append(sub)\n",
    "        count_scores.append(sub_count)\n",
    "\n",
    "    return ss,sis_scores,count_scores\n",
    "\n",
    "def compute_context_sis_score(source_word, sis_context, substitution_selection, fasttext_dico, fasttext_emb):\n",
    "    context_sis = []\n",
    "\n",
    "    word_context = []\n",
    "\n",
    "    \n",
    "\n",
    "    for con in sis_context:\n",
    "        if con==source_word or (con not in fasttext_dico):\n",
    "            continue\n",
    "\n",
    "        word_context.append(con)\n",
    "\n",
    "    if len(word_context)!=0:\n",
    "        for sub in substitution_selection:\n",
    "            sub_emb = fasttext_emb[fasttext_dico.index(sub)].reshape(1,-1)\n",
    "            all_sis = 0\n",
    "            for con in word_context:\n",
    "                token_index_fast = fasttext_dico.index(con)\n",
    "                all_sis += cosine(sub_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            context_sis.append(all_sis/len(word_context))\n",
    "    else:\n",
    "        for i in range(len(substitution_selection)):\n",
    "            context_sis.append(len(substitution_selection)-i)\n",
    "\n",
    "            \n",
    "    return context_sis\n",
    "\n",
    "\n",
    "def substitution_ranking(source_word, source_context, substitution_selection, fasttext_dico, fasttext_emb, word_count, ssPPDB, tokenizer, maskedLM, lables):\n",
    "\n",
    "    ss,sis_scores,count_scores=preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count)\n",
    "\n",
    "    #print(ss)\n",
    "    if len(ss)==0:\n",
    "        return source_word\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        seq = sorted(sis_scores,reverse = True )\n",
    "        sis_rank = [seq.index(v)+1 for v in sis_scores]\n",
    "    \n",
    "    rank_count = sorted(count_scores,reverse = True )\n",
    "    \n",
    "    count_rank = [rank_count.index(v)+1 for v in count_scores]\n",
    "  \n",
    "    lm_score,source_lm = LM_score(source_word,source_context,ss,tokenizer,maskedLM)\n",
    "\n",
    "    rank_lm = sorted(lm_score)\n",
    "    lm_rank = [rank_lm.index(v)+1 for v in lm_score]\n",
    "    \n",
    "\n",
    "    bert_rank = []\n",
    "    ppdb_rank =[]\n",
    "    for i in range(len(ss)):\n",
    "        bert_rank.append(i+1)\n",
    "\n",
    "        if ss[i] in ssPPDB:\n",
    "        \tppdb_rank.append(1)\n",
    "        else:\n",
    "        \tppdb_rank.append(len(ss)/3)\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        all_ranks = [bert+sis+count+LM+ppdb  for bert,sis,count,LM,ppdb in zip(bert_rank,sis_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    else:\n",
    "        all_ranks = [bert+count+LM+ppdb  for bert,count,LM,ppdb in zip(bert_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    #all_ranks = [con for con in zip(context_rank)]\n",
    "\n",
    "\n",
    "    pre_index = all_ranks.index(min(all_ranks))\n",
    "\n",
    "    #return ss[pre_index]\n",
    "\n",
    "    pre_count = count_scores[pre_index]\n",
    "\n",
    "    if source_word in word_count:\n",
    "    \tsource_count = word_count[source_word]\n",
    "    else:\n",
    "    \tsource_count = 0\n",
    "\n",
    "    pre_lm = lm_score[pre_index]\n",
    "\n",
    "    #print(lm_score)\n",
    "    #print(source_lm)\n",
    "    #print(pre_lm)\n",
    "\n",
    "\n",
    "    #pre_word = ss[pre_index]\n",
    "\n",
    "\n",
    "    if source_lm>pre_lm or pre_count>source_count:\n",
    "    \tpre_word = ss[pre_index]\n",
    "    else:\n",
    "    \tpre_word = source_word\n",
    "\n",
    "    return pre_word\n",
    "\n",
    "\n",
    "def evaulation_SS_scores(ss,labels):\n",
    "    assert len(ss)==len(labels)\n",
    "\n",
    "    potential = 0\n",
    "    instances = len(ss)\n",
    "    precision = 0\n",
    "    precision_all = 0\n",
    "    recall = 0\n",
    "    recall_all = 0\n",
    "\n",
    "    for i in range(len(ss)):\n",
    "\n",
    "        one_prec = 0\n",
    "        \n",
    "        common = list(set(ss[i]).intersection(labels[i]))\n",
    "\n",
    "        if len(common)>=1:\n",
    "            potential +=1\n",
    "        precision += len(common)\n",
    "        recall += len(common)\n",
    "        precision_all += len(ss[i])\n",
    "        recall_all += len(labels[i])\n",
    "\n",
    "    potential /=  instances\n",
    "    precision /= precision_all\n",
    "    recall /= recall_all\n",
    "    F_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return potential,precision,recall,F_score\n",
    "\n",
    "\n",
    "def evaulation_pipeline_scores(substitution_words,source_words,gold_words):\n",
    "\n",
    "    instances = len(substitution_words)\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    changed_proportion = 0\n",
    "\n",
    "    for sub, source, gold in zip(substitution_words,source_words,gold_words):\n",
    "        if sub==source or (sub in gold):\n",
    "            precision += 1\n",
    "        if sub!=source and (sub in gold):\n",
    "            accuracy += 1\n",
    "        if sub!=source:\n",
    "            changed_proportion += 1\n",
    "\n",
    "    return precision/instances,accuracy/instances,changed_proportion/instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_context(words, mask_index, window):\n",
    "    #extract 7 words around the content word\n",
    "\n",
    "    length = len(words)\n",
    "\n",
    "    half = int(window/2)\n",
    "\n",
    "    assert mask_index>=0 and mask_index<length\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    if length<=window:\n",
    "        context = words\n",
    "    elif mask_index<length-half and mask_index>=half:\n",
    "        context = words[mask_index-half:mask_index+half+1]\n",
    "    elif mask_index<half:\n",
    "        context = words[0:window]\n",
    "    elif mask_index>=length-half:\n",
    "        context = words[length-window:length]\n",
    "    else:\n",
    "        print(\"Wrong!\")\n",
    "\n",
    "    return context\n",
    "\n",
    "def preprocess_tag(tag):\n",
    "    if tag[0] ==\"V\" or tag[0]==\"N\":\n",
    "        return tag\n",
    "    if tag[0]==\"R\":\n",
    "        return \"r\"\n",
    "    if tag[0]==\"J\" or tag[0]==\"I\":\n",
    "        return 'a'\n",
    "    else:\n",
    "        return 's'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--eval_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The evaluation data dir.\")\n",
    "parser.add_argument(\"--bert_model\", default=None, type=str,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "\n",
    "parser.add_argument(\"--output_SR_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The output directory of writing substitution selection.\")\n",
    "parser.add_argument(\"--word_embeddings\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word embeddings\")\n",
    "parser.add_argument(\"--word_frequency\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "    \n",
    "parser.add_argument(\"--ppdb\",\n",
    "                        default=\"./ppdb-2.0-tldr\",\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "\n",
    "parser.add_argument(\"--prob_mask\",\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of the masked words in first sentence. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "\n",
    "\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--num_selections\",\n",
    "                        default=20,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--num_eval_epochs\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-vocab.txt\n",
      "INFO:pytorch_pretrained_bert.modeling:loading weights file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-pytorch_model.bin\n",
      "INFO:pytorch_pretrained_bert.modeling:loading configuration file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-config.json\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "if args.server_ip and args.server_port:\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not args.do_eval:\n",
    "    raise ValueError(\"At least `do_eval` must be True.\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "    \n",
    "\n",
    "# Prepare model\n",
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "model = BertForMaskedLM.from_pretrained(args.bert_model,output_attentions=True,cache_dir=cache_dir)\n",
    "if args.fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "\n",
    "output_sr_file = open(args.output_SR_file,\"a+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings ...\n",
      "2000000 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embeddings ...\")\n",
    "\n",
    "wordVecPath = args.word_embeddings\n",
    "#wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\"\n",
    "\n",
    "fasttext_dico, fasttext_emb = getWordmap(wordVecPath)\n",
    "\n",
    "#stopword = set(stopwords.words('english'))\n",
    "word_count_path = args.word_frequency\n",
    "#word_count_path = \"word_frequency_wiki.txt\"\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PPDB ...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading PPDB ...\")\n",
    "ppdb_path = args.ppdb\n",
    "ppdb_model = Ppdb(ppdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_eval=True\n",
    "args.do_lower_case=True\n",
    "args.num_selections=15\n",
    "args.prob_mask=0.0\n",
    "args.eval_dir='D:/data/bert_ppdb/datasets/lex.mturk.txt'\n",
    "args.bert_model='bert-large-uncased-whole-word-masking'\n",
    "args.max_seq_length=250\n",
    "args.word_embeddings='D:/data/bert_ppdb/crawl-300d-2M-subword.vec'\n",
    "args.word_frequency='D:/data/bert_ppdb/SUBTLEX_frequency.xlsx'\n",
    "args.ppdb='D:/data/bert_ppdb/ppdb-2.0-tldr'\n",
    "args.output_SR_file='D:/data/bert_ppdb/results/NNSeval'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.944 0.4 0.15554518587649713 0.22398924851607124\n",
      "The score of evaluation for full LS pipeline\n",
      "0.806 0.7 0.894\n",
      "10\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.982 0.2958 0.23005132991133925 0.25881529442645906\n",
      "The score of evaluation for full LS pipeline\n",
      "0.852 0.768 0.916\n",
      "15\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.99 0.2396 0.2795146990200653 0.2580228300667672\n",
      "The score of evaluation for full LS pipeline\n",
      "0.868 0.794 0.926\n",
      "20\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.992 0.2017 0.3137346399128947 0.2455414206585915\n",
      "The score of evaluation for full LS pipeline\n",
      "0.878 0.812 0.934\n",
      "25\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.992 0.17504 0.3403328666977757 0.2311796714036663\n",
      "The score of evaluation for full LS pipeline\n",
      "0.876 0.82 0.944\n",
      "30\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "0.996 0.1534 0.3579094727018199 0.2147557048859023\n",
      "The score of evaluation for full LS pipeline\n",
      "0.872 0.822 0.95\n",
      "35\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of evaluation for BERT candidate generation\n",
      "1.0 0.13754285714285713 0.3743972624047286 0.20117848635546826\n",
      "The score of evaluation for full LS pipeline\n",
      "0.868 0.83 0.962\n",
      "40\n",
      "prob_mask: 0.0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "for num in range(9):\n",
    "    args.seed=num*10+5\n",
    "    #print(args.seed)\n",
    "    args.num_selections=(num+1)*5\n",
    "    print(args.num_selections)\n",
    "    #print('第 ',num,'次循环')\n",
    "    \n",
    "    \n",
    "    CGBERT = []\n",
    "    CSBERT = []\n",
    "    SS = []\n",
    "    substitution_words = []\n",
    "   \n",
    "    num_selection = args.num_selections\n",
    "\n",
    "    bre_i=0\n",
    "\n",
    "    window_context = 11\n",
    "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        \n",
    "     \n",
    "        fileName = args.eval_dir.split('/')[-1][:-4]\n",
    "        if fileName=='lex.mturk':\n",
    "            eval_examples, mask_words, mask_labels = read_eval_dataset(args.eval_dir)\n",
    "        else:\n",
    "            eval_examples, mask_words, mask_labels = read_eval_index_dataset(args.eval_dir)\n",
    "\n",
    "       \n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "            #logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "      \n",
    "        eval_size = len(eval_examples)\n",
    "\n",
    "        print(\"prob_mask:\",args.prob_mask)\n",
    "    \n",
    "    \n",
    "    for i in range(eval_size):\n",
    "        if (i%100==0):\n",
    "            print(i)\n",
    "        #print('Sentence {} rankings: '.format(i))\n",
    "        #output_sr_file.write(str(i))\n",
    "        #output_sr_file.write(' sentence: ')\n",
    "        #output_sr_file.write('\\n')\n",
    "        #print(eval_examples[i])\n",
    "        #print(mask_words[i])\n",
    "        tokens, words, position = convert_sentence_to_token(eval_examples[i], args.max_seq_length, tokenizer)\n",
    "\n",
    "        assert len(words)==len(position)\n",
    "\n",
    "        mask_index = words.index(mask_words[i])\n",
    "\n",
    "        mask_context = extract_context(words,mask_index,window_context)\n",
    "\n",
    "        len_tokens = len(tokens)\n",
    "\n",
    "        mask_position = position[mask_index]\n",
    " \n",
    "        if isinstance(mask_position,list):\n",
    "            feature = convert_whole_word_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "        else:\n",
    "            feature = convert_token_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "\n",
    "        tokens_tensor = torch.tensor([feature.input_ids])\n",
    "\n",
    "        token_type_ids = torch.tensor([feature.input_type_ids])\n",
    "\n",
    "        attention_mask = torch.tensor([feature.input_mask])\n",
    "    \n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        token_type_ids = token_type_ids.to('cuda')\n",
    "        attention_mask = attention_mask.to('cuda')\n",
    "\n",
    "            # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            all_attentions,prediction_scores = model(tokens_tensor, token_type_ids,attention_mask)\n",
    "\n",
    "\n",
    "            \n",
    "        if isinstance(mask_position,list):\n",
    "            predicted_top = prediction_scores[0, mask_position[0]].topk(80)\n",
    "        else:\n",
    "            predicted_top = prediction_scores[0, mask_position].topk(80)\n",
    "            #print(predicted_top[0].cpu().numpy())\n",
    "        pre_tokens = tokenizer.convert_ids_to_tokens(predicted_top[1].cpu().numpy())\n",
    "            \n",
    "        #print(predicted_top[0].cpu().numpy())\n",
    "\n",
    "        sentence = eval_examples[i].lower()\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        words_tag = nltk.pos_tag(words)\n",
    "\n",
    "        complex_word_index = words.index(mask_words[i])\n",
    "\n",
    "        complex_word_tag = words_tag[complex_word_index][1]\n",
    "\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        complex_word_tag = preprocess_tag(complex_word_tag)\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        #mask_context = extract_context(words,complex_word_index,window_context)\n",
    "\n",
    "        #break\n",
    "        #print(mask_words[i])\n",
    "        #cgPPDB,csPPDB = PPDB_candidate_generation(mask_words[i],complex_word_tag, ppdb_model, ps, word_count, 30)\n",
    "        cgPPDB = ppdb_model.predict(mask_words[i],complex_word_tag)\n",
    "\n",
    "        #if(len(cgPPDB)>30):\n",
    "        #cgPPDB=cgPPDB[:30]\n",
    "\n",
    "        #break\n",
    "        cgBERT = BERT_candidate_generation(mask_words[i], pre_tokens, predicted_top[0].cpu().numpy(), ps, args.num_selections)\n",
    "            \n",
    "        CGBERT.append(cgBERT)\n",
    "\n",
    "\n",
    "        pre_word = substitution_ranking(mask_words[i], mask_context, cgBERT, fasttext_dico, fasttext_emb,word_count,cgPPDB,tokenizer,model,mask_labels[i])\n",
    "\n",
    "        substitution_words.append(pre_word)\n",
    "    potential,precision,recall,F_score=evaulation_SS_scores(CGBERT, mask_labels)\n",
    "    print(\"The score of evaluation for BERT candidate generation\")\n",
    "    print(potential,precision,recall,F_score)\n",
    "\n",
    "        \n",
    "\n",
    "    precision,accuracy,changed_proportion=evaulation_pipeline_scores(substitution_words, mask_words, mask_labels)\n",
    "    print(\"The score of evaluation for full LS pipeline\")\n",
    "    print(precision,accuracy,changed_proportion)\n",
    "\n",
    "\n",
    "        #output_sr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "# 音频初始化\n",
    "pygame.mixer.init()\n",
    "# 加载音频文件路径 (路径必须真实存在，音频文件格式支持mp3/ogg等格式)\n",
    "pygame.mixer.music.load(r'D:/1.mp3')\n",
    "pygame.mixer.music.play()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
