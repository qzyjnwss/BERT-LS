{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/home/qiang/Desktop/pytorch-pretrained-BERT')\n",
    "\n",
    "retval = os.getcwd()\n",
    "print (\"当前工作目录为 %s\" % retval)\n",
    "os.chdir('./examples')\n",
    "\n",
    "import extract_features\n",
    "\n",
    "from extract_features import *\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_file = \"../samples/input.txt\"\n",
    "\n",
    "learning_rate = 2e-5\n",
    "warmup_proportion = 0.1\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "train_batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "do_train=True\n",
    "\n",
    "special_tokens = [ '[MASK]','[SEP]']\n",
    "    \n",
    "special_tokens_id = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "\n",
    "word_pred = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录为 /home/qiang/Desktop/pytorch-pretrained-BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2019 15:21:19 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/qiang/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/qiang/Desktop/pytorch-pretrained-BERT')\n",
    "\n",
    "retval = os.getcwd()\n",
    "print (\"当前工作目录为 %s\" % retval)\n",
    "os.chdir('./examples')\n",
    "\n",
    "import extract_features\n",
    "\n",
    "from extract_features import *\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_file = \"../samples/input.txt\"\n",
    "\n",
    "learning_rate = 2e-5\n",
    "warmup_proportion = 0.1\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "train_batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "do_train=True\n",
    "\n",
    "special_tokens = [ '[MASK]','[SEP]']\n",
    "    \n",
    "special_tokens_id = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "\n",
    "word_pred = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2019 15:21:42 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/qiang/.cache/torch/pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "06/24/2019 15:21:42 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/qiang/.cache/torch/pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpuu2iz3do\n",
      "06/24/2019 15:21:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/24/2019 15:21:47 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "    \n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "if do_train:\n",
    "    train_examples = read_examples(input_file)\n",
    "    num_train_optimization_steps = int( len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.to('cuda')\n",
    "\n",
    "if do_train:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ] \n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=learning_rate,\n",
    "                                 warmup=warmup_proportion,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2019 15:22:02 - INFO - extract_features -   *** Example ***\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   unique_id: 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   tokens: [CLS] there is manuscript evidence that austen continued to work on these pieces as late as the period 1809 a ' ' 11 , and that her niece and nephew , anna and james edward austen , made further additions as late as 1814 . [SEP] there is some proof that austen continued to work on these pieces later in life . her nephew and niece , james edward and anna austen , may have made further additions to her work in around 1814 . [SEP]\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_ids: 101 2045 2003 8356 3350 2008 24177 2506 2000 2147 2006 2122 4109 2004 2397 2004 1996 2558 12861 1037 1005 1005 2340 1010 1998 2008 2014 12286 1998 7833 1010 4698 1998 2508 3487 24177 1010 2081 2582 13134 2004 2397 2004 10977 1012 102 2045 2003 2070 6947 2008 24177 2506 2000 2147 2006 2122 4109 2101 1999 2166 1012 2014 7833 1998 12286 1010 2508 3487 1998 4698 24177 1010 2089 2031 2081 2582 13134 2000 2014 2147 1999 2105 10977 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   *** Example ***\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   unique_id: 1\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   tokens: [CLS] many species had vanished by the end of the nineteenth century , with european settlement . [SEP] many species had disappeared by the end of the nineteenth century , with european settlement . [SEP]\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_ids: 101 2116 2427 2018 9955 2011 1996 2203 1997 1996 9137 2301 1010 2007 2647 4093 1012 102 2116 2427 2018 5419 2011 1996 2203 1997 1996 9137 2301 1010 2007 2647 4093 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   *** Example ***\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   unique_id: 2\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   tokens: [CLS] admission to ts ##ing ##hua is extremely competitive . [SEP] entrance to ts ##ing ##hua is very very difficult . [SEP]\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_ids: 101 9634 2000 24529 2075 14691 2003 5186 6975 1012 102 4211 2000 24529 2075 14691 2003 2200 2200 3697 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   *** Example ***\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   unique_id: 3\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   tokens: [CLS] she remained in the united states until 1927 when she and her husband returned to france . [SEP] she stayed in the united states until 1927 then she and her husband went to france . [SEP]\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_ids: 101 2016 2815 1999 1996 2142 2163 2127 4764 2043 2016 1998 2014 3129 2513 2000 2605 1012 102 2016 4370 1999 1996 2142 2163 2127 4764 2059 2016 1998 2014 3129 2253 2000 2605 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   *** Example ***\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   unique_id: 4\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   tokens: [CLS] today nr ##c is organised as an independent , private foundation . [SEP] today nr ##c is organised as an independent foundation . [SEP]\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_ids: 101 2651 17212 2278 2003 7362 2004 2019 2981 1010 2797 3192 1012 102 2651 17212 2278 2003 7362 2004 2019 2981 3192 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/24/2019 15:22:02 - INFO - extract_features -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-215f0b7a3720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch-pretrained-BERT/pytorch_pretrained_bert/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mlr_scheduled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mlr_scheduled\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'schedule'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mupdate_with_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduled\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch-pretrained-BERT/pytorch_pretrained_bert/optimization.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self, step, nowarn)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_total\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# warning for exceeding t_total (only active with warmup_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def mask_out(input_ids):\n",
    "    \n",
    "    bs, slen = input_ids.size()\n",
    "    #print('slen ', slen, ' bs ',bs)\n",
    "    #masked_lm_labels = [-1] * len(input_ids)\n",
    "    \n",
    "    masked_lm_labels = np.random.rand(bs,slen) <= word_pred\n",
    "    masked_lm_labels = torch.from_numpy(masked_lm_labels.astype(np.int16)).long().cuda()\n",
    "    \n",
    "    masked_lm_labels[input_ids == 0] = 0\n",
    "    \n",
    "    #masked_lm_labels = input_ids[masked_lm_labels==1]\n",
    "    \n",
    "    masked_lm_labels[ masked_lm_labels == 0] = -1\n",
    "\n",
    "    masked_lm_labels[:,0] = -1\n",
    "    \n",
    "    masked_lm_labels[input_ids == special_tokens_id[1]] = -1\n",
    "    input_id2 = input_ids.clone()\n",
    "    masked_lm_labels [masked_lm_labels == 1] = input_id2[masked_lm_labels == 1]\n",
    "    input_ids [masked_lm_labels > 1] = special_tokens_id[0]\n",
    "    \n",
    "    return input_ids, masked_lm_labels\n",
    "\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "if do_train:\n",
    "    \n",
    "    \n",
    "    features = convert_examples_to_features(\n",
    "        examples=train_examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    #print(all_input_ids)\n",
    "    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            \n",
    "            batch = tuple(t.to('cuda') for t in batch)\n",
    "            input_ids, input_mask, input_type_ids = batch\n",
    "            input_ids, masked_lm_labels = mask_out(input_ids)\n",
    "            #\n",
    "\n",
    "            loss = model(input_ids, input_type_ids, input_mask,masked_lm_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-587310c07234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdo_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdo_train\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one of `do_train` or `do_eval` must be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'do_train' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "\n",
    "output_dir = \"./tmp/DataAug_TS\"\n",
    "do_eval = True\n",
    "\n",
    "if do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "        \n",
    "if do_train:\n",
    "        # Save a trained model, configuration and tokenizer\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "        output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        model_to_save.config.to_json_file(output_config_file)\n",
    "        tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = BertForMaskedLM.from_pretrained(output_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-9-3d7657b4171d>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3d7657b4171d>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    output_file.write('\\n')\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 3\n",
    "\n",
    "num_eval_epochs = 2\n",
    "\n",
    "#word_pred = 0.1\n",
    "output_file = open(\"../samples/output.txt\",\"w\")\n",
    "\n",
    "with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "    line = reader.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    line = line.strip()\n",
    "    output_file.write(line)\n",
    "    output_file.write('\\n')\n",
    "\n",
    "if do_eval:\n",
    "    eval_examples = read_examples(input_file)\n",
    "    \n",
    "    eval_features = convert_examples_to_features(\n",
    "        examples=eval_examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_input_type_ids = torch.tensor([f.input_type_ids for f in eval_features], dtype=torch.long)\n",
    "    \n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids)\n",
    "    eval_sampler = RandomSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "    \n",
    "    for _ in range(num_eval_epochs):\n",
    "        for input_ids, input_mask, input_type_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "\n",
    "            #print(\"input_ids \", input_ids)\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            input_type_ids = input_type_ids.to(device)\n",
    "\n",
    "\n",
    "            input_ids, masked_lm_labels = mask_out(input_ids)\n",
    "\n",
    "            #print(\"masked_lm_labels: \", masked_lm_labels)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction_scores = model(input_ids, input_type_ids, input_mask)\n",
    "\n",
    "            #preds.append(prediction_scores.detach().numpy())    \n",
    "            #pred = prediction_scores[:,0,:]\n",
    "            #print(input_ids)\n",
    "            #print(prediction_scores.size())\n",
    "\n",
    "            for i in range(prediction_scores.size(0)):\n",
    "\n",
    "                id_list = []\n",
    "                isSame = True\n",
    "\n",
    "                for j in range(len(input_ids[i])):\n",
    "                    if(input_ids[i][j].item()==0):\n",
    "                        break\n",
    "                    if input_ids[i][j]==special_tokens_id[0]:\n",
    "                        #print(masked_lm_labels[i][j])\n",
    "                        predicted_index = torch.argmax(prediction_scores[i, j]).item()\n",
    "\n",
    "                        if(predicted_index!=masked_lm_labels[i][j]):\n",
    "                            #print(predicted_index)\n",
    "                            isSame = False\n",
    "\n",
    "                        id_list.append(predicted_index)\n",
    "                    elif input_ids[i][i]>0:\n",
    "                        id_list.append(input_ids[i][j].item())\n",
    "\n",
    "\n",
    "                #print(id_list)\n",
    "                if(isSame):\n",
    "                    continue\n",
    "                predicted_tokens = tokenizer.convert_ids_to_tokens(id_list)\n",
    "                #print(predicted_tokens)\n",
    "                sent = ''\n",
    "                for xi in range(len(predicted_tokens)-1):\n",
    "                    if predicted_tokens[xi]=='[CLS]':\n",
    "                        continue\n",
    "                    elif predicted_tokens[xi]=='[SEP]':\n",
    "                        sent = sent + \" ||| \"\n",
    "                    else:\n",
    "                        sent = sent + predicted_tokens[xi] + \" \"\n",
    "                #sent = ' '.join[x for x in predicted_tokens]\n",
    "                fine_sent = sent.replace(' ##', '')\n",
    "                output_file.write(fine_sent)\n",
    "                #output_file.write('\\n')\n",
    "            #if(not isSame):\n",
    "             #   print(fine_sent)\n",
    "            \n",
    "            \n",
    "        #print(pred[])\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import re\n",
    "\n",
    "def splitTwoFiles(input_file, out_complex_file, out_simple_file):\n",
    "    out_complex = open(out_complex_file, \"w\")\n",
    "    out_simple = open(out_simple_file, \"w\")\n",
    "    \n",
    "    id = 0\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            #print(id, \" \",line)\n",
    "            #id = id+1\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            \n",
    "            sents = line.split(\"|||\")\n",
    "            #m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "            out_complex.write(sents[0])\n",
    "            out_complex.write('\\n')\n",
    "            out_simple.write(sents[1])\n",
    "            out_simple.write('\\n')\n",
    "            \n",
    "    out_complex.close()\n",
    "    out_simple.close()\n",
    "    \n",
    "\n",
    "splitTwoFiles(\"./samples/wikilarge_merge.txt\",\"./samples/wikilarge_merge.src\",\"./samples/wikilarge_merge.tgt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeTwoFiles(input_complex_file, input_simple_file, merge_file):\n",
    "    \n",
    "    complex_lines = open(input_complex_file, \"r\").readlines()\n",
    "    simple_lines = open(input_simple_file, \"r\").readlines()\n",
    "    \n",
    "    assert len(complex_lines) ==  len(simple_lines)\n",
    "    merge_file = open(merge_file, \"w\")\n",
    "    \n",
    "    id = 0\n",
    "    for i in range(len(complex_lines)):\n",
    "        c = complex_lines[i].strip()\n",
    "        s = simple_lines[i].strip()\n",
    "        \n",
    "        merge_file.write(c)\n",
    "        merge_file.write(\" ||| \")\n",
    "        merge_file.write(s)\n",
    "        merge_file.write('\\n')\n",
    "            \n",
    "\n",
    "    merge_file.close()\n",
    "    \n",
    "\n",
    "MergeTwoFiles(\"./samples/delete_wikismall.src\",\"./samples/delete_wikismall.dst\",\"./samples/wikismall_delete.merge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.2, 2.3, 1, 4, 2.3]\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [3.2,2.3,1,4,2.3]\n",
    "print(a)\n",
    "b=sorted(enumerate(a), key=lambda x:x[1])\n",
    "\n",
    "for num in b:\n",
    "    print(num[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "bb\n",
      "cc.txt\n",
      "cc\n"
     ]
    }
   ],
   "source": [
    "a = 'aa/bb/cc.txt'\n",
    "a = a.split('/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ecb47b6c567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not float"
     ]
    }
   ],
   "source": [
    "bbb = open('test.txt',\"w\")\n",
    "\n",
    "bbb.write(str(0.3))\n",
    "bbb.write('\\t')\n",
    "bbb.write(str(0.5))\n",
    "\n",
    "bbb.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
