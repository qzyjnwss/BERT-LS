{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertForMaskedLM\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PPDB import Ppdb\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def convert_sentence_to_token(sentence, seq_length, tokenizer):\n",
    "  \n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "\n",
    "    if len(tokenized_text) > seq_length - 2:\n",
    "        tokenized_text = tokenized_text[0:(seq_length - 2)]\n",
    "\n",
    "    position =[]\n",
    "    special =[]\n",
    "    isSpecial = False\n",
    "\n",
    "    whole_word = ''\n",
    "    words = []\n",
    "\n",
    "    start_pos =  len(tokenized_text)  + 2\n",
    "\n",
    "    connect_sign = 0\n",
    "    for index in range(len(tokenized_text)-1):\n",
    "        \n",
    "        if(tokenized_text[index+1]==\"-\" and tokenized_text[index+2]!=\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]==\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "\n",
    "            if(tokenized_text[index-1]==\"-\"):\n",
    "                words.append(whole_word)\n",
    "                position.append(special)\n",
    "                special = []\n",
    "                whole_word = ''\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]!=\"-\" and tokenized_text[index-1]==\"-\"):\n",
    "            whole_word += tokenized_text[index]\n",
    "            words.append(whole_word)\n",
    "            whole_word = ''\n",
    "            special.append(start_pos+index)\n",
    "            position.append(special)\n",
    "            special = []\n",
    "            continue    \n",
    "\n",
    "        if(tokenized_text[index+1][0:2]==\"##\"):\n",
    "            special.append(start_pos+index)\n",
    "            whole_word += tokenized_text[index]\n",
    "            isSpecial = True\n",
    "            continue\n",
    "        else:\n",
    "            if isSpecial:\n",
    "                isSpecial = False\n",
    "                special.append(start_pos+index)\n",
    "                position.append(special)\n",
    "                whole_word += tokenized_text[index]\n",
    "                whole_word = whole_word.replace('##','')\n",
    "                words.append(whole_word)\n",
    "                whole_word = ''\n",
    "                special =  []\n",
    "            else:\n",
    "                position.append(start_pos+index)\n",
    "                words.append(tokenized_text[index])\n",
    "\n",
    "    if isSpecial:\n",
    "        isSpecial = False\n",
    "        special.append(start_pos+index+1)\n",
    "        position.append(special)\n",
    "        whole_word += tokenized_text[index+1]\n",
    "        whole_word = whole_word.replace('##','')\n",
    "        words.append(whole_word)\n",
    "    else:\n",
    "        position.append(start_pos+index+1)\n",
    "        words.append(tokenized_text[index+1])\n",
    "       \n",
    "    return tokenized_text, words, position\n",
    "\n",
    "def convert_whole_word_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"Convert_whole_word_to_feature\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "\n",
    "    class ClassName(object):\n",
    "\n",
    "\n",
    "    \t\"\"\"docstring for ClassName\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \tdef __init__(self, arg):\n",
    "\n",
    "    \t\tsuper(ClassName, self).__init__()\n",
    "\n",
    "\n",
    "    \t\tself.arg = arg\n",
    "\n",
    "    \t\t\n",
    "\n",
    "    \n",
    "    len_tokens = len(tokens_a)\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    mask_index = []\n",
    "\n",
    "    for mask_pos in mask_position:\n",
    "        mask_index.append(mask_pos-len_tokens-2)\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i in mask_index:\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    index = 0\n",
    "    count = 0\n",
    "    mask_position_length = len(mask_position)\n",
    "\n",
    "    while count in range(mask_position_length):\n",
    "        index = mask_position_length - 1 - count\n",
    "\n",
    "        pos = mask_position[index]\n",
    "        if index == 0:\n",
    "            tokens[pos] = '[MASK]'\n",
    "        else:\n",
    "            del tokens[pos]\n",
    "            del input_type_ids[pos]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def convert_token_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"----------\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    len_tokens = len(tokens_a)\n",
    "    #print(\"length of tokens: \", len_tokens)\n",
    "\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i==(mask_position-len_tokens-2):\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    true_word = tokens[mask_position]\n",
    "    tokens[mask_position] =  '[MASK]'\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def getWordmap(wordVecPath):\n",
    "    words=[]\n",
    "    We = []\n",
    "    f = open(wordVecPath,'r', encoding=\"utf-8\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for (n,line) in enumerate(lines):\n",
    "        if (n == 0) :\n",
    "            print(line)\n",
    "            continue\n",
    "        word, vect = line.rstrip().split(' ', 1)\n",
    "                    \n",
    "        vect = np.fromstring(vect, sep=' ')\n",
    "                \n",
    "        We.append(vect)\n",
    "\n",
    "        words.append(word)\n",
    "\n",
    "        #if(n==200000):\n",
    "        #    break\n",
    "    f.close()       \n",
    "    return (words, We)\n",
    "\n",
    "\n",
    "def getWordCount(word_count_path):\n",
    "    word2count = {}\n",
    "    xlsx_file = Path('',word_count_path)\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "    sheet = wb_obj.active\n",
    "\n",
    "    last_column = sheet.max_column-1\n",
    "    for i, row in enumerate(sheet.iter_rows(values_only=True)):\n",
    "        if i==0:\n",
    "            continue\n",
    "        word2count[row[0]] = round(float(row[last_column]),3)\n",
    "        \n",
    "    return word2count\n",
    "\n",
    "def read_eval_index_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            \n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "            mask_word,labels = words.strip().split('\\t',1)\n",
    "            label = labels.split('\\t')\n",
    "                \n",
    "            sentences.append(sentence)\n",
    "            mask_words.append(mask_word)\n",
    "                \n",
    "            one_labels = []\n",
    "            for la in label[1:]:\n",
    "                if la not in one_labels:\n",
    "                    la_id,la_word = la.split(':')\n",
    "                    one_labels.append(la_word)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "            mask_labels.append(one_labels)\n",
    "            \n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def read_eval_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "    id = 0\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if is_label:\n",
    "                id += 1\n",
    "                if id==1:\n",
    "                    continue\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "                mask_word,labels = words.strip().split('\\t',1)\n",
    "                label = labels.split('\\t')\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "                \n",
    "                one_labels = []\n",
    "                for la in label:\n",
    "                    if la not in one_labels:\n",
    "                        one_labels.append(la)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "                mask_labels.append(one_labels)\n",
    "            else:\n",
    "                if not line:\n",
    "                    break\n",
    "                #print(line)\n",
    "                sentence,mask_word = line.strip().split('\\t')\n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def BERT_candidate_generation(source_word, pre_tokens, pre_scores, ps, num_selection=10):\n",
    "\n",
    "    cur_tokens=[]\n",
    "   \n",
    "\n",
    "    source_stem = ps.stem(source_word)\n",
    "\n",
    "    assert num_selection<=len(pre_tokens)\n",
    "\n",
    "    for i in range(len(pre_tokens)):\n",
    "        token = pre_tokens[i]\n",
    "     \n",
    "        if token[0:2]==\"##\":\n",
    "            continue\n",
    "\n",
    "        if(token==source_word):\n",
    "            continue\n",
    "\n",
    "        token_stem = ps.stem(token)\n",
    "\n",
    "        if(token_stem == source_stem):\n",
    "            continue\n",
    "\n",
    "        if (len(token_stem)>=3) and (token_stem[:3]==source_stem[:3]):\n",
    "            continue\n",
    "\n",
    "        cur_tokens.append(token)\n",
    "        \n",
    "\n",
    "        if(len(cur_tokens)==num_selection):\n",
    "            break\n",
    "    \n",
    "    if(len(cur_tokens)==0):\n",
    "        cur_tokens = pre_tokens[0:num_selection+1]\n",
    "        \n",
    "\n",
    "    assert len(cur_tokens)>0       \n",
    "\n",
    "    return cur_tokens\n",
    "\n",
    "def cross_entropy_word(X,i,pos):\n",
    "    \n",
    "    #print(X)\n",
    "    #print(X[0,2,3])\n",
    "    X = softmax(X,axis=1)\n",
    "    loss = 0\n",
    "    loss -= np.log10(X[i,pos])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_score(sentence,tokenizer,maskedLM):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "\n",
    "    len_sen = len(tokenize_input)\n",
    "\n",
    "    START_TOKEN = '[CLS]'\n",
    "    SEPARATOR_TOKEN = '[SEP]'\n",
    "\n",
    "    tokenize_input.insert(0, START_TOKEN)\n",
    "    tokenize_input.append(SEPARATOR_TOKEN)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenize_input)\n",
    "\n",
    "    #tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    #print(\"tensor_input\")\n",
    "    #print(tensor_input)\n",
    "    #tensor_input = tensor_input.to('cuda')\n",
    "    sentence_loss = 0\n",
    "    \n",
    "    for i,word in enumerate(tokenize_input):\n",
    "\n",
    "        if(word == START_TOKEN or word==SEPARATOR_TOKEN):\n",
    "            continue\n",
    "\n",
    "        orignial_word = tokenize_input[i]\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        #print(tokenize_input)\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        #print(mask_input)\n",
    "        mask_input = mask_input.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            att, pre_word =maskedLM(mask_input)\n",
    "        word_loss = cross_entropy_word(pre_word[0].cpu().numpy(),i,input_ids[i])\n",
    "        sentence_loss += word_loss\n",
    "        #print(word_loss)\n",
    "        tokenize_input[i] = orignial_word\n",
    "        \n",
    "    return np.exp(sentence_loss/len_sen)\n",
    "\n",
    "\n",
    "def LM_score(source_word,source_context,substitution_selection,tokenizer,maskedLM):\n",
    "    #source_index = source_context.index(source_word)\n",
    "\n",
    "    source_sentence = ''\n",
    "\n",
    "    for context in source_context:\n",
    "        source_sentence += context + \" \"\n",
    "    \n",
    "    source_sentence = source_sentence.strip()\n",
    "    #print(source_sentence)\n",
    "    LM = []\n",
    "\n",
    "    source_loss = get_score(source_sentence,tokenizer,maskedLM)\n",
    "\n",
    "    for substibution in substitution_selection:\n",
    "        \n",
    "        sub_sentence = source_sentence.replace(source_word,substibution)\n",
    "\n",
    "        \n",
    "        #print(sub_sentence)\n",
    "        score = get_score(sub_sentence,tokenizer,maskedLM)\n",
    "\n",
    "        #print(score)\n",
    "        LM.append(score)\n",
    "\n",
    "    return LM,source_loss\n",
    "\n",
    "\n",
    "def preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count):\n",
    "    ss = []\n",
    "    ##ss_score=[]\n",
    "    sis_scores=[]\n",
    "    count_scores=[]\n",
    "\n",
    "    isFast = True\n",
    "\n",
    "    if(source_word not in fasttext_dico):\n",
    "        isFast = False\n",
    "    else:\n",
    "        source_emb = fasttext_emb[fasttext_dico.index(source_word)].reshape(1,-1)\n",
    "\n",
    "    #ss.append(source_word)\n",
    "\n",
    "    for sub in substitution_selection:\n",
    "\n",
    "        if sub not in word_count:\n",
    "            continue\n",
    "        else:\n",
    "            sub_count = word_count[sub]\n",
    "\n",
    "        if(sub_count<=3):\n",
    "            continue\n",
    "\n",
    "        #if sub_count<source_count:\n",
    "         #   continue\n",
    "        if isFast:\n",
    "            if sub not in fasttext_dico:\n",
    "                continue\n",
    "\n",
    "            token_index_fast = fasttext_dico.index(sub)\n",
    "            sis = cosine(source_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            #if sis<0.35:\n",
    "            #    continue\n",
    "            sis_scores.append(sis)\n",
    "\n",
    "        ss.append(sub)\n",
    "        count_scores.append(sub_count)\n",
    "\n",
    "    return ss,sis_scores,count_scores\n",
    "\n",
    "def compute_context_sis_score(source_word, sis_context, substitution_selection, fasttext_dico, fasttext_emb):\n",
    "    context_sis = []\n",
    "\n",
    "    word_context = []\n",
    "\n",
    "    \n",
    "\n",
    "    for con in sis_context:\n",
    "        if con==source_word or (con not in fasttext_dico):\n",
    "            continue\n",
    "\n",
    "        word_context.append(con)\n",
    "\n",
    "    if len(word_context)!=0:\n",
    "        for sub in substitution_selection:\n",
    "            sub_emb = fasttext_emb[fasttext_dico.index(sub)].reshape(1,-1)\n",
    "            all_sis = 0\n",
    "            for con in word_context:\n",
    "                token_index_fast = fasttext_dico.index(con)\n",
    "                all_sis += cosine(sub_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            context_sis.append(all_sis/len(word_context))\n",
    "    else:\n",
    "        for i in range(len(substitution_selection)):\n",
    "            context_sis.append(len(substitution_selection)-i)\n",
    "\n",
    "            \n",
    "    return context_sis\n",
    "\n",
    "\n",
    "def substitution_ranking(source_word, source_context, substitution_selection, fasttext_dico, fasttext_emb, word_count, ssPPDB, tokenizer, maskedLM, lables):\n",
    "\n",
    "    ss,sis_scores,count_scores=preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count)\n",
    "\n",
    "    #print(ss)\n",
    "    if len(ss)==0:\n",
    "        return source_word\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        seq = sorted(sis_scores,reverse = True )\n",
    "        sis_rank = [seq.index(v)+1 for v in sis_scores]\n",
    "    \n",
    "    rank_count = sorted(count_scores,reverse = True )\n",
    "    \n",
    "    count_rank = [rank_count.index(v)+1 for v in count_scores]\n",
    "  \n",
    "    lm_score,source_lm = LM_score(source_word,source_context,ss,tokenizer,maskedLM)\n",
    "\n",
    "    rank_lm = sorted(lm_score)\n",
    "    lm_rank = [rank_lm.index(v)+1 for v in lm_score]\n",
    "    \n",
    "\n",
    "    bert_rank = []\n",
    "    ppdb_rank =[]\n",
    "    for i in range(len(ss)):\n",
    "        bert_rank.append(i+1)\n",
    "\n",
    "        if ss[i] in ssPPDB:\n",
    "        \tppdb_rank.append(1)\n",
    "        else:\n",
    "        \tppdb_rank.append(len(ss)/3)\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        all_ranks = [bert+sis+count+LM+ppdb  for bert,sis,count,LM,ppdb in zip(bert_rank,sis_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    else:\n",
    "        all_ranks = [bert+count+LM+ppdb  for bert,count,LM,ppdb in zip(bert_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    #all_ranks = [con for con in zip(context_rank)]\n",
    "\n",
    "\n",
    "    pre_index = all_ranks.index(min(all_ranks))\n",
    "\n",
    "    #return ss[pre_index]\n",
    "\n",
    "    pre_count = count_scores[pre_index]\n",
    "\n",
    "    if source_word in word_count:\n",
    "    \tsource_count = word_count[source_word]\n",
    "    else:\n",
    "    \tsource_count = 0\n",
    "\n",
    "    pre_lm = lm_score[pre_index]\n",
    "\n",
    "    #print(lm_score)\n",
    "    #print(source_lm)\n",
    "    #print(pre_lm)\n",
    "\n",
    "\n",
    "    #pre_word = ss[pre_index]\n",
    "\n",
    "\n",
    "    if source_lm>pre_lm or pre_count>source_count:\n",
    "    \tpre_word = ss[pre_index]\n",
    "    else:\n",
    "    \tpre_word = source_word\n",
    "\n",
    "    \n",
    "    return pre_word\n",
    "\n",
    "\n",
    "def evaulation_SS_scores(ss,labels):\n",
    "    assert len(ss)==len(labels)\n",
    "\n",
    "    potential = 0\n",
    "    instances = len(ss)\n",
    "    precision = 0\n",
    "    precision_all = 0\n",
    "    recall = 0\n",
    "    recall_all = 0\n",
    "\n",
    "    for i in range(len(ss)):\n",
    "\n",
    "        one_prec = 0\n",
    "        \n",
    "        common = list(set(ss[i]).intersection(labels[i]))\n",
    "\n",
    "        if len(common)>=1:\n",
    "            potential +=1\n",
    "        precision += len(common)\n",
    "        recall += len(common)\n",
    "        precision_all += len(ss[i])\n",
    "        recall_all += len(labels[i])\n",
    "\n",
    "    potential /=  instances\n",
    "    precision /= precision_all\n",
    "    recall /= recall_all\n",
    "    F_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return potential,precision,recall,F_score\n",
    "\n",
    "\n",
    "def evaulation_pipeline_scores(substitution_words,source_words,gold_words):\n",
    "\n",
    "    instances = len(substitution_words)\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    changed_proportion = 0\n",
    "\n",
    "    for sub, source, gold in zip(substitution_words,source_words,gold_words):\n",
    "        if sub==source or (sub in gold):\n",
    "            precision += 1\n",
    "        if sub!=source and (sub in gold):\n",
    "            accuracy += 1\n",
    "        if sub!=source:\n",
    "            changed_proportion += 1\n",
    "\n",
    "    return precision/instances,accuracy/instances,changed_proportion/instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_context(words, mask_index, window):\n",
    "    #extract 7 words around the content word\n",
    "\n",
    "    length = len(words)\n",
    "\n",
    "    half = int(window/2)\n",
    "\n",
    "    assert mask_index>=0 and mask_index<length\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    if length<=window:\n",
    "        context = words\n",
    "    elif mask_index<length-half and mask_index>=half:\n",
    "        context = words[mask_index-half:mask_index+half+1]\n",
    "    elif mask_index<half:\n",
    "        context = words[0:window]\n",
    "    elif mask_index>=length-half:\n",
    "        context = words[length-window:length]\n",
    "    else:\n",
    "        print(\"Wrong!\")\n",
    "\n",
    "    return context\n",
    "\n",
    "def preprocess_tag(tag):\n",
    "    if tag[0] ==\"V\" or tag[0]==\"N\":\n",
    "        return tag\n",
    "    if tag[0]==\"R\":\n",
    "        return \"r\"\n",
    "    if tag[0]==\"J\" or tag[0]==\"I\":\n",
    "        return 'a'\n",
    "    else:\n",
    "        return 's'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--eval_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The evaluation data dir.\")\n",
    "parser.add_argument(\"--bert_model\", default=None, type=str,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "\n",
    "parser.add_argument(\"--output_SR_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The output directory of writing substitution selection.\")\n",
    "parser.add_argument(\"--word_embeddings\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word embeddings\")\n",
    "parser.add_argument(\"--word_frequency\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "    \n",
    "parser.add_argument(\"--ppdb\",\n",
    "                        default=\"./ppdb-2.0-tldr\",\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "\n",
    "parser.add_argument(\"--prob_mask\",\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of the masked words in first sentence. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "\n",
    "\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--num_selections\",\n",
    "                        default=20,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--num_eval_epochs\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_eval=True\n",
    "args.do_lower_case=True\n",
    "args.num_selections=20\n",
    "args.prob_mask=0.5\n",
    "args.eval_dir='D:/data/bert_ppdb/datasets/lex.mturk.txt'\n",
    "args.bert_model='bert-large-uncased-whole-word-masking'\n",
    "args.max_seq_length=250\n",
    "args.word_embeddings='D:/data/bert_ppdb/crawl-300d-2M-subword.vec'\n",
    "args.word_frequency='D:/data/bert_ppdb/SUBTLEX_frequency.xlsx'\n",
    "args.ppdb='D:/data/bert_ppdb/ppdb-2.0-tldr'\n",
    "args.output_SR_file='D:/data/bert_ppdb/results/NNSeval'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-vocab.txt\n",
      "INFO:pytorch_pretrained_bert.modeling:loading weights file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-pytorch_model.bin\n",
      "INFO:pytorch_pretrained_bert.modeling:loading configuration file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-config.json\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "if args.server_ip and args.server_port:\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not args.do_eval:\n",
    "    raise ValueError(\"At least `do_eval` must be True.\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "    \n",
    "\n",
    "# Prepare model\n",
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "model = BertForMaskedLM.from_pretrained(args.bert_model,output_attentions=True,cache_dir=cache_dir)\n",
    "if args.fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "\n",
    "output_sr_file = open(args.output_SR_file,\"a+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings ...\n",
      "2000000 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embeddings ...\")\n",
    "\n",
    "wordVecPath = args.word_embeddings\n",
    "#wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\"\n",
    "\n",
    "fasttext_dico, fasttext_emb = getWordmap(wordVecPath)\n",
    "\n",
    "#stopword = set(stopwords.words('english'))\n",
    "word_count_path = args.word_frequency\n",
    "#word_count_path = \"word_frequency_wiki.txt\"\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PPDB ...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading PPDB ...\")\n",
    "ppdb_path = args.ppdb\n",
    "ppdb_model = Ppdb(ppdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "第  0 次循环\n",
      "prob_mask: 0.5\n",
      "Sentence 0 rankings: \n",
      " sentence:  \"In March 1992 , Linux version 0.95 was the first to be capable of running X. This large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent .\"\n",
      "mask_words  pieces\n",
      "cgBERT  ['parts', 'components', 'fragments', 'elements', 'sections', 'tiles', 'puzzles', 'devices', 'details', 'chunks', 'items', 'things', 'bits', 'phrases', 'members', 'portions', 'chips', 'objects', 'changes', 'parameters']\n",
      "Sentence 1 rankings: \n",
      " sentence:  Much of the water carried by these streams is diverted .\n",
      "mask_words  diverted\n",
      "cgBERT  ['reclaimed', 'displaced', 'routed', 'transferred', 'derived', 'pumped', 'sourced', 'severed', 'shifted', 'converted', 'retained', 'bypassed', 'wasted', 'relocated', 'prescribed', 'discontinued', 'directed', 'diminished', 'drained', 'damaged']\n",
      "Sentence 2 rankings: \n",
      " sentence:  \"Harry also becomes the worthy possessor of the remaining Deathly Hallows : the Invisibility Cloak and the Resurrection Stone , hence becoming the true Master of Death .\"\n",
      "mask_words  possessor\n",
      "cgBERT  ['owner', 'bearer', 'host', 'recipient', 'holder', 'inventor', 'proprietor', 'tenant', 'master', 'incumbent', 'carrier', 'heir', 'user', 'successor', 'acquire', 'vessel', 'landowner', 'mistress', 'manifest', 'provider']\n",
      "Sentence 3 rankings: \n",
      " sentence:  \"Escapologists escape from handcuffs , straitjackets , cages , coffins , steel boxes , barrels , bags , burning buildings , fish-tanks and other perils , often in combination .\"\n",
      "mask_words  perils\n",
      "cgBERT  ['dangers', 'risks', 'devices', 'parts', 'precautions', 'traps', 'complications', 'disasters', 'rewards', 'danger', 'charges', 'problems', 'difficulties', 'forms', 'providers', 'tests', 'places', 'themes', 'methods', 'crimes']\n",
      "Sentence 4 rankings: \n",
      " sentence:  \"The storm continued , crossing the Outer Banks of North Carolina , and retained its strength until June 20 when it became extratropical near Newfoundland .\"\n",
      "mask_words  retained\n",
      "cgBERT  ['maintained', 'remained', 'kept', 'conserved', 'sustained', 'exhibited', 'continued', 'held', 'preserved', 'maintain', 'regained', 'renewed', 'registered', 'reiterated', 'defended', 'demonstrated', 'gained', 'persisted', 'upheld', 'lodged']\n",
      "Sentence 5 rankings: \n",
      " sentence:  The Convent has been the official residence of the Governor of Gibraltar since 1728 .\n",
      "mask_words  residence\n",
      "cgBERT  ['home', 'seat', 'mansion', 'house', 'pursuit', 'dwelling', 'possession', 'housing', 'palace', 'retreat', 'place', 'driveway', 'property', 'homestead', 'household', 'refuge', 'headquarters', 'site', 'haven', 'location']\n",
      "Sentence 6 rankings: \n",
      " sentence:  Food is procured with its suckers and then crushed using its tough `` beak '' of chitin .\n",
      "mask_words  procured\n",
      "cgBERT  ['obtained', 'prepared', 'supplied', 'secured', 'purchased', 'generated', 'sourced', 'harvested', 'gathered', 'grown', 'derived', 'cultivated', 'screened', 'supplemented', 'acquired', 'sought', 'selected', 'plucked', 'picked', 'furnished']\n",
      "Sentence 7 rankings: \n",
      " sentence:  \"The United States convened a 13-nation conference of the International Opium Commission in 1909 in Shanghai , China in response to increasing criticism of the opium trade .\"\n",
      "mask_words  convened\n",
      "cgBERT  ['hosted', 'summoned', 'scheduled', 'instituted', 'chaired', 'commissioned', 'assembled', 'presided', 'held', 'led', 'petitioned', 'prompted', 'staged', 'gathered', 'mandated', 'organized', 'mobilized', 'compiled', 'designated', 'commanded']\n",
      "Sentence 8 rankings: \n",
      " sentence:  Photosynthesis is vital for life on Earth .\n",
      "mask_words  vital\n",
      "cgBERT  ['essential', 'crucial', 'fundamental', 'critical', 'key', 'valuable', 'important', 'necessary', 'basic', 'relevant', 'beneficial', 'significant', 'central', 'useful', 'sacred', 'integral', 'dramatic', 'useless', 'dynamic', 'strategic']\n",
      "Sentence 9 rankings: \n",
      " sentence:  \"Dodd simply retained his athletic director position , which he had acquired in 1950 .\"\n",
      "mask_words  acquired\n",
      "cgBERT  ['obtained', 'gained', 'purchased', 'attained', 'secured', 'earned', 'accumulated', 'received', 'amassed', 'settled', 'established', 'bought', 'instituted', 'inherited', 'retained', 'adopted', 'developed', 'garnered', 'achieved', 'assumed']\n",
      "Sentence 10 rankings: \n",
      " sentence:  \"Radiometric dating is a technique used to date materials , usually based on a comparison between the observed abundance of a naturally occurring radioactive isotope and its decay products , using known decay rates .\"\n",
      "mask_words  abundance\n",
      "cgBERT  ['occurrence', 'amount', 'prevalence', 'presence', 'extinction', 'quantity', 'availability', 'existence', 'accumulation', 'dominance', 'intensity', 'population', 'appearance', 'inclusion', 'ancestry', 'importance', 'concentration', 'equivalent', 'activity', 'evidence']\n",
      "Sentence 11 rankings: \n",
      " sentence:  \"Bacterial contaminants are ubiquitous , and foods left unused too long will often acquire substantial amounts of bacterial colonies and become dangerous to eat , leading to food poisoning .\"\n",
      "mask_words  substantial\n",
      "cgBERT  ['significant', 'large', 'considerable', 'extensive', 'massive', 'sizable', 'larger', 'sufficient', 'vast', 'excessive', 'heavy', 'tremendous', 'prominent', 'powerful', 'additional', 'lengthy', 'certain', 'moderate', 'numerous', 'greater']\n",
      "Sentence 12 rankings: \n",
      " sentence:  \"Tibooburra has an arid , desert climate with temperatures soaring above 40 Celsius in summer , often reaching as high as 47 C .\"\n",
      "mask_words  arid\n",
      "cgBERT  ['extreme', 'harsh', 'abrupt', 'exotic', 'dry', 'oppressive', 'isolated', 'alternative', 'ancient', 'ambiguous', 'desert', 'open', 'alternate', 'energetic', 'authoritarian', 'acute', 'urban', 'active', 'uncomfortable', 'old']\n",
      "Sentence 13 rankings: \n",
      " sentence:  \"Hanna meandered around the southeastern Bahamas , weakening to a tropical storm while also dumping heavy rain on already-devastated Haiti .\"\n",
      "mask_words  meandered\n",
      "cgBERT  ['wandered', 'drifted', 'curved', 'raced', 'travelled', 'tracked', 'flowed', 'traveled', 'slipped', 'continued', 'floated', 'moved', 'circled', 'traced', 'curled', 'ran', 'twisted', 'swirled', 'slid', 'rolled']\n",
      "Sentence 14 rankings: \n",
      " sentence:  \"With the high Gulf pressures - a ship reported a pressure of 1015.5 millibars less than 60 m from the storm center at the time it was upgraded to a tropical storm - Alicia was unable to gain size , staying very small , but generated faster winds , and became a Category 1 hurricane on August 16\"\n",
      "mask_words  generated\n",
      "cgBERT  ['provided', 'produced', 'created', 'spawned', 'caused', 'inspired', 'collected', 'sparked', 'issued', 'emitted', 'achieved', 'erupted', 'released', 'garnered', 'presented', 'gave', 'encountered', 'maintained', 'contributed', 'had']\n",
      "Sentence 15 rankings: \n",
      " sentence:  \"Das Rheingold is the first of the four operas that comprise Der Ring des Nibelungen , by Richard Wagner .\"\n",
      "mask_words  comprise\n",
      "cgBERT  ['constitute', 'contain', 'collect', 'encompass', 'form', 'consist', 'provide', 'include', 'construct', 'make', 'gather', 'consume', 'assemble', 'aggregate', 'mesh', 'require', 'establish', 'become', 'depict', 'occur']\n",
      "Sentence 16 rankings: \n",
      " sentence:  A frenulum is a small fold of tissue that secures or restricts the motion of a mobile organ in the body .\n",
      "mask_words  secures\n",
      "cgBERT  ['holds', 'supports', 'anchors', 'guarantees', 'maintains', 'carries', 'ties', 'ensures', 'captures', 'facilitates', 'binds', 'contains', 'protects', 'confines', 'provides', 'establishes', 'closes', 'catches', 'retains', 'stays']\n",
      "Sentence 17 rankings: \n",
      " sentence:  \"Helen Hunt has been recognized extensively in her career . In 1998 she become the second actress to win a Golden Globe Award , an Academy Award and an Emmy Award in the same year .\"\n",
      "mask_words  career\n",
      "cgBERT  ['life', 'lifetime', 'success', 'history', 'work', 'comeback', 'childhood', 'retirement', 'performance', 'relationship', 'profession', 'role', 'industry', 'talent', 'service', 'business', 'activity', 'period', 'year', 'time']\n",
      "Sentence 18 rankings: \n",
      " sentence:  \"When Wotan refuses to abandon his `` free hero '' , Fricka lays bare his self-deception : Siegmund is in no sense independent since his fate has been pre-ordained by Wotan , who has even indirectly led him to find the magic sword .\"\n",
      "mask_words  refuses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgBERT  ['declines', 'threatens', 'insists', 'decides', 'continues', 'leaves', 'chooses', 'fails', 'yields', 'agrees', 'denies', 'demands', 'begins', 'accepts', 'offers', 'seems', 'stops', 'tries', 'returns', 'asks']\n",
      "Sentence 19 rankings: \n",
      " sentence:  \"The storm never approached land during its lifespan , and no damage or casualties were reported .\"\n",
      "mask_words  casualties\n",
      "cgBERT  ['losses', 'fatalities', 'deaths', 'injuries', 'civilians', 'incidents', 'consequences', 'victims', 'inhabitants', 'labourers', 'ratings', 'survivors', 'occupants', 'symptoms', 'developments', 'events', 'memorials', 'outcomes', 'disturbances', 'divisions']\n",
      "Sentence 20 rankings: \n",
      " sentence:  \"The purpose of public speaking can range from simply transmitting information , to motivating people to act , to simply telling a story .\"\n",
      "mask_words  transmitting\n",
      "cgBERT  ['broadcasting', 'delivering', 'sending', 'communicating', 'providing', 'conducting', 'distributing', 'passing', 'displaying', 'presenting', 'bringing', 'receiving', 'relay', 'carrying', 'reporting', 'supplying', 'telling', 'releasing', 'performing', 'exchanging']\n",
      "Sentence 21 rankings: \n",
      " sentence:  \"Published by Tor Books , it was released on August 15 , 1994 in hardcover , and in paperback on July 15 , 1997 .\"\n",
      "mask_words  released\n",
      "cgBERT  ['published', 'opened', 'issued', 'launched', 'sold', 'announced', 'available', 'introduced', 'premiered', 'written', 'discovered', 'stated', 'licensed', 'distributed', 'out', 'located', 'born', 'reissued', 'revealed', 'reviewed']\n",
      "Sentence 22 rankings: \n",
      " sentence:  \"In many circumstances , these investments can be less expensive than fossil fuel energy systems .\"\n",
      "mask_words  circumstances\n",
      "cgBERT  ['conditions', 'situations', 'occasions', 'cases', 'contexts', 'surroundings', 'instances', 'times', 'environments', 'situation', 'scenarios', 'senses', 'events', 'manners', 'soils', 'factors', 'arrangements', 'configurations', 'occurrences', 'combinations']\n",
      "Sentence 23 rankings: \n",
      " sentence:  \"Following the death of Schidlof from a heart attack in 1987 , the Amadeus Quartet disbanded .\"\n",
      "mask_words  disbanded\n",
      "cgBERT  ['ceased', 'folded', 'retired', 'closed', 'formed', 'terminated', 'faltered', 'resigned', 'withdrew', 'reformed', 'collapsed', 'vacated', 'divorced', 'departed', 'concluded', 'contracted', 'died', 'split', 'reunited', 'scrapped']\n",
      "Sentence 24 rankings: \n",
      " sentence:  \"By 1960 he had developed the short story into a screenplay , and envisaged it as containing a suitable role for Monroe .\"\n",
      "mask_words  envisaged\n",
      "cgBERT  ['imagined', 'regarded', 'anticipated', 'contemplated', 'intended', 'realised', 'arranged', 'conceived', 'considered', 'pictured', 'assessed', 'planned', 'indicated', 'identified', 'recognised', 'organised', 'interpreted', 'examined', 'illustrated', 'modelled']\n",
      "Sentence 25 rankings: \n",
      " sentence:  \"With poor sales of the A340-200 , Airbus decided to use the fuselage of the A340-200 with the wings and engines of the A330-300 .\"\n",
      "mask_words  fuselage\n",
      "cgBERT  ['chassis', 'body', 'layout', 'backbone', 'fabric', 'hull', 'structure', 'cockpit', 'platform', 'torso', 'tube', 'framework', 'frame', 'cabin', 'cab', 'wreckage', 'spectrum', 'skin', 'skeleton', 'shape']\n",
      "Sentence 26 rankings: \n",
      " sentence:  \"The last event was held on June 11 , 2000 , not to be held again due to the acquisition of WCW by World Wrestling Federation .\"\n",
      "mask_words  acquisition\n",
      "cgBERT  ['purchase', 'ownership', 'absorption', 'interception', 'takeover', 'enrollment', 'administration', 'invasion', 'unification', 'purchasing', 'sale', 'owner', 'assumption', 'consolidation', 'establishment', 'inheritance', 'agreement', 'merger', 'annexation', 'occupation']\n",
      "Sentence 27 rankings: \n",
      " sentence:  Kowal suggested the name and the IAU endorsed it in 1975 .\n",
      "mask_words  endorsed\n",
      "cgBERT  ['supported', 'adopted', 'approved', 'accepted', 'sponsored', 'embraced', 'backed', 'sanctioned', 'ratified', 'empowered', 'upheld', 'affiliated', 'advocated', 'accredited', 'signed', 'mandated', 'praised', 'implemented', 'honored', 'favored']\n",
      "Sentence 28 rankings: \n",
      " sentence:  Dry air wrapping around the southern periphery of the cyclone eroded most of the deep convection by early on September 12 .\n",
      "mask_words  periphery\n",
      "cgBERT  ['rim', 'fringe', 'apex', 'edge', 'edges', 'outskirts', 'verge', 'coastline', 'nucleus', 'bulge', 'verandah', 'envelope', 'quadrant', 'boundary', 'margin', 'heartland', 'core', 'vicinity', 'lip', 'outline']\n",
      "Sentence 29 rankings: \n",
      " sentence:  \"Brief additional internal links are generally tolerated when used to facilitate communication or to provide general information , but undesirable if seen as canvassing for some purpose .\"\n",
      "mask_words  tolerated\n",
      "cgBERT  ['permitted', 'forgiven', 'facilitated', 'preserved', 'acceptable', 'accepted', 'encouraged', 'allowed', 'trusted', 'sustained', 'declined', 'spared', 'survived', 'endured', 'retained', 'recognized', 'enjoyed', 'respected', 'valued', 'acknowledged']\n",
      "Sentence 30 rankings: \n",
      " sentence:  \"Alfonso drops the marriage contract in front of the officers , and , when they read it , they become enraged .\"\n",
      "mask_words  enraged\n",
      "cgBERT  ['furious', 'outraged', 'angry', 'angered', 'appalled', 'crazed', 'aggravated', 'upset', 'alarmed', 'excited', 'agitated', 'murderous', 'impressed', 'ill', 'stricken', 'impatient', 'aroused', 'astonished', 'obsessed', 'horrified']\n",
      "Sentence 31 rankings: \n",
      " sentence:  \"The Overland Expedition , also called the Overland Relief Expedition or Point Barrow-Overland Relief Expedition , was an expedition in the winter of 1897-1898 by officers of the United States Revenue Cutter Service to save the lives of 265 whalers trapped in the Arctic Ocean by ice around their ships near Point Barrow , Alaska .\"\n",
      "mask_words  expedition\n",
      "cgBERT  ['effort', 'mission', 'operation', 'excursion', 'enterprise', 'voyage', 'trip', 'campaign', 'attempt', 'project', 'episode', 'affair', 'incident', 'journey', 'initiative', 'adventure', 'event', 'accident', 'investigation', 'endeavor']\n",
      "Sentence 32 rankings: \n",
      " sentence:  \"Located on the River Pedieos and situated almost in the center of the island , it is the seat of government as well as the main business center .\"\n",
      "mask_words  situated\n",
      "cgBERT  ['positioned', 'placed', 'located', 'lodged', 'constructed', 'thrown', 'fitted', 'erected', 'realised', 'established', 'perched', 'found', 'centred', 'laid', 'characterised', 'secured', 'taken', 'whilst', 'constituted', 'neighbourhood']\n",
      "Sentence 33 rankings: \n",
      " sentence:  \"Iero thought that it would be difficult to promote the album himself , if released through his own label , with his current obligations to My Chemical Romance and the other bands signed to his label .\"\n",
      "mask_words  obligations\n",
      "cgBERT  ['responsibilities', 'commitments', 'duties', 'requirements', 'ties', 'contracts', 'debts', 'payments', 'allegiance', 'constraints', 'offerings', 'commitment', 'appointments', 'demands', 'vows', 'aspirations', 'fixtures', 'engagements', 'roles', 'assets']\n",
      "Sentence 34 rankings: \n",
      " sentence:  \"It has now been determined that Europa is a prograde rotator , but the exact direction in which its pole points remains ambiguous .\"\n",
      "mask_words  ambiguous\n",
      "cgBERT  ['uncertain', 'obscure', 'unclear', 'vague', 'unspecified', 'oblique', 'unsure', 'undisclosed', 'unknown', 'arbitrary', 'cryptic', 'elusive', 'binary', 'undeveloped', 'controversial', 'conditional', 'inconsistent', 'fluid', 'offensive', 'indefinite']\n",
      "Sentence 35 rankings: \n",
      " sentence:  \"The Bishops ceded the Oversticht to the Emperor Charles V in 1528 , who styled himself `` Lord of Overijssel '' , thereby giving the province its modern name .\"\n",
      "mask_words  ceded\n",
      "cgBERT  ['surrendered', 'pledged', 'transferred', 'relinquished', 'conceded', 'donated', 'leased', 'yielded', 'submitted', 'bequeathed', 'rendered', 'sold', 'granted', 'resigned', 'ratified', 'conveyed', 'delivered', 'gave', 'loaned', 'subdued']\n",
      "Sentence 36 rankings: \n",
      " sentence:  \"Many parts of Odessa were damaged during its siege and recapture on 10 April 1944 , when the city was finally liberated by the Red Army .\"\n",
      "mask_words  siege\n",
      "cgBERT  ['garrison', 'blockade', 'capture', 'bombardment', 'besieged', 'conquest', 'surrender', 'campaign', 'challenge', 'captivity', 'defense', 'lease', 'recapture', 'shell', 'rescue', 'occupation', 'defence', 'prison', 'imprisonment', 'battle']\n",
      "Sentence 37 rankings: \n",
      " sentence:  \"The city was first founded by the British in 1827 , who leased the island from Spain during the colonial period .\"\n",
      "mask_words  leased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgBERT  ['rented', 'licensed', 'contracted', 'borrowed', 'sampled', 'ceded', 'furnished', 'occupied', 'loaned', 'rent', 'secured', 'seized', 'appropriated', 'purchased', 'reclaimed', 'reserved', 'obtained', 'shared', 'chartered', 'took']\n",
      "Sentence 38 rankings: \n",
      " sentence:  \"In reference to the landscape , bush describes a wooded area , intermediate between a shrubland and a forest , generally of dry and nitrogen-poor soil , mostly grassless , thin to thick woody shrubs and bushes , under a sparse canapy of eucalypts .\"\n",
      "mask_words  sparse\n",
      "cgBERT  ['dense', 'scarce', 'loose', 'thick', 'plump', 'thin', 'faint', 'scattered', 'brief', 'minimal', 'heavy', 'few', 'careless', 'partial', 'lush', 'crisp', 'residual', 'marginal', 'handful', 'slight']\n",
      "Sentence 39 rankings: \n",
      " sentence:  \"The theory of incentive compatibility that Hurwicz developed changed the way many economists thought about outcomes , explaining why centrally planned economies may fail and how incentives for individuals make a difference in decision making .\"\n",
      "mask_words  incentives\n",
      "cgBERT  ['opportunities', 'rewards', 'outcomes', 'alternatives', 'advantages', 'attractions', 'subsidies', 'activities', 'initiatives', 'efforts', 'objectives', 'contributions', 'punishments', 'constraints', 'aids', 'innovations', 'gifts', 'consequences', 'influences', 'institutions']\n",
      "Sentence 40 rankings: \n",
      " sentence:  \"The axe , or ax , is an implement that has been used for millennia to shape , split and cut wood , harvest timber , as a weapon and a ceremonial or heraldic symbol . The axe has many forms and specialized uses but generally consists of an axe head with a handle , or helve .\"\n",
      "mask_words  implement\n",
      "cgBERT  ['instrument', 'object', 'equipment', 'item', 'tool', 'axe', 'expression', 'article', 'apparatus', 'accessory', 'assembly', 'artifact', 'application', 'attribute', 'indication', 'indicator', 'ease', 'ax', 'entity', 'invention']\n",
      "Sentence 41 rankings: \n",
      " sentence:  But critics note that Francis Galton did not advocate coercion when he defined the principles of eugenics .\n",
      "mask_words  advocate\n",
      "cgBERT  ['offer', 'encourage', 'support', 'approach', 'allow', 'employ', 'oppose', 'propose', 'attempt', 'exhibit', 'associate', 'suggest', 'represent', 'use', 'promote', 'issue', 'embrace', 'aim', 'expect', 'practice']\n",
      "Sentence 42 rankings: \n",
      " sentence:  \"After its sale in 1998 , it merged into Festival Mushroom Records .\"\n",
      "mask_words  merged\n",
      "cgBERT  ['amalgamated', 'integrated', 'combined', 'blended', 'consolidated', 'developed', 'folded', 'linked', 'unified', 'dissolved', 'joined', 'changed', 'incorporated', 'fused', 'formed', 'transformed', 'mixed', 'absorbed', 'reorganized', 'disappeared']\n",
      "Sentence 43 rankings: \n",
      " sentence:  Cobra and Tango and Cash did solid business domestically but overseas they did blockbuster business grossing over $ 100 million in foreign markets and over $ 160 million worldwide .\n",
      "mask_words  grossing\n",
      "cgBERT  ['earning', 'netting', 'scoring', 'totaling', 'billing', 'investing', 'costing', 'purchasing', 'spending', 'generating', 'averaging', 'securing', 'gaining', 'reaching', 'numbering', 'making', 'producing', 'harvesting', 'releasing', 'sponsoring']\n",
      "Sentence 44 rankings: \n",
      " sentence:  \"Triangles can also be classified according to their internal angles , measured here in degrees .\"\n",
      "mask_words  classified\n",
      "cgBERT  ['subdivided', 'categorized', 'differentiated', 'graded', 'grouped', 'divided', 'described', 'regarded', 'segregated', 'sorted', 'separated', 'defined', 'designated', 'determined', 'categories', 'identified', 'constructed', 'recognized', 'considered', 'distinguished']\n",
      "Sentence 45 rankings: \n",
      " sentence:  \"Later describing the experience as a `` nightmare , '' she was able to phone Joe DiMaggio from the clinic , who immediately traveled from Florida to New York to facilitate her transfer to the Columbia Presbyterian Medical Center , where she remained for three weeks .\"\n",
      "mask_words  facilitate\n",
      "cgBERT  ['permit', 'accommodate', 'ensure', 'secure', 'support', 'provide', 'achieve', 'coordinate', 'ease', 'accelerate', 'assure', 'schedule', 'handle', 'enable', 'receive', 'administer', 'convenience', 'accomplish', 'allow', 'stabilize']\n",
      "Sentence 46 rankings: \n",
      " sentence:  \"The Pennines constitute the main watershed in northern England , dividing the eastern and western parts of the country .\"\n",
      "mask_words  constitute\n",
      "cgBERT  ['comprise', 'are', 'form', 'compose', 'represent', 'encompass', 'remain', 'possess', 'occupy', 'resemble', 'provide', 'become', 'occur', 'serve', 'engage', 'reside', 'retain', 'commence', 'distinguish', 'acquire']\n",
      "Sentence 47 rankings: \n",
      " sentence:  \"They locate food by smell , using sensors in the tip of their snout , and regularly feast on ants and termites .\"\n",
      "mask_words  snout\n",
      "cgBERT  ['nose', 'muzzle', 'beak', 'skull', 'tail', 'forehead', 'noses', 'nostrils', 'nasal', 'head', 'hood', 'nosed', 'lance', 'mouth', 'neck', 'chin', 'tails', 'tongue', 'backside', 'butt']\n",
      "Sentence 48 rankings: \n",
      " sentence:  \"At present it is formed by the Aa , which descends from the Rigi and enters the southern extremity of the lake .\"\n",
      "mask_words  extremity\n",
      "cgBERT  ['end', 'part', 'tip', 'edge', 'limit', 'arm', 'boundary', 'entrance', 'shore', 'section', 'portion', 'side', 'area', 'half', 'point', 'terminus', 'rim', 'limb', 'border', 'corner']\n",
      "Sentence 49 rankings: \n",
      " sentence:  \"Specialized English operates in the civil society sector , and the developers aspire to make programs for a variety of public service purposes , subject to resources being available .\"\n",
      "mask_words  aspire\n",
      "cgBERT  ['as', 'strive', 'aim', 'claim', 'argue', 'rise', 'endeavor', 'are', 'exist', 'avid', 'attempt', 'imagine', 'continue', 'attain', 'seem', 'desire', 'expect', 'undertake', 'afford', 'endeavour']\n",
      "Sentence 50 rankings: \n",
      " sentence:  \"In this case , a relatively large kernel with sophisticated capabilities is adapted to suit an embedded environment .\"\n",
      "mask_words  sophisticated\n",
      "cgBERT  ['advanced', 'complex', 'complicated', 'elaborate', 'rigorous', 'modern', 'profound', 'precise', 'specialist', 'specialized', 'refined', 'powerful', 'primitive', 'detailed', 'severe', 'innovative', 'modernized', 'specialised', 'diversified', 'exotic']\n",
      "Sentence 51 rankings: \n",
      " sentence:  This period spanned the years from 1278 through 1288 .\n",
      "mask_words  spanned\n",
      "cgBERT  ['encompassed', 'stretched', 'covered', 'crossed', 'reached', 'embraced', 'merged', 'wiped', 'bordered', 'lasted', 'across', 'narrowed', 'squeezed', 'leapt', 'dominated', 'swept', 'bounded', 'started', 'tackled', 'linked']\n",
      "Sentence 52 rankings: \n",
      " sentence:  \"Realising that the gang could not elude the police forever , Moondyne Joe formulated a plan to escape the colony by traveling overland to the colony of South Australia .\"\n",
      "mask_words  elude\n",
      "cgBERT  ['evade', 'escape', 'el', 'avoid', 'dodge', 'eva', 'flee', 'ignore', 'escaping', 'beat', 'escaped', 'foil', 'fool', 'bypass', 'resist', 'satisfy', 'escapes', 'hide', 'find', 'chase']\n",
      "Sentence 53 rankings: \n",
      " sentence:  \"Southeastern Oklahoma , also known by its official tourism designation , Kiamichi Country , encompasses the southeastern quarter of the state of Oklahoma .\"\n",
      "mask_words  encompasses\n",
      "cgBERT  ['incorporates', 'comprises', 'occupies', 'constitutes', 'includes', 'surrounds', 'covers', 'contains', 'captures', 'addresses', 'is', 'spans', 'represents', 'preserves', 'involves', 'expands', 'impacts', 'extends', 'defines', 'concludes']\n",
      "Sentence 54 rankings: \n",
      " sentence:  The A-train family comprises trains for both commuter services and limited express services .\n",
      "mask_words  comprises\n",
      "cgBERT  ['includes', 'consists', 'incorporates', 'contains', 'provides', 'encompasses', 'involves', 'collects', 'occupies', 'employs', 'covers', 'is', 'denotes', 'forms', 'concerns', 'produces', 'constitutes', 'editions', 'depicts', 'realises']\n",
      "Sentence 55 rankings: \n",
      " sentence:  \"In DNA , adenine binds to thymine via two hydrogen bonds to assist in stabilizing the nucleic acid structures .\"\n",
      "mask_words  binds\n",
      "cgBERT  ['connects', 'ties', 'bonds', 'coordinates', 'joins', 'belongs', 'reacts', 'corresponds', 'relates', 'commits', 'refers', 'maps', 'links', 'engages', 'holds', 'compounds', 'conducts', 'sinks', 'adds', 'contracts']\n",
      "Sentence 56 rankings: \n",
      " sentence:  \"In 2001 , UNESCO inscribed the 2,750-year-old city on the World Heritage List as Samarkand - Crossroads of Cultures .\"\n",
      "mask_words  inscribed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgBERT  ['listed', 'designated', 'established', 'engraved', 'identified', 'incorporated', 'inaugurated', 'elected', 'entered', 'included', 'nominated', 'assigned', 'added', 'awarded', 'integrated', 'earned', 'etched', 'declared', 'engaged', 'placed']\n",
      "Sentence 57 rankings: \n",
      " sentence:  \"Radames affirms that Aida is the person he will marry , and Aida convinces him to flee to the desert with her .\"\n",
      "mask_words  affirms\n",
      "cgBERT  ['agrees', 'asserts', 'confirms', 'insists', 'declares', 'maintains', 'states', 'acknowledges', 'concludes', 'accepts', 'announces', 'decides', 'implies', 'believes', 'establishes', 'says', 'guarantees', 'emphasizes', 'proves', 'stresses']\n",
      "Sentence 58 rankings: \n",
      " sentence:  \"Like many Bolshevik artists , Eisenstein envisioned a new society which would subsidize artists totally , freeing them from the confines of bosses and budgets , leaving them absolutely free to create , but budgets and producers were as significant to the Soviet film industry as the rest of the world .\"\n",
      "mask_words  envisioned\n",
      "cgBERT  ['imagined', 'anticipated', 'proclaimed', 'depicted', 'dreamed', 'inhabited', 'intended', 'contemplated', 'conceived', 'proposed', 'projected', 'expected', 'depicts', 'viewed', 'predicted', 'desired', 'promised', 'described', 'advocated', 'epic']\n",
      "Sentence 59 rankings: \n",
      " sentence:  \"Forming in the western Caribbean sea during the first week of November , Hurricane Gordon became the twelfth tropical depression , the seventh named tropical storm , and the third hurricane of the 1994 Atlantic hurricane season . An erratic , long-lived system which remained a tropical storm for most of its existence , it followed a winding path through the western Caribbean and into Florida before strengthening into a Category 1 hurricane and threatening North Carolina .\"\n",
      "mask_words  erratic\n",
      "cgBERT  ['irregular', 'eccentric', 'unconventional', 'intermittent', 'unpredictable', 'impromptu', 'energetic', 'improvised', 'imaginative', 'intermediate', 'inconsistent', 'elusive', 'orderly', 'intensive', 'agitated', 'experimental', 'unreliable', 'arbitrary', 'unusual', 'ardent']\n",
      "Sentence 60 rankings: \n",
      " sentence:  \"The latter means basic or radical change ; whereas reform may be no more than fine tuning , or at most redressing serious wrongs without altering the fundamentals of the system .\"\n",
      "mask_words  altering\n",
      "cgBERT  ['changing', 'modifying', 'affecting', 'adjusting', 'improving', 'manipulating', 'disturbing', 'examining', 'shifting', 'violating', 'evolving', 'adapting', 'influencing', 'increasing', 'cutting', 'enhancing', 'addressing', 'extending', 'upgrading', 'abandoning']\n",
      "Sentence 61 rankings: \n",
      " sentence:  \"The Kyoto Protocol was adopted at the third session of the Conference of Parties to the UNFCCC in 1997 in Kyoto , Japan .\"\n",
      "mask_words  adopted\n",
      "cgBERT  ['ratified', 'accepted', 'agreed', 'established', 'approved', 'passed', 'developed', 'chosen', 'taken', 'issued', 'introduced', 'implemented', 'held', 'endorsed', 'enacted', 'decided', 'concluded', 'negotiated', 'signed', 'resolved']\n",
      "Sentence 62 rankings: \n",
      " sentence:  \"From 1885 to 1901 , locomotives stopped at Young Tank to replenish their water .\"\n",
      "mask_words  replenish\n",
      "cgBERT  ['conserve', 'supply', 'maintain', 'feed', 'supplement', 'renew', 'fill', 'reclaim', 'empty', 'sustain', 'reserve', 'store', 'gather', 'recover', 'provide', 'increase', 'relieve', 'exercise', 'keep', 'obtain']\n",
      "Sentence 63 rankings: \n",
      " sentence:  Stone floor tiles tend to be heavier than ceramic tiles and somewhat more prone to breakage during shipment .\n",
      "mask_words  prone\n",
      "cgBERT  ['susceptible', 'likely', 'liable', 'robust', 'frequent', 'subject', 'resistant', 'costly', 'vulnerable', 'difficult', 'unlikely', 'readily', 'sensitive', 'due', 'related', 'tendency', 'common', 'reliable', 'yielding', 'tender']\n",
      "Sentence 64 rankings: \n",
      " sentence:  \"To enjoy qin songs , one must learn to become accustomed to the eccentric style some players may sing their songs to , like in the case of Zha Fuxi .\"\n",
      "mask_words  eccentric\n",
      "cgBERT  ['erratic', 'outrageous', 'bizarre', 'absurd', 'unconventional', 'unusual', 'irregular', 'authoritarian', 'satirical', 'archaic', 'artistic', 'insane', 'upright', 'extraordinary', 'invalid', 'innovative', 'extravagant', 'eclectic', 'uncommon', 'odd']\n",
      "Sentence 65 rankings: \n",
      " sentence:  A voluntary association or union is a group of individuals who voluntarily enter into an agreement to form a body to accomplish a purpose .\n",
      "mask_words  accomplish\n",
      "cgBERT  ['achieve', 'implement', 'fulfill', 'serve', 'attain', 'realize', 'perform', 'pursue', 'conduct', 'complete', 'satisfy', 'deliver', 'honor', 'tackle', 'meet', 'manage', 'execute', 'obtain', 'succeed', 'facilitate']\n",
      "Sentence 66 rankings: \n",
      " sentence:  \"It is the teaching that , as a consequence of the Fall of Man , every person born into the world is enslaved to the service of sin and , apart from the efficacious or prevenient grace of God , is utterly unable to choose to follow God or choose to accept salvation as it is freely offered .\"\n",
      "mask_words  efficacious\n",
      "cgBERT  ['benevolent', 'inspired', 'irresistible', 'abundant', 'exemplary', 'inspiration', 'operative', 'energetic', 'inspiring', 'ubiquitous', 'empowered', 'authoritative', 'enabling', 'miraculous', 'inspirational', 'extraordinary', 'helpful', 'essential', 'exclusive', 'generous']\n",
      "Sentence 67 rankings: \n",
      " sentence:  He was kidnapped and held by the notorious Pirate Peter Easton in Harbour Grace .\n",
      "mask_words  notorious\n",
      "cgBERT  ['famous', 'ruthless', 'famed', 'infamous', 'outlawed', 'legendary', 'vicious', 'treacherous', 'renegade', 'reputed', 'popular', 'controversial', 'titular', 'criminal', 'royal', 'celebrated', 'reformed', 'scottish', 'furious', 'dangerous']\n",
      "Sentence 68 rankings: \n",
      " sentence:  \"As a result of this acquisition and the legal dispute with Hearst Corporation , Mandrakesoft announced that the new company name would be Mandriva , and that Mandriva Linux would be the new name covering products .\"\n",
      "mask_words  dispute\n",
      "cgBERT  ['fight', 'conflict', 'battle', 'contest', 'argument', 'settlement', 'claim', 'struggle', 'relationship', 'controversy', 'clash', 'tension', 'process', 'feud', 'agreement', 'quarrel', 'fire', 'difference', 'issue', 'debate']\n",
      "Sentence 69 rankings: \n",
      " sentence:  \"Despite almost daily reports of missing property , he was able to evade capture until 15 February , when a man named Wimbow , who had been pursuing him with a partner for days , found him in an area of thick brush called Liberty Plains and shot him .\"\n",
      "mask_words  evade\n",
      "cgBERT  ['avoid', 'escape', 'dodge', 'flee', 'avoided', 'avoids', 'pursue', 'resist', 'escapes', 'avoiding', 'escaped', 'garner', 'ignore', 'survive', 'deny', 'elusive', 'avoidance', 'skip', 'bypass', 'eliminate']\n",
      "Sentence 70 rankings: \n",
      " sentence:  \"He was born at Plessiel , a hamlet of Drucat near Abbeville , to a long-established family of Picardy , the great-nephew of the painter Eustache Le Sueur .\"\n",
      "mask_words  hamlet\n",
      "cgBERT  ['village', 'settlement', 'locality', 'borough', 'part', 'place', 'lad', 'portion', 'town', 'parish', 'townland', 'community', 'shed', 'township', 'clearing', 'crossroads', 'section', 'chapel', 'ward', 'suburb']\n",
      "Sentence 71 rankings: \n",
      " sentence:  The Man in the High Castle occurs in an alternate universe United States ruled by the victorious Axis powers .\n",
      "mask_words  alternate\n",
      "cgBERT  ['extended', 'intermediate', 'inner', 'extra', 'unlikely', 'earlier', 'elliptical', 'ultimate', 'official', 'actual', 'original', 'ideal', 'expanded', 'early', 'end', 'offset', 'odds', 'exclusive', 'additional', 'eventual']\n",
      "Sentence 72 rankings: \n",
      " sentence:  Their quick acceleration makes them suitable for services with short intervals between stations .\n",
      "mask_words  suitable\n"
     ]
    }
   ],
   "source": [
    "for num in range(5):\n",
    "    args.seed=num*10+5\n",
    "    print(args.seed)\n",
    "    print('第 ',num,'次循环')\n",
    "    \n",
    "    \n",
    "    CGBERT = []\n",
    "    CSBERT = []\n",
    "    SS = []\n",
    "    substitution_words = []\n",
    "   \n",
    "    num_selection = args.num_selections\n",
    "\n",
    "    bre_i=0\n",
    "\n",
    "    window_context = 11\n",
    "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        \n",
    "     \n",
    "        fileName = args.eval_dir.split('/')[-1][:-4]\n",
    "        if fileName=='lex.mturk':\n",
    "            eval_examples, mask_words, mask_labels = read_eval_dataset(args.eval_dir)\n",
    "        else:\n",
    "            eval_examples, mask_words, mask_labels = read_eval_index_dataset(args.eval_dir)\n",
    "\n",
    "       \n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "            #logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "      \n",
    "        eval_size = len(eval_examples)\n",
    "\n",
    "        print(\"prob_mask:\",args.prob_mask)\n",
    "    \n",
    "    \n",
    "    for i in range(eval_size):\n",
    "        print('Sentence {} rankings: '.format(i))\n",
    "        #output_sr_file.write(str(i))\n",
    "        #output_sr_file.write(' sentence: ')\n",
    "        #output_sr_file.write('\\n')\n",
    "        print(' sentence: ',eval_examples[i])\n",
    "        print('mask_words ',mask_words[i])\n",
    "        tokens, words, position = convert_sentence_to_token(eval_examples[i], args.max_seq_length, tokenizer)\n",
    "\n",
    "        assert len(words)==len(position)\n",
    "\n",
    "        mask_index = words.index(mask_words[i])\n",
    "\n",
    "        mask_context = extract_context(words,mask_index,window_context)\n",
    "\n",
    "        len_tokens = len(tokens)\n",
    "\n",
    "        mask_position = position[mask_index]\n",
    " \n",
    "        if isinstance(mask_position,list):\n",
    "            feature = convert_whole_word_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "        else:\n",
    "            feature = convert_token_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "\n",
    "        tokens_tensor = torch.tensor([feature.input_ids])\n",
    "\n",
    "        token_type_ids = torch.tensor([feature.input_type_ids])\n",
    "\n",
    "        attention_mask = torch.tensor([feature.input_mask])\n",
    "    \n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        token_type_ids = token_type_ids.to('cuda')\n",
    "        attention_mask = attention_mask.to('cuda')\n",
    "\n",
    "            # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            all_attentions,prediction_scores = model(tokens_tensor, token_type_ids,attention_mask)\n",
    "\n",
    "\n",
    "            \n",
    "        if isinstance(mask_position,list):\n",
    "            predicted_top = prediction_scores[0, mask_position[0]].topk(80)\n",
    "        else:\n",
    "            predicted_top = prediction_scores[0, mask_position].topk(80)\n",
    "            #print(predicted_top[0].cpu().numpy())\n",
    "        pre_tokens = tokenizer.convert_ids_to_tokens(predicted_top[1].cpu().numpy())\n",
    "            \n",
    "        #print(predicted_top[0].cpu().numpy())\n",
    "\n",
    "        sentence = eval_examples[i].lower()\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        words_tag = nltk.pos_tag(words)\n",
    "\n",
    "        complex_word_index = words.index(mask_words[i])\n",
    "\n",
    "        complex_word_tag = words_tag[complex_word_index][1]\n",
    "\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        complex_word_tag = preprocess_tag(complex_word_tag)\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        #mask_context = extract_context(words,complex_word_index,window_context)\n",
    "\n",
    "        #break\n",
    "        #print(mask_words[i])\n",
    "        #cgPPDB,csPPDB = PPDB_candidate_generation(mask_words[i],complex_word_tag, ppdb_model, ps, word_count, 30)\n",
    "        cgPPDB = ppdb_model.predict(mask_words[i],complex_word_tag)\n",
    "\n",
    "        #if(len(cgPPDB)>30):\n",
    "        #cgPPDB=cgPPDB[:30]\n",
    "\n",
    "        #break\n",
    "        cgBERT = BERT_candidate_generation(mask_words[i], pre_tokens, predicted_top[0].cpu().numpy(), ps, args.num_selections)\n",
    "            \n",
    "        CGBERT.append(cgBERT)\n",
    "\n",
    "        pre_word = substitution_ranking(mask_words[i], mask_context, cgBERT, fasttext_dico, fasttext_emb,word_count,cgPPDB,tokenizer,model,mask_labels[i])\n",
    "\n",
    "        print('cgBERT ',cgBERT)\n",
    "        substitution_words.append(pre_word)\n",
    "    potential,precision,recall,F_score=evaulation_SS_scores(CGBERT, mask_labels)\n",
    "    print(\"The score of evaluation for BERT candidate generation\")\n",
    "    print(potential,precision,recall,F_score)\n",
    "\n",
    "        \n",
    "\n",
    "    precision,accuracy,changed_proportion=evaulation_pipeline_scores(substitution_words, mask_words, mask_labels)\n",
    "    print(\"The score of evaluation for full LS pipeline\")\n",
    "    print(precision,accuracy,changed_proportion)\n",
    "\n",
    "\n",
    "        #output_sr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "# 音频初始化\n",
    "pygame.mixer.init()\n",
    "# 加载音频文件路径 (路径必须真实存在，音频文件格式支持mp3/ogg等格式)\n",
    "pygame.mixer.music.load(r'D:/1.mp3')\n",
    "pygame.mixer.music.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
