{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertForMaskedLM\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PPDB import Ppdb\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def convert_sentence_to_token(sentence, seq_length, tokenizer):\n",
    "  \n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "\n",
    "    if len(tokenized_text) > seq_length - 2:\n",
    "        tokenized_text = tokenized_text[0:(seq_length - 2)]\n",
    "\n",
    "    position =[]\n",
    "    special =[]\n",
    "    isSpecial = False\n",
    "\n",
    "    whole_word = ''\n",
    "    words = []\n",
    "\n",
    "    start_pos =  len(tokenized_text)  + 2\n",
    "\n",
    "    connect_sign = 0\n",
    "    for index in range(len(tokenized_text)-1):\n",
    "        \n",
    "        if(tokenized_text[index+1]==\"-\" and tokenized_text[index+2]!=\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]==\"-\"):\n",
    "            \n",
    "            whole_word += tokenized_text[index]\n",
    "            special.append(start_pos+index)\n",
    "\n",
    "            if(tokenized_text[index-1]==\"-\"):\n",
    "                words.append(whole_word)\n",
    "                position.append(special)\n",
    "                special = []\n",
    "                whole_word = ''\n",
    "            continue\n",
    "\n",
    "        if(tokenized_text[index]!=\"-\" and tokenized_text[index-1]==\"-\"):\n",
    "            whole_word += tokenized_text[index]\n",
    "            words.append(whole_word)\n",
    "            whole_word = ''\n",
    "            special.append(start_pos+index)\n",
    "            position.append(special)\n",
    "            special = []\n",
    "            continue    \n",
    "\n",
    "        if(tokenized_text[index+1][0:2]==\"##\"):\n",
    "            special.append(start_pos+index)\n",
    "            whole_word += tokenized_text[index]\n",
    "            isSpecial = True\n",
    "            continue\n",
    "        else:\n",
    "            if isSpecial:\n",
    "                isSpecial = False\n",
    "                special.append(start_pos+index)\n",
    "                position.append(special)\n",
    "                whole_word += tokenized_text[index]\n",
    "                whole_word = whole_word.replace('##','')\n",
    "                words.append(whole_word)\n",
    "                whole_word = ''\n",
    "                special =  []\n",
    "            else:\n",
    "                position.append(start_pos+index)\n",
    "                words.append(tokenized_text[index])\n",
    "\n",
    "    if isSpecial:\n",
    "        isSpecial = False\n",
    "        special.append(start_pos+index+1)\n",
    "        position.append(special)\n",
    "        whole_word += tokenized_text[index+1]\n",
    "        whole_word = whole_word.replace('##','')\n",
    "        words.append(whole_word)\n",
    "    else:\n",
    "        position.append(start_pos+index+1)\n",
    "        words.append(tokenized_text[index+1])\n",
    "       \n",
    "    return tokenized_text, words, position\n",
    "\n",
    "def convert_whole_word_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"Convert_whole_word_to_feature\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "\n",
    "    class ClassName(object):\n",
    "        \"\"\"docstring for ClassName\"\"\"\n",
    "        def __init__(self, arg):\n",
    "            super(ClassName, self).__init__()\n",
    "            self.arg = arg\n",
    "    \n",
    "    len_tokens = len(tokens_a)\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    mask_index = []\n",
    "\n",
    "    for mask_pos in mask_position:\n",
    "        mask_index.append(mask_pos-len_tokens-2)\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i in mask_index:\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    index = 0\n",
    "    count = 0\n",
    "    mask_position_length = len(mask_position)\n",
    "\n",
    "    while count in range(mask_position_length):\n",
    "        index = mask_position_length - 1 - count\n",
    "\n",
    "        pos = mask_position[index]\n",
    "        if index == 0:\n",
    "            tokens[pos] = '[MASK]'\n",
    "        else:\n",
    "            del tokens[pos]\n",
    "            del input_type_ids[pos]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def convert_token_to_feature(tokens_a, mask_position, seq_length, tokenizer, prob_mask):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "    #print(\"----------\")\n",
    "    #print(tokens_a)\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    len_tokens = len(tokens_a)\n",
    "    #print(\"length of tokens: \", len_tokens)\n",
    "\n",
    "    first_sentence_mask_random = random.sample(range(0,len_tokens), int(prob_mask*len_tokens))\n",
    "\n",
    "    for i in range(len_tokens):\n",
    "\n",
    "        if i==(mask_position-len_tokens-2):\n",
    "            tokens.append(tokens_a[i])\n",
    "        elif i in first_sentence_mask_random:\n",
    "            tokens.append('[MASK]')\n",
    "        else:\n",
    "            tokens.append(tokens_a[i])\n",
    "        input_type_ids.append(0)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(1)\n",
    "\n",
    "    true_word = ''\n",
    "    true_word = tokens[mask_position]\n",
    "    tokens[mask_position] =  '[MASK]'\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def getWordmap(wordVecPath):\n",
    "    words=[]\n",
    "    We = []\n",
    "    f = open(wordVecPath,'r', encoding=\"utf-8\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for (n,line) in enumerate(lines):\n",
    "        if (n == 0) :\n",
    "            print(line)\n",
    "            continue\n",
    "        word, vect = line.rstrip().split(' ', 1)\n",
    "                    \n",
    "        vect = np.fromstring(vect, sep=' ')\n",
    "                \n",
    "        We.append(vect)\n",
    "\n",
    "        words.append(word)\n",
    "\n",
    "        #if(n==200000):\n",
    "        #    break\n",
    "    f.close()       \n",
    "    return (words, We)\n",
    "\n",
    "\n",
    "def getWordCount(word_count_path):\n",
    "    word2count = {}\n",
    "    xlsx_file = Path('',word_count_path)\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "    sheet = wb_obj.active\n",
    "\n",
    "    last_column = sheet.max_column-1\n",
    "    for i, row in enumerate(sheet.iter_rows(values_only=True)):\n",
    "        if i==0:\n",
    "            continue\n",
    "        word2count[row[0]] = round(float(row[last_column]),3)\n",
    "        \n",
    "    return word2count\n",
    "\n",
    "def read_eval_index_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            \n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "            mask_word,labels = words.strip().split('\\t',1)\n",
    "            label = labels.split('\\t')\n",
    "                \n",
    "            sentences.append(sentence)\n",
    "            mask_words.append(mask_word)\n",
    "                \n",
    "            one_labels = []\n",
    "            for la in label[1:]:\n",
    "                if la not in one_labels:\n",
    "                    la_id,la_word = la.split(':')\n",
    "                    one_labels.append(la_word)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "            mask_labels.append(one_labels)\n",
    "            \n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def read_eval_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "    id = 0\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if is_label:\n",
    "                id += 1\n",
    "                if id==1:\n",
    "                    continue\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "                mask_word,labels = words.strip().split('\\t',1)\n",
    "                label = labels.split('\\t')\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "                \n",
    "                one_labels = []\n",
    "                for la in label:\n",
    "                    if la not in one_labels:\n",
    "                        one_labels.append(la)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "                mask_labels.append(one_labels)\n",
    "            else:\n",
    "                if not line:\n",
    "                    break\n",
    "                #print(line)\n",
    "                sentence,mask_word = line.strip().split('\\t')\n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def BERT_candidate_generation(source_word, pre_tokens, pre_scores, ps, num_selection=10):\n",
    "\n",
    "    cur_tokens=[]\n",
    "   \n",
    "\n",
    "    source_stem = ps.stem(source_word)\n",
    "\n",
    "    assert num_selection<=len(pre_tokens)\n",
    "\n",
    "    for i in range(len(pre_tokens)):\n",
    "        token = pre_tokens[i]\n",
    "     \n",
    "        if token[0:2]==\"##\":\n",
    "            continue\n",
    "\n",
    "        if(token==source_word):\n",
    "            continue\n",
    "\n",
    "        token_stem = ps.stem(token)\n",
    "\n",
    "        if(token_stem == source_stem):\n",
    "            continue\n",
    "\n",
    "        if (len(token_stem)>=3) and (token_stem[:3]==source_stem[:3]):\n",
    "            continue\n",
    "\n",
    "        cur_tokens.append(token)\n",
    "        \n",
    "\n",
    "        if(len(cur_tokens)==num_selection):\n",
    "            break\n",
    "    \n",
    "    if(len(cur_tokens)==0):\n",
    "        cur_tokens = pre_tokens[0:num_selection+1]\n",
    "        \n",
    "\n",
    "    assert len(cur_tokens)>0       \n",
    "\n",
    "    return cur_tokens\n",
    "\n",
    "def cross_entropy_word(X,i,pos):\n",
    "    \n",
    "    #print(X)\n",
    "    #print(X[0,2,3])\n",
    "    X = softmax(X,axis=1)\n",
    "    loss = 0\n",
    "    loss -= np.log10(X[i,pos])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_score(sentence,tokenizer,maskedLM):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "\n",
    "    len_sen = len(tokenize_input)\n",
    "\n",
    "    START_TOKEN = '[CLS]'\n",
    "    SEPARATOR_TOKEN = '[SEP]'\n",
    "\n",
    "    tokenize_input.insert(0, START_TOKEN)\n",
    "    tokenize_input.append(SEPARATOR_TOKEN)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenize_input)\n",
    "\n",
    "    #tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    #print(\"tensor_input\")\n",
    "    #print(tensor_input)\n",
    "    #tensor_input = tensor_input.to('cuda')\n",
    "    sentence_loss = 0\n",
    "    \n",
    "    for i,word in enumerate(tokenize_input):\n",
    "\n",
    "        if(word == START_TOKEN or word==SEPARATOR_TOKEN):\n",
    "            continue\n",
    "\n",
    "        orignial_word = tokenize_input[i]\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        #print(tokenize_input)\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        #print(mask_input)\n",
    "        mask_input = mask_input.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            att, pre_word =maskedLM(mask_input)\n",
    "        word_loss = cross_entropy_word(pre_word[0].cpu().numpy(),i,input_ids[i])\n",
    "        sentence_loss += word_loss\n",
    "        #print(word_loss)\n",
    "        tokenize_input[i] = orignial_word\n",
    "        \n",
    "    return np.exp(sentence_loss/len_sen)\n",
    "\n",
    "\n",
    "def LM_score(source_word,source_context,substitution_selection,tokenizer,maskedLM):\n",
    "    #source_index = source_context.index(source_word)\n",
    "\n",
    "    source_sentence = ''\n",
    "\n",
    "    for context in source_context:\n",
    "        source_sentence += context + \" \"\n",
    "    \n",
    "    source_sentence = source_sentence.strip()\n",
    "    #print(source_sentence)\n",
    "    LM = []\n",
    "\n",
    "    source_loss = get_score(source_sentence,tokenizer,maskedLM)\n",
    "\n",
    "    for substibution in substitution_selection:\n",
    "        \n",
    "        sub_sentence = source_sentence.replace(source_word,substibution)\n",
    "\n",
    "        \n",
    "        #print(sub_sentence)\n",
    "        score = get_score(sub_sentence,tokenizer,maskedLM)\n",
    "\n",
    "        #print(score)\n",
    "        LM.append(score)\n",
    "\n",
    "    return LM,source_loss\n",
    "\n",
    "\n",
    "def preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count):\n",
    "    ss = []\n",
    "    ##ss_score=[]\n",
    "    sis_scores=[]\n",
    "    count_scores=[]\n",
    "\n",
    "    isFast = True\n",
    "\n",
    "    if(source_word not in fasttext_dico):\n",
    "        isFast = False\n",
    "    else:\n",
    "        source_emb = fasttext_emb[fasttext_dico.index(source_word)].reshape(1,-1)\n",
    "\n",
    "    #ss.append(source_word)\n",
    "\n",
    "    for sub in substitution_selection:\n",
    "\n",
    "        if sub not in word_count:\n",
    "            continue\n",
    "        else:\n",
    "            sub_count = word_count[sub]\n",
    "\n",
    "        if(sub_count<=3):\n",
    "            continue\n",
    "\n",
    "        #if sub_count<source_count:\n",
    "         #   continue\n",
    "        if isFast:\n",
    "            if sub not in fasttext_dico:\n",
    "                continue\n",
    "\n",
    "            token_index_fast = fasttext_dico.index(sub)\n",
    "            sis = cosine(source_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            #if sis<0.35:\n",
    "            #    continue\n",
    "            sis_scores.append(sis)\n",
    "\n",
    "        ss.append(sub)\n",
    "        count_scores.append(sub_count)\n",
    "\n",
    "    return ss,sis_scores,count_scores\n",
    "\n",
    "def compute_context_sis_score(source_word, sis_context, substitution_selection, fasttext_dico, fasttext_emb):\n",
    "    context_sis = []\n",
    "\n",
    "    word_context = []\n",
    "\n",
    "    \n",
    "\n",
    "    for con in sis_context:\n",
    "        if con==source_word or (con not in fasttext_dico):\n",
    "            continue\n",
    "\n",
    "        word_context.append(con)\n",
    "\n",
    "    if len(word_context)!=0:\n",
    "        for sub in substitution_selection:\n",
    "            sub_emb = fasttext_emb[fasttext_dico.index(sub)].reshape(1,-1)\n",
    "            all_sis = 0\n",
    "            for con in word_context:\n",
    "                token_index_fast = fasttext_dico.index(con)\n",
    "                all_sis += cosine(sub_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            context_sis.append(all_sis/len(word_context))\n",
    "    else:\n",
    "        for i in range(len(substitution_selection)):\n",
    "            context_sis.append(len(substitution_selection)-i)\n",
    "\n",
    "            \n",
    "    return context_sis\n",
    "\n",
    "\n",
    "def substitution_ranking(source_word, source_context, substitution_selection, fasttext_dico, fasttext_emb, word_count, ssPPDB, tokenizer, maskedLM, lables):\n",
    "\n",
    "    ss,sis_scores,count_scores=preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count)\n",
    "\n",
    "    #print(ss)\n",
    "    if len(ss)==0:\n",
    "        return source_word\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        seq = sorted(sis_scores,reverse = True )\n",
    "        sis_rank = [seq.index(v)+1 for v in sis_scores]\n",
    "        #print('sis_rank',sis_rank)\n",
    "    rank_count = sorted(count_scores,reverse = True )\n",
    "    count_rank = [rank_count.index(v)+1 for v in count_scores]\n",
    "    #print('count_rank',count_rank)\n",
    "    \n",
    "    \n",
    "    lm_score,source_lm = LM_score(source_word,source_context,ss,tokenizer,maskedLM)\n",
    "\n",
    "    rank_lm = sorted(lm_score)\n",
    "    lm_rank = [rank_lm.index(v)+1 for v in lm_score]\n",
    "    #print('lm_rank',lm_rank)\n",
    "    \n",
    "\n",
    "\n",
    "    bert_rank = []\n",
    "    ppdb_rank =[]\n",
    "    for i in range(len(ss)):\n",
    "        bert_rank.append(i+1)\n",
    "\n",
    "        if ss[i] in ssPPDB:\n",
    "            ppdb_rank.append(1)\n",
    "        else:\n",
    "            ppdb_rank.append(len(ss)/3)\n",
    "            \n",
    "    #print('bert_rank',bert_rank)\n",
    "    #print('ppdb_rank',ppdb_rank)\n",
    "    if len(sis_scores)>0:\n",
    "        all_ranks = [bert+sis+count+ppdb  for bert,sis,count,LM,ppdb in zip(bert_rank,sis_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    else:\n",
    "        all_ranks = [bert+count+ppdb  for bert,count,LM,ppdb in zip(bert_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "        \n",
    "        \n",
    "    '''if len(sis_scores)>0:\n",
    "        all_ranks = [bert+sis+count+LM+ppdb  for bert,sis,count,LM,ppdb in zip(bert_rank,sis_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "    else:\n",
    "        all_ranks = [bert+count+LM+ppdb  for bert,count,LM,ppdb in zip(bert_rank,count_rank,lm_rank,ppdb_rank)]\n",
    "        '''\n",
    "    #print('all_ranks',all_ranks)\n",
    "    pre_index = all_ranks.index(min(all_ranks))\n",
    "\n",
    "    #return ss[pre_index]\n",
    "\n",
    "    pre_count = count_scores[pre_index]\n",
    "\n",
    "    if source_word in word_count:\n",
    "        source_count = word_count[source_word]\n",
    "    else:\n",
    "        source_count = 0\n",
    "\n",
    "    pre_lm = lm_score[pre_index]\n",
    "\n",
    "    #print(lm_score)\n",
    "    #print(source_lm)\n",
    "    #print(pre_lm)\n",
    "\n",
    "\n",
    "    #pre_word = ss[pre_index]\n",
    "\n",
    "\n",
    "    if source_lm>pre_lm or pre_count>source_count:\n",
    "        pre_word = ss[pre_index]\n",
    "    else:\n",
    "        pre_word = source_word\n",
    "    \n",
    "    \n",
    "    return pre_word\n",
    "\n",
    "\n",
    "def evaulation_SS_scores(ss,labels):\n",
    "    assert len(ss)==len(labels)\n",
    "\n",
    "    potential = 0\n",
    "    instances = len(ss)\n",
    "    precision = 0\n",
    "    precision_all = 0\n",
    "    recall = 0\n",
    "    recall_all = 0\n",
    "\n",
    "    for i in range(len(ss)):\n",
    "\n",
    "        one_prec = 0\n",
    "        \n",
    "        common = list(set(ss[i]).intersection(labels[i]))\n",
    "\n",
    "        if len(common)>=1:\n",
    "            potential +=1\n",
    "        precision += len(common)\n",
    "        recall += len(common)\n",
    "        precision_all += len(ss[i])\n",
    "        recall_all += len(labels[i])\n",
    "\n",
    "    potential /=  instances\n",
    "    precision /= precision_all\n",
    "    recall /= recall_all\n",
    "    F_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return potential,precision,recall,F_score\n",
    "\n",
    "\n",
    "def evaulation_pipeline_scores(substitution_words,source_words,gold_words):\n",
    "\n",
    "    instances = len(substitution_words)\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    changed_proportion = 0\n",
    "\n",
    "    for sub, source, gold in zip(substitution_words,source_words,gold_words):\n",
    "        if sub==source or (sub in gold):\n",
    "            precision += 1\n",
    "        if sub!=source and (sub in gold):\n",
    "            accuracy += 1\n",
    "        if sub!=source:\n",
    "            changed_proportion += 1\n",
    "\n",
    "    return precision/instances,accuracy/instances,changed_proportion/instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_context(words, mask_index, window):\n",
    "    #extract 7 words around the content word\n",
    "\n",
    "    length = len(words)\n",
    "\n",
    "    half = int(window/2)\n",
    "\n",
    "    assert mask_index>=0 and mask_index<length\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    if length<=window:\n",
    "        context = words\n",
    "    elif mask_index<length-half and mask_index>=half:\n",
    "        context = words[mask_index-half:mask_index+half+1]\n",
    "    elif mask_index<half:\n",
    "        context = words[0:window]\n",
    "    elif mask_index>=length-half:\n",
    "        context = words[length-window:length]\n",
    "    else:\n",
    "        print(\"Wrong!\")\n",
    "\n",
    "    return context\n",
    "\n",
    "def preprocess_tag(tag):\n",
    "    if tag[0] ==\"V\" or tag[0]==\"N\":\n",
    "        return tag\n",
    "    if tag[0]==\"R\":\n",
    "        return \"r\"\n",
    "    if tag[0]==\"J\" or tag[0]==\"I\":\n",
    "        return 'a'\n",
    "    else:\n",
    "        return 's'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--eval_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The evaluation data dir.\")\n",
    "parser.add_argument(\"--bert_model\", default=None, type=str,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "\n",
    "parser.add_argument(\"--output_SR_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The output directory of writing substitution selection.\")\n",
    "parser.add_argument(\"--word_embeddings\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word embeddings\")\n",
    "parser.add_argument(\"--word_frequency\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "    \n",
    "parser.add_argument(\"--ppdb\",\n",
    "                        default=\"./ppdb-2.0-tldr\",\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "\n",
    "parser.add_argument(\"--prob_mask\",\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of the masked words in first sentence. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "\n",
    "\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--num_selections\",\n",
    "                        default=20,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--num_eval_epochs\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_eval=True\n",
    "args.do_lower_case=True\n",
    "args.num_selections=10\n",
    "\n",
    "args.prob_mask=0.5\n",
    "\n",
    "args.eval_dir='D:/data/bert_ppdb/datasets/BenchLS.txt'\n",
    "args.bert_model='bert-large-uncased-whole-word-masking'\n",
    "args.max_seq_length=250\n",
    "args.word_embeddings='D:/data/bert_ppdb/crawl-300d-2M-subword.vec'\n",
    "args.word_frequency='D:/data/bert_ppdb/SUBTLEX_frequency.xlsx'\n",
    "args.ppdb='D:/data/bert_ppdb/ppdb-2.0-tldr'\n",
    "args.output_SR_file='D:/data/bert_ppdb/results/NNSeval'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings ...\n",
      "2000000 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embeddings ...\")\n",
    "\n",
    "wordVecPath = args.word_embeddings\n",
    "#wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\"\n",
    "\n",
    "fasttext_dico, fasttext_emb = getWordmap(wordVecPath)\n",
    "\n",
    "#stopword = set(stopwords.words('english'))\n",
    "word_count_path = args.word_frequency\n",
    "#word_count_path = \"word_frequency_wiki.txt\"\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PPDB ...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading PPDB ...\")\n",
    "ppdb_path = args.ppdb\n",
    "ppdb_model = Ppdb(ppdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-vocab.txt\n",
      "INFO:pytorch_pretrained_bert.modeling:loading weights file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-pytorch_model.bin\n",
      "INFO:pytorch_pretrained_bert.modeling:loading configuration file D:/data/bert_ppdb/bert-large-uncased-whole-word-masking-config.json\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "if args.server_ip and args.server_port:\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not args.do_eval:\n",
    "    raise ValueError(\"At least `do_eval` must be True.\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "    \n",
    "\n",
    "# Prepare model\n",
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "model = BertForMaskedLM.from_pretrained(args.bert_model,output_attentions=True,cache_dir=cache_dir)\n",
    "if args.fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "\n",
    "output_sr_file = open(args.output_SR_file,\"a+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running evaluation *****\n",
      "INFO:__main__:  Num examples = 929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "第  0 次循环\n",
      "bert-large-uncased-whole-word-masking\n",
      "prob_mask: 0.5\n",
      "Sentence 0 rankings: \n",
      "******在标签中\n",
      "Sentence 1 rankings: \n",
      "######不在标签中\n",
      "Sentence 2 rankings: \n",
      "******在标签中\n",
      "Sentence 3 rankings: \n",
      "******在标签中\n",
      "Sentence 4 rankings: \n",
      "******在标签中\n",
      "Sentence 5 rankings: \n",
      "******在标签中\n",
      "Sentence 6 rankings: \n",
      "******在标签中\n",
      "Sentence 7 rankings: \n",
      "######不在标签中\n",
      "Sentence 8 rankings: \n",
      "******在标签中\n",
      "Sentence 9 rankings: \n",
      "######不在标签中\n",
      "Sentence 10 rankings: \n",
      "******在标签中\n",
      "Sentence 11 rankings: \n",
      "******在标签中\n",
      "Sentence 12 rankings: \n",
      "######不在标签中\n",
      "Sentence 13 rankings: \n",
      "******在标签中\n",
      "Sentence 14 rankings: \n",
      "******在标签中\n",
      "Sentence 15 rankings: \n",
      "******在标签中\n",
      "Sentence 16 rankings: \n",
      "******在标签中\n",
      "Sentence 17 rankings: \n",
      "******在标签中\n",
      "Sentence 18 rankings: \n",
      "######不在标签中\n",
      "Sentence 19 rankings: \n",
      "******在标签中\n",
      "Sentence 20 rankings: \n",
      "******在标签中\n",
      "Sentence 21 rankings: \n",
      "******在标签中\n",
      "Sentence 22 rankings: \n",
      "******在标签中\n",
      "Sentence 23 rankings: \n",
      "######不在标签中\n",
      "Sentence 24 rankings: \n",
      "******在标签中\n",
      "Sentence 25 rankings: \n",
      "######不在标签中\n",
      "Sentence 26 rankings: \n",
      "######不在标签中\n",
      "Sentence 27 rankings: \n",
      "******在标签中\n",
      "Sentence 28 rankings: \n",
      "******在标签中\n",
      "Sentence 29 rankings: \n",
      "######不在标签中\n",
      "Sentence 30 rankings: \n",
      "******在标签中\n",
      "Sentence 31 rankings: \n",
      "******在标签中\n",
      "Sentence 32 rankings: \n",
      "******在标签中\n",
      "Sentence 33 rankings: \n",
      "******在标签中\n",
      "Sentence 34 rankings: \n",
      "******在标签中\n",
      "Sentence 35 rankings: \n",
      "******在标签中\n",
      "Sentence 36 rankings: \n",
      "######不在标签中\n",
      "Sentence 37 rankings: \n",
      "******在标签中\n",
      "Sentence 38 rankings: \n",
      "######不在标签中\n",
      "Sentence 39 rankings: \n",
      "******在标签中\n",
      "Sentence 40 rankings: \n",
      "******在标签中\n",
      "Sentence 41 rankings: \n",
      "******在标签中\n",
      "Sentence 42 rankings: \n",
      "######不在标签中\n",
      "Sentence 43 rankings: \n",
      "******在标签中\n",
      "Sentence 44 rankings: \n",
      "######不在标签中\n",
      "Sentence 45 rankings: \n",
      "******在标签中\n",
      "Sentence 46 rankings: \n",
      "******在标签中\n",
      "Sentence 47 rankings: \n",
      "******在标签中\n",
      "Sentence 48 rankings: \n",
      "******在标签中\n",
      "Sentence 49 rankings: \n",
      "******在标签中\n",
      "Sentence 50 rankings: \n",
      "******在标签中\n",
      "Sentence 51 rankings: \n",
      "******在标签中\n",
      "Sentence 52 rankings: \n",
      "******在标签中\n",
      "Sentence 53 rankings: \n",
      "******在标签中\n",
      "Sentence 54 rankings: \n",
      "******在标签中\n",
      "Sentence 55 rankings: \n",
      "******在标签中\n",
      "Sentence 56 rankings: \n",
      "******在标签中\n",
      "Sentence 57 rankings: \n",
      "******在标签中\n",
      "Sentence 58 rankings: \n",
      "******在标签中\n",
      "Sentence 59 rankings: \n",
      "******在标签中\n",
      "Sentence 60 rankings: \n",
      "******在标签中\n",
      "Sentence 61 rankings: \n",
      "######不在标签中\n",
      "Sentence 62 rankings: \n",
      "######不在标签中\n",
      "Sentence 63 rankings: \n",
      "******在标签中\n",
      "Sentence 64 rankings: \n",
      "******在标签中\n",
      "Sentence 65 rankings: \n",
      "******在标签中\n",
      "Sentence 66 rankings: \n",
      "######不在标签中\n",
      "Sentence 67 rankings: \n",
      "######不在标签中\n",
      "Sentence 68 rankings: \n",
      "******在标签中\n",
      "Sentence 69 rankings: \n",
      "******在标签中\n",
      "Sentence 70 rankings: \n",
      "******在标签中\n",
      "Sentence 71 rankings: \n",
      "######不在标签中\n",
      "Sentence 72 rankings: \n",
      "******在标签中\n",
      "Sentence 73 rankings: \n",
      "******在标签中\n",
      "Sentence 74 rankings: \n",
      "######不在标签中\n",
      "Sentence 75 rankings: \n",
      "******在标签中\n",
      "Sentence 76 rankings: \n",
      "******在标签中\n",
      "Sentence 77 rankings: \n",
      "******在标签中\n",
      "Sentence 78 rankings: \n",
      "******在标签中\n",
      "Sentence 79 rankings: \n",
      "******在标签中\n",
      "Sentence 80 rankings: \n",
      "******在标签中\n",
      "Sentence 81 rankings: \n",
      "******在标签中\n",
      "Sentence 82 rankings: \n",
      "******在标签中\n",
      "Sentence 83 rankings: \n",
      "******在标签中\n",
      "Sentence 84 rankings: \n",
      "******在标签中\n",
      "Sentence 85 rankings: \n",
      "******在标签中\n",
      "Sentence 86 rankings: \n",
      "******在标签中\n",
      "Sentence 87 rankings: \n",
      "******在标签中\n",
      "Sentence 88 rankings: \n",
      "******在标签中\n",
      "Sentence 89 rankings: \n",
      "******在标签中\n",
      "Sentence 90 rankings: \n",
      "******在标签中\n",
      "Sentence 91 rankings: \n",
      "******在标签中\n",
      "Sentence 92 rankings: \n",
      "******在标签中\n",
      "Sentence 93 rankings: \n",
      "******在标签中\n",
      "Sentence 94 rankings: \n",
      "######不在标签中\n",
      "Sentence 95 rankings: \n",
      "******在标签中\n",
      "Sentence 96 rankings: \n",
      "######不在标签中\n",
      "Sentence 97 rankings: \n",
      "******在标签中\n",
      "Sentence 98 rankings: \n",
      "******在标签中\n",
      "Sentence 99 rankings: \n",
      "******在标签中\n",
      "Sentence 100 rankings: \n",
      "******在标签中\n",
      "Sentence 101 rankings: \n",
      "******在标签中\n",
      "Sentence 102 rankings: \n",
      "******在标签中\n",
      "Sentence 103 rankings: \n",
      "######不在标签中\n",
      "Sentence 104 rankings: \n",
      "******在标签中\n",
      "Sentence 105 rankings: \n",
      "******在标签中\n",
      "Sentence 106 rankings: \n",
      "######不在标签中\n",
      "Sentence 107 rankings: \n",
      "******在标签中\n",
      "Sentence 108 rankings: \n",
      "######不在标签中\n",
      "Sentence 109 rankings: \n",
      "******在标签中\n",
      "Sentence 110 rankings: \n",
      "******在标签中\n",
      "Sentence 111 rankings: \n",
      "######不在标签中\n",
      "Sentence 112 rankings: \n",
      "******在标签中\n",
      "Sentence 113 rankings: \n",
      "######不在标签中\n",
      "Sentence 114 rankings: \n",
      "######不在标签中\n",
      "Sentence 115 rankings: \n",
      "******在标签中\n",
      "Sentence 116 rankings: \n",
      "######不在标签中\n",
      "Sentence 117 rankings: \n",
      "######不在标签中\n",
      "Sentence 118 rankings: \n",
      "******在标签中\n",
      "Sentence 119 rankings: \n",
      "******在标签中\n",
      "Sentence 120 rankings: \n",
      "######不在标签中\n",
      "Sentence 121 rankings: \n",
      "******在标签中\n",
      "Sentence 122 rankings: \n",
      "******在标签中\n",
      "Sentence 123 rankings: \n",
      "######不在标签中\n",
      "Sentence 124 rankings: \n",
      "******在标签中\n",
      "Sentence 125 rankings: \n",
      "******在标签中\n",
      "Sentence 126 rankings: \n",
      "******在标签中\n",
      "Sentence 127 rankings: \n",
      "******在标签中\n",
      "Sentence 128 rankings: \n",
      "######不在标签中\n",
      "Sentence 129 rankings: \n",
      "******在标签中\n",
      "Sentence 130 rankings: \n",
      "******在标签中\n",
      "Sentence 131 rankings: \n",
      "******在标签中\n",
      "Sentence 132 rankings: \n",
      "******在标签中\n",
      "Sentence 133 rankings: \n",
      "******在标签中\n",
      "Sentence 134 rankings: \n",
      "******在标签中\n",
      "Sentence 135 rankings: \n",
      "######不在标签中\n",
      "Sentence 136 rankings: \n",
      "******在标签中\n",
      "Sentence 137 rankings: \n",
      "******在标签中\n",
      "Sentence 138 rankings: \n",
      "######不在标签中\n",
      "Sentence 139 rankings: \n",
      "******在标签中\n",
      "Sentence 140 rankings: \n",
      "******在标签中\n",
      "Sentence 141 rankings: \n",
      "******在标签中\n",
      "Sentence 142 rankings: \n",
      "******在标签中\n",
      "Sentence 143 rankings: \n",
      "******在标签中\n",
      "Sentence 144 rankings: \n",
      "******在标签中\n",
      "Sentence 145 rankings: \n",
      "******在标签中\n",
      "Sentence 146 rankings: \n",
      "******在标签中\n",
      "Sentence 147 rankings: \n",
      "######不在标签中\n",
      "Sentence 148 rankings: \n",
      "######不在标签中\n",
      "Sentence 149 rankings: \n",
      "******在标签中\n",
      "Sentence 150 rankings: \n",
      "******在标签中\n",
      "Sentence 151 rankings: \n",
      "######不在标签中\n",
      "Sentence 152 rankings: \n",
      "######不在标签中\n",
      "Sentence 153 rankings: \n",
      "######不在标签中\n",
      "Sentence 154 rankings: \n",
      "******在标签中\n",
      "Sentence 155 rankings: \n",
      "******在标签中\n",
      "Sentence 156 rankings: \n",
      "######不在标签中\n",
      "Sentence 157 rankings: \n",
      "******在标签中\n",
      "Sentence 158 rankings: \n",
      "******在标签中\n",
      "Sentence 159 rankings: \n",
      "******在标签中\n",
      "Sentence 160 rankings: \n",
      "******在标签中\n",
      "Sentence 161 rankings: \n",
      "######不在标签中\n",
      "Sentence 162 rankings: \n",
      "######不在标签中\n",
      "Sentence 163 rankings: \n",
      "######不在标签中\n",
      "Sentence 164 rankings: \n",
      "******在标签中\n",
      "Sentence 165 rankings: \n",
      "******在标签中\n",
      "Sentence 166 rankings: \n",
      "######不在标签中\n",
      "Sentence 167 rankings: \n",
      "######不在标签中\n",
      "Sentence 168 rankings: \n",
      "******在标签中\n",
      "Sentence 169 rankings: \n",
      "######不在标签中\n",
      "Sentence 170 rankings: \n",
      "******在标签中\n",
      "Sentence 171 rankings: \n",
      "******在标签中\n",
      "Sentence 172 rankings: \n",
      "######不在标签中\n",
      "Sentence 173 rankings: \n",
      "******在标签中\n",
      "Sentence 174 rankings: \n",
      "######不在标签中\n",
      "Sentence 175 rankings: \n",
      "######不在标签中\n",
      "Sentence 176 rankings: \n",
      "######不在标签中\n",
      "Sentence 177 rankings: \n",
      "******在标签中\n",
      "Sentence 178 rankings: \n",
      "******在标签中\n",
      "Sentence 179 rankings: \n",
      "******在标签中\n",
      "Sentence 180 rankings: \n",
      "******在标签中\n",
      "Sentence 181 rankings: \n",
      "******在标签中\n",
      "Sentence 182 rankings: \n",
      "******在标签中\n",
      "Sentence 183 rankings: \n",
      "######不在标签中\n",
      "Sentence 184 rankings: \n",
      "******在标签中\n",
      "Sentence 185 rankings: \n",
      "******在标签中\n",
      "Sentence 186 rankings: \n",
      "******在标签中\n",
      "Sentence 187 rankings: \n",
      "******在标签中\n",
      "Sentence 188 rankings: \n",
      "******在标签中\n",
      "Sentence 189 rankings: \n",
      "######不在标签中\n",
      "Sentence 190 rankings: \n",
      "******在标签中\n",
      "Sentence 191 rankings: \n",
      "######不在标签中\n",
      "Sentence 192 rankings: \n",
      "******在标签中\n",
      "Sentence 193 rankings: \n",
      "******在标签中\n",
      "Sentence 194 rankings: \n",
      "******在标签中\n",
      "Sentence 195 rankings: \n",
      "******在标签中\n",
      "Sentence 196 rankings: \n",
      "******在标签中\n",
      "Sentence 197 rankings: \n",
      "######不在标签中\n",
      "Sentence 198 rankings: \n",
      "######不在标签中\n",
      "Sentence 199 rankings: \n",
      "******在标签中\n",
      "Sentence 200 rankings: \n",
      "######不在标签中\n",
      "Sentence 201 rankings: \n",
      "******在标签中\n",
      "Sentence 202 rankings: \n",
      "******在标签中\n",
      "Sentence 203 rankings: \n",
      "******在标签中\n",
      "Sentence 204 rankings: \n",
      "******在标签中\n",
      "Sentence 205 rankings: \n",
      "******在标签中\n",
      "Sentence 206 rankings: \n",
      "######不在标签中\n",
      "Sentence 207 rankings: \n",
      "******在标签中\n",
      "Sentence 208 rankings: \n",
      "******在标签中\n",
      "Sentence 209 rankings: \n",
      "******在标签中\n",
      "Sentence 210 rankings: \n",
      "######不在标签中\n",
      "Sentence 211 rankings: \n",
      "******在标签中\n",
      "Sentence 212 rankings: \n",
      "******在标签中\n",
      "Sentence 213 rankings: \n",
      "######不在标签中\n",
      "Sentence 214 rankings: \n",
      "******在标签中\n",
      "Sentence 215 rankings: \n",
      "######不在标签中\n",
      "Sentence 216 rankings: \n",
      "******在标签中\n",
      "Sentence 217 rankings: \n",
      "******在标签中\n",
      "Sentence 218 rankings: \n",
      "######不在标签中\n",
      "Sentence 219 rankings: \n",
      "******在标签中\n",
      "Sentence 220 rankings: \n",
      "******在标签中\n",
      "Sentence 221 rankings: \n",
      "******在标签中\n",
      "Sentence 222 rankings: \n",
      "******在标签中\n",
      "Sentence 223 rankings: \n",
      "######不在标签中\n",
      "Sentence 224 rankings: \n",
      "******在标签中\n",
      "Sentence 225 rankings: \n",
      "******在标签中\n",
      "Sentence 226 rankings: \n",
      "******在标签中\n",
      "Sentence 227 rankings: \n",
      "******在标签中\n",
      "Sentence 228 rankings: \n",
      "######不在标签中\n",
      "Sentence 229 rankings: \n",
      "######不在标签中\n",
      "Sentence 230 rankings: \n",
      "******在标签中\n",
      "Sentence 231 rankings: \n",
      "######不在标签中\n",
      "Sentence 232 rankings: \n",
      "******在标签中\n",
      "Sentence 233 rankings: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******在标签中\n",
      "Sentence 234 rankings: \n",
      "******在标签中\n",
      "Sentence 235 rankings: \n",
      "******在标签中\n",
      "Sentence 236 rankings: \n",
      "******在标签中\n",
      "Sentence 237 rankings: \n",
      "******在标签中\n",
      "Sentence 238 rankings: \n",
      "######不在标签中\n",
      "Sentence 239 rankings: \n",
      "######不在标签中\n",
      "Sentence 240 rankings: \n",
      "******在标签中\n",
      "Sentence 241 rankings: \n",
      "######不在标签中\n",
      "Sentence 242 rankings: \n",
      "######不在标签中\n",
      "Sentence 243 rankings: \n",
      "******在标签中\n",
      "Sentence 244 rankings: \n",
      "******在标签中\n",
      "Sentence 245 rankings: \n",
      "######不在标签中\n",
      "Sentence 246 rankings: \n",
      "######不在标签中\n",
      "Sentence 247 rankings: \n",
      "******在标签中\n",
      "Sentence 248 rankings: \n",
      "######不在标签中\n",
      "Sentence 249 rankings: \n",
      "******在标签中\n",
      "Sentence 250 rankings: \n",
      "******在标签中\n",
      "Sentence 251 rankings: \n",
      "******在标签中\n",
      "Sentence 252 rankings: \n",
      "######不在标签中\n",
      "Sentence 253 rankings: \n",
      "******在标签中\n",
      "Sentence 254 rankings: \n",
      "######不在标签中\n",
      "Sentence 255 rankings: \n",
      "******在标签中\n",
      "Sentence 256 rankings: \n",
      "######不在标签中\n",
      "Sentence 257 rankings: \n",
      "******在标签中\n",
      "Sentence 258 rankings: \n",
      "######不在标签中\n",
      "Sentence 259 rankings: \n",
      "******在标签中\n",
      "Sentence 260 rankings: \n",
      "******在标签中\n",
      "Sentence 261 rankings: \n",
      "******在标签中\n",
      "Sentence 262 rankings: \n",
      "######不在标签中\n",
      "Sentence 263 rankings: \n",
      "######不在标签中\n",
      "Sentence 264 rankings: \n",
      "******在标签中\n",
      "Sentence 265 rankings: \n",
      "######不在标签中\n",
      "Sentence 266 rankings: \n",
      "******在标签中\n",
      "Sentence 267 rankings: \n",
      "######不在标签中\n",
      "Sentence 268 rankings: \n",
      "******在标签中\n",
      "Sentence 269 rankings: \n",
      "######不在标签中\n",
      "Sentence 270 rankings: \n",
      "######不在标签中\n",
      "Sentence 271 rankings: \n",
      "******在标签中\n",
      "Sentence 272 rankings: \n",
      "******在标签中\n",
      "Sentence 273 rankings: \n",
      "******在标签中\n",
      "Sentence 274 rankings: \n",
      "######不在标签中\n",
      "Sentence 275 rankings: \n",
      "******在标签中\n",
      "Sentence 276 rankings: \n",
      "######不在标签中\n",
      "Sentence 277 rankings: \n",
      "******在标签中\n",
      "Sentence 278 rankings: \n",
      "******在标签中\n",
      "Sentence 279 rankings: \n",
      "######不在标签中\n",
      "Sentence 280 rankings: \n",
      "******在标签中\n",
      "Sentence 281 rankings: \n",
      "######不在标签中\n",
      "Sentence 282 rankings: \n",
      "******在标签中\n",
      "Sentence 283 rankings: \n",
      "******在标签中\n",
      "Sentence 284 rankings: \n",
      "******在标签中\n",
      "Sentence 285 rankings: \n",
      "******在标签中\n",
      "Sentence 286 rankings: \n",
      "######不在标签中\n",
      "Sentence 287 rankings: \n",
      "######不在标签中\n",
      "Sentence 288 rankings: \n",
      "######不在标签中\n",
      "Sentence 289 rankings: \n",
      "******在标签中\n",
      "Sentence 290 rankings: \n",
      "######不在标签中\n",
      "Sentence 291 rankings: \n",
      "******在标签中\n",
      "Sentence 292 rankings: \n",
      "******在标签中\n",
      "Sentence 293 rankings: \n",
      "******在标签中\n",
      "Sentence 294 rankings: \n",
      "******在标签中\n",
      "Sentence 295 rankings: \n",
      "******在标签中\n",
      "Sentence 296 rankings: \n",
      "******在标签中\n",
      "Sentence 297 rankings: \n",
      "######不在标签中\n",
      "Sentence 298 rankings: \n",
      "******在标签中\n",
      "Sentence 299 rankings: \n",
      "******在标签中\n",
      "Sentence 300 rankings: \n",
      "######不在标签中\n",
      "Sentence 301 rankings: \n",
      "******在标签中\n",
      "Sentence 302 rankings: \n",
      "******在标签中\n",
      "Sentence 303 rankings: \n",
      "******在标签中\n",
      "Sentence 304 rankings: \n",
      "******在标签中\n",
      "Sentence 305 rankings: \n",
      "******在标签中\n",
      "Sentence 306 rankings: \n",
      "******在标签中\n",
      "Sentence 307 rankings: \n",
      "######不在标签中\n",
      "Sentence 308 rankings: \n",
      "######不在标签中\n",
      "Sentence 309 rankings: \n",
      "******在标签中\n",
      "Sentence 310 rankings: \n",
      "******在标签中\n",
      "Sentence 311 rankings: \n",
      "******在标签中\n",
      "Sentence 312 rankings: \n",
      "******在标签中\n",
      "Sentence 313 rankings: \n",
      "******在标签中\n",
      "Sentence 314 rankings: \n",
      "******在标签中\n",
      "Sentence 315 rankings: \n",
      "******在标签中\n",
      "Sentence 316 rankings: \n",
      "******在标签中\n",
      "Sentence 317 rankings: \n",
      "******在标签中\n",
      "Sentence 318 rankings: \n",
      "******在标签中\n",
      "Sentence 319 rankings: \n",
      "******在标签中\n",
      "Sentence 320 rankings: \n",
      "######不在标签中\n",
      "Sentence 321 rankings: \n",
      "******在标签中\n",
      "Sentence 322 rankings: \n",
      "******在标签中\n",
      "Sentence 323 rankings: \n",
      "######不在标签中\n",
      "Sentence 324 rankings: \n",
      "******在标签中\n",
      "Sentence 325 rankings: \n",
      "######不在标签中\n",
      "Sentence 326 rankings: \n",
      "######不在标签中\n",
      "Sentence 327 rankings: \n",
      "******在标签中\n",
      "Sentence 328 rankings: \n",
      "******在标签中\n",
      "Sentence 329 rankings: \n",
      "######不在标签中\n",
      "Sentence 330 rankings: \n",
      "######不在标签中\n",
      "Sentence 331 rankings: \n",
      "******在标签中\n",
      "Sentence 332 rankings: \n",
      "******在标签中\n",
      "Sentence 333 rankings: \n",
      "******在标签中\n",
      "Sentence 334 rankings: \n",
      "******在标签中\n",
      "Sentence 335 rankings: \n",
      "######不在标签中\n",
      "Sentence 336 rankings: \n",
      "******在标签中\n",
      "Sentence 337 rankings: \n",
      "######不在标签中\n",
      "Sentence 338 rankings: \n",
      "******在标签中\n",
      "Sentence 339 rankings: \n",
      "******在标签中\n",
      "Sentence 340 rankings: \n",
      "******在标签中\n",
      "Sentence 341 rankings: \n",
      "******在标签中\n",
      "Sentence 342 rankings: \n",
      "******在标签中\n",
      "Sentence 343 rankings: \n",
      "******在标签中\n",
      "Sentence 344 rankings: \n",
      "######不在标签中\n",
      "Sentence 345 rankings: \n",
      "******在标签中\n",
      "Sentence 346 rankings: \n",
      "******在标签中\n",
      "Sentence 347 rankings: \n",
      "######不在标签中\n",
      "Sentence 348 rankings: \n",
      "******在标签中\n",
      "Sentence 349 rankings: \n",
      "******在标签中\n",
      "Sentence 350 rankings: \n",
      "######不在标签中\n",
      "Sentence 351 rankings: \n",
      "******在标签中\n",
      "Sentence 352 rankings: \n",
      "######不在标签中\n",
      "Sentence 353 rankings: \n",
      "######不在标签中\n",
      "Sentence 354 rankings: \n",
      "******在标签中\n",
      "Sentence 355 rankings: \n",
      "******在标签中\n",
      "Sentence 356 rankings: \n",
      "######不在标签中\n",
      "Sentence 357 rankings: \n",
      "######不在标签中\n",
      "Sentence 358 rankings: \n",
      "******在标签中\n",
      "Sentence 359 rankings: \n",
      "******在标签中\n",
      "Sentence 360 rankings: \n",
      "######不在标签中\n",
      "Sentence 361 rankings: \n",
      "######不在标签中\n",
      "Sentence 362 rankings: \n",
      "******在标签中\n",
      "Sentence 363 rankings: \n",
      "******在标签中\n",
      "Sentence 364 rankings: \n",
      "******在标签中\n",
      "Sentence 365 rankings: \n",
      "******在标签中\n",
      "Sentence 366 rankings: \n",
      "******在标签中\n",
      "Sentence 367 rankings: \n",
      "######不在标签中\n",
      "Sentence 368 rankings: \n",
      "######不在标签中\n",
      "Sentence 369 rankings: \n",
      "******在标签中\n",
      "Sentence 370 rankings: \n",
      "######不在标签中\n",
      "Sentence 371 rankings: \n",
      "######不在标签中\n",
      "Sentence 372 rankings: \n",
      "******在标签中\n",
      "Sentence 373 rankings: \n",
      "******在标签中\n",
      "Sentence 374 rankings: \n",
      "******在标签中\n",
      "Sentence 375 rankings: \n",
      "######不在标签中\n",
      "Sentence 376 rankings: \n",
      "######不在标签中\n",
      "Sentence 377 rankings: \n",
      "******在标签中\n",
      "Sentence 378 rankings: \n",
      "******在标签中\n",
      "Sentence 379 rankings: \n",
      "######不在标签中\n",
      "Sentence 380 rankings: \n",
      "######不在标签中\n",
      "Sentence 381 rankings: \n",
      "******在标签中\n",
      "Sentence 382 rankings: \n",
      "######不在标签中\n",
      "Sentence 383 rankings: \n",
      "######不在标签中\n",
      "Sentence 384 rankings: \n",
      "******在标签中\n",
      "Sentence 385 rankings: \n",
      "******在标签中\n",
      "Sentence 386 rankings: \n",
      "######不在标签中\n",
      "Sentence 387 rankings: \n",
      "######不在标签中\n",
      "Sentence 388 rankings: \n",
      "******在标签中\n",
      "Sentence 389 rankings: \n",
      "******在标签中\n",
      "Sentence 390 rankings: \n",
      "######不在标签中\n",
      "Sentence 391 rankings: \n",
      "######不在标签中\n",
      "Sentence 392 rankings: \n",
      "******在标签中\n",
      "Sentence 393 rankings: \n",
      "******在标签中\n",
      "Sentence 394 rankings: \n",
      "******在标签中\n",
      "Sentence 395 rankings: \n",
      "******在标签中\n",
      "Sentence 396 rankings: \n",
      "######不在标签中\n",
      "Sentence 397 rankings: \n",
      "######不在标签中\n",
      "Sentence 398 rankings: \n",
      "######不在标签中\n",
      "Sentence 399 rankings: \n",
      "******在标签中\n",
      "Sentence 400 rankings: \n",
      "******在标签中\n",
      "Sentence 401 rankings: \n",
      "******在标签中\n",
      "Sentence 402 rankings: \n",
      "******在标签中\n",
      "Sentence 403 rankings: \n",
      "******在标签中\n",
      "Sentence 404 rankings: \n",
      "******在标签中\n",
      "Sentence 405 rankings: \n",
      "######不在标签中\n",
      "Sentence 406 rankings: \n",
      "******在标签中\n",
      "Sentence 407 rankings: \n",
      "######不在标签中\n",
      "Sentence 408 rankings: \n",
      "******在标签中\n",
      "Sentence 409 rankings: \n",
      "******在标签中\n",
      "Sentence 410 rankings: \n",
      "######不在标签中\n",
      "Sentence 411 rankings: \n",
      "######不在标签中\n",
      "Sentence 412 rankings: \n",
      "######不在标签中\n",
      "Sentence 413 rankings: \n",
      "******在标签中\n",
      "Sentence 414 rankings: \n",
      "######不在标签中\n",
      "Sentence 415 rankings: \n",
      "******在标签中\n",
      "Sentence 416 rankings: \n",
      "******在标签中\n",
      "Sentence 417 rankings: \n",
      "######不在标签中\n",
      "Sentence 418 rankings: \n",
      "******在标签中\n",
      "Sentence 419 rankings: \n",
      "######不在标签中\n",
      "Sentence 420 rankings: \n",
      "######不在标签中\n",
      "Sentence 421 rankings: \n",
      "******在标签中\n",
      "Sentence 422 rankings: \n",
      "******在标签中\n",
      "Sentence 423 rankings: \n",
      "######不在标签中\n",
      "Sentence 424 rankings: \n",
      "######不在标签中\n",
      "Sentence 425 rankings: \n",
      "******在标签中\n",
      "Sentence 426 rankings: \n",
      "******在标签中\n",
      "Sentence 427 rankings: \n",
      "******在标签中\n",
      "Sentence 428 rankings: \n",
      "******在标签中\n",
      "Sentence 429 rankings: \n",
      "******在标签中\n",
      "Sentence 430 rankings: \n",
      "******在标签中\n",
      "Sentence 431 rankings: \n",
      "******在标签中\n",
      "Sentence 432 rankings: \n",
      "******在标签中\n",
      "Sentence 433 rankings: \n",
      "******在标签中\n",
      "Sentence 434 rankings: \n",
      "******在标签中\n",
      "Sentence 435 rankings: \n",
      "******在标签中\n",
      "Sentence 436 rankings: \n",
      "######不在标签中\n",
      "Sentence 437 rankings: \n",
      "******在标签中\n",
      "Sentence 438 rankings: \n",
      "******在标签中\n",
      "Sentence 439 rankings: \n",
      "######不在标签中\n",
      "Sentence 440 rankings: \n",
      "******在标签中\n",
      "Sentence 441 rankings: \n",
      "******在标签中\n",
      "Sentence 442 rankings: \n",
      "******在标签中\n",
      "Sentence 443 rankings: \n",
      "******在标签中\n",
      "Sentence 444 rankings: \n",
      "******在标签中\n",
      "Sentence 445 rankings: \n",
      "######不在标签中\n",
      "Sentence 446 rankings: \n",
      "******在标签中\n",
      "Sentence 447 rankings: \n",
      "######不在标签中\n",
      "Sentence 448 rankings: \n",
      "******在标签中\n",
      "Sentence 449 rankings: \n",
      "######不在标签中\n",
      "Sentence 450 rankings: \n",
      "######不在标签中\n",
      "Sentence 451 rankings: \n",
      "######不在标签中\n",
      "Sentence 452 rankings: \n",
      "******在标签中\n",
      "Sentence 453 rankings: \n",
      "******在标签中\n",
      "Sentence 454 rankings: \n",
      "######不在标签中\n",
      "Sentence 455 rankings: \n",
      "******在标签中\n",
      "Sentence 456 rankings: \n",
      "######不在标签中\n",
      "Sentence 457 rankings: \n",
      "******在标签中\n",
      "Sentence 458 rankings: \n",
      "******在标签中\n",
      "Sentence 459 rankings: \n",
      "######不在标签中\n",
      "Sentence 460 rankings: \n",
      "******在标签中\n",
      "Sentence 461 rankings: \n",
      "######不在标签中\n",
      "Sentence 462 rankings: \n",
      "######不在标签中\n",
      "Sentence 463 rankings: \n",
      "******在标签中\n",
      "Sentence 464 rankings: \n",
      "######不在标签中\n",
      "Sentence 465 rankings: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******在标签中\n",
      "Sentence 466 rankings: \n",
      "******在标签中\n",
      "Sentence 467 rankings: \n",
      "******在标签中\n",
      "Sentence 468 rankings: \n",
      "******在标签中\n",
      "Sentence 469 rankings: \n",
      "******在标签中\n",
      "Sentence 470 rankings: \n",
      "******在标签中\n",
      "Sentence 471 rankings: \n",
      "******在标签中\n",
      "Sentence 472 rankings: \n",
      "******在标签中\n",
      "Sentence 473 rankings: \n",
      "******在标签中\n",
      "Sentence 474 rankings: \n",
      "######不在标签中\n",
      "Sentence 475 rankings: \n",
      "******在标签中\n",
      "Sentence 476 rankings: \n",
      "******在标签中\n",
      "Sentence 477 rankings: \n",
      "######不在标签中\n",
      "Sentence 478 rankings: \n",
      "******在标签中\n",
      "Sentence 479 rankings: \n",
      "******在标签中\n",
      "Sentence 480 rankings: \n",
      "######不在标签中\n",
      "Sentence 481 rankings: \n",
      "******在标签中\n",
      "Sentence 482 rankings: \n",
      "******在标签中\n",
      "Sentence 483 rankings: \n",
      "######不在标签中\n",
      "Sentence 484 rankings: \n",
      "******在标签中\n",
      "Sentence 485 rankings: \n",
      "******在标签中\n",
      "Sentence 486 rankings: \n",
      "******在标签中\n",
      "Sentence 487 rankings: \n",
      "******在标签中\n",
      "Sentence 488 rankings: \n",
      "######不在标签中\n",
      "Sentence 489 rankings: \n",
      "######不在标签中\n",
      "Sentence 490 rankings: \n",
      "******在标签中\n",
      "Sentence 491 rankings: \n",
      "######不在标签中\n",
      "Sentence 492 rankings: \n",
      "######不在标签中\n",
      "Sentence 493 rankings: \n",
      "******在标签中\n",
      "Sentence 494 rankings: \n",
      "******在标签中\n",
      "Sentence 495 rankings: \n",
      "******在标签中\n",
      "Sentence 496 rankings: \n",
      "******在标签中\n",
      "Sentence 497 rankings: \n",
      "######不在标签中\n",
      "Sentence 498 rankings: \n",
      "######不在标签中\n",
      "Sentence 499 rankings: \n",
      "******在标签中\n",
      "Sentence 500 rankings: \n",
      "######不在标签中\n",
      "Sentence 501 rankings: \n",
      "######不在标签中\n",
      "Sentence 502 rankings: \n",
      "######不在标签中\n",
      "Sentence 503 rankings: \n",
      "######不在标签中\n",
      "Sentence 504 rankings: \n",
      "######不在标签中\n",
      "Sentence 505 rankings: \n",
      "######不在标签中\n",
      "Sentence 506 rankings: \n",
      "######不在标签中\n",
      "Sentence 507 rankings: \n",
      "######不在标签中\n",
      "Sentence 508 rankings: \n",
      "######不在标签中\n",
      "Sentence 509 rankings: \n",
      "######不在标签中\n",
      "Sentence 510 rankings: \n",
      "######不在标签中\n",
      "Sentence 511 rankings: \n",
      "******在标签中\n",
      "Sentence 512 rankings: \n",
      "######不在标签中\n",
      "Sentence 513 rankings: \n",
      "######不在标签中\n",
      "Sentence 514 rankings: \n",
      "######不在标签中\n",
      "Sentence 515 rankings: \n",
      "######不在标签中\n",
      "Sentence 516 rankings: \n",
      "******在标签中\n",
      "Sentence 517 rankings: \n",
      "######不在标签中\n",
      "Sentence 518 rankings: \n",
      "######不在标签中\n",
      "Sentence 519 rankings: \n",
      "######不在标签中\n",
      "Sentence 520 rankings: \n",
      "######不在标签中\n",
      "Sentence 521 rankings: \n",
      "######不在标签中\n",
      "Sentence 522 rankings: \n",
      "######不在标签中\n",
      "Sentence 523 rankings: \n",
      "******在标签中\n",
      "Sentence 524 rankings: \n",
      "######不在标签中\n",
      "Sentence 525 rankings: \n",
      "######不在标签中\n",
      "Sentence 526 rankings: \n",
      "******在标签中\n",
      "Sentence 527 rankings: \n",
      "######不在标签中\n",
      "Sentence 528 rankings: \n",
      "######不在标签中\n",
      "Sentence 529 rankings: \n",
      "******在标签中\n",
      "Sentence 530 rankings: \n",
      "######不在标签中\n",
      "Sentence 531 rankings: \n",
      "######不在标签中\n",
      "Sentence 532 rankings: \n",
      "######不在标签中\n",
      "Sentence 533 rankings: \n",
      "######不在标签中\n",
      "Sentence 534 rankings: \n",
      "######不在标签中\n",
      "Sentence 535 rankings: \n",
      "######不在标签中\n",
      "Sentence 536 rankings: \n",
      "######不在标签中\n",
      "Sentence 537 rankings: \n",
      "######不在标签中\n",
      "Sentence 538 rankings: \n",
      "######不在标签中\n",
      "Sentence 539 rankings: \n",
      "******在标签中\n",
      "Sentence 540 rankings: \n",
      "######不在标签中\n",
      "Sentence 541 rankings: \n",
      "******在标签中\n",
      "Sentence 542 rankings: \n",
      "******在标签中\n",
      "Sentence 543 rankings: \n",
      "######不在标签中\n",
      "Sentence 544 rankings: \n",
      "######不在标签中\n",
      "Sentence 545 rankings: \n",
      "######不在标签中\n",
      "Sentence 546 rankings: \n",
      "######不在标签中\n",
      "Sentence 547 rankings: \n",
      "######不在标签中\n",
      "Sentence 548 rankings: \n",
      "######不在标签中\n",
      "Sentence 549 rankings: \n",
      "######不在标签中\n",
      "Sentence 550 rankings: \n",
      "######不在标签中\n",
      "Sentence 551 rankings: \n",
      "######不在标签中\n",
      "Sentence 552 rankings: \n",
      "######不在标签中\n",
      "Sentence 553 rankings: \n",
      "######不在标签中\n",
      "Sentence 554 rankings: \n",
      "######不在标签中\n",
      "Sentence 555 rankings: \n",
      "######不在标签中\n",
      "Sentence 556 rankings: \n",
      "******在标签中\n",
      "Sentence 557 rankings: \n",
      "######不在标签中\n",
      "Sentence 558 rankings: \n",
      "######不在标签中\n",
      "Sentence 559 rankings: \n",
      "******在标签中\n",
      "Sentence 560 rankings: \n",
      "******在标签中\n",
      "Sentence 561 rankings: \n",
      "******在标签中\n",
      "Sentence 562 rankings: \n",
      "******在标签中\n",
      "Sentence 563 rankings: \n",
      "******在标签中\n",
      "Sentence 564 rankings: \n",
      "######不在标签中\n",
      "Sentence 565 rankings: \n",
      "******在标签中\n",
      "Sentence 566 rankings: \n",
      "******在标签中\n",
      "Sentence 567 rankings: \n",
      "******在标签中\n",
      "Sentence 568 rankings: \n",
      "######不在标签中\n",
      "Sentence 569 rankings: \n",
      "######不在标签中\n",
      "Sentence 570 rankings: \n",
      "######不在标签中\n",
      "Sentence 571 rankings: \n",
      "######不在标签中\n",
      "Sentence 572 rankings: \n",
      "******在标签中\n",
      "Sentence 573 rankings: \n",
      "######不在标签中\n",
      "Sentence 574 rankings: \n",
      "######不在标签中\n",
      "Sentence 575 rankings: \n",
      "######不在标签中\n",
      "Sentence 576 rankings: \n",
      "######不在标签中\n",
      "Sentence 577 rankings: \n",
      "######不在标签中\n",
      "Sentence 578 rankings: \n",
      "######不在标签中\n",
      "Sentence 579 rankings: \n",
      "######不在标签中\n",
      "Sentence 580 rankings: \n",
      "######不在标签中\n",
      "Sentence 581 rankings: \n",
      "******在标签中\n",
      "Sentence 582 rankings: \n",
      "######不在标签中\n",
      "Sentence 583 rankings: \n",
      "######不在标签中\n",
      "Sentence 584 rankings: \n",
      "******在标签中\n",
      "Sentence 585 rankings: \n",
      "######不在标签中\n",
      "Sentence 586 rankings: \n",
      "######不在标签中\n",
      "Sentence 587 rankings: \n",
      "******在标签中\n",
      "Sentence 588 rankings: \n",
      "######不在标签中\n",
      "Sentence 589 rankings: \n",
      "******在标签中\n",
      "Sentence 590 rankings: \n",
      "******在标签中\n",
      "Sentence 591 rankings: \n",
      "******在标签中\n",
      "Sentence 592 rankings: \n",
      "******在标签中\n",
      "Sentence 593 rankings: \n",
      "******在标签中\n",
      "Sentence 594 rankings: \n",
      "******在标签中\n",
      "Sentence 595 rankings: \n",
      "******在标签中\n",
      "Sentence 596 rankings: \n",
      "******在标签中\n",
      "Sentence 597 rankings: \n",
      "******在标签中\n",
      "Sentence 598 rankings: \n",
      "######不在标签中\n",
      "Sentence 599 rankings: \n",
      "######不在标签中\n",
      "Sentence 600 rankings: \n",
      "******在标签中\n",
      "Sentence 601 rankings: \n",
      "******在标签中\n",
      "Sentence 602 rankings: \n",
      "######不在标签中\n",
      "Sentence 603 rankings: \n",
      "******在标签中\n",
      "Sentence 604 rankings: \n",
      "******在标签中\n",
      "Sentence 605 rankings: \n",
      "######不在标签中\n",
      "Sentence 606 rankings: \n",
      "######不在标签中\n",
      "Sentence 607 rankings: \n",
      "******在标签中\n",
      "Sentence 608 rankings: \n",
      "******在标签中\n",
      "Sentence 609 rankings: \n",
      "******在标签中\n",
      "Sentence 610 rankings: \n",
      "######不在标签中\n",
      "Sentence 611 rankings: \n",
      "******在标签中\n",
      "Sentence 612 rankings: \n",
      "******在标签中\n",
      "Sentence 613 rankings: \n",
      "######不在标签中\n",
      "Sentence 614 rankings: \n",
      "******在标签中\n",
      "Sentence 615 rankings: \n",
      "######不在标签中\n",
      "Sentence 616 rankings: \n",
      "******在标签中\n",
      "Sentence 617 rankings: \n",
      "######不在标签中\n",
      "Sentence 618 rankings: \n",
      "######不在标签中\n",
      "Sentence 619 rankings: \n",
      "******在标签中\n",
      "Sentence 620 rankings: \n",
      "######不在标签中\n",
      "Sentence 621 rankings: \n",
      "######不在标签中\n",
      "Sentence 622 rankings: \n",
      "######不在标签中\n",
      "Sentence 623 rankings: \n",
      "######不在标签中\n",
      "Sentence 624 rankings: \n",
      "######不在标签中\n",
      "Sentence 625 rankings: \n",
      "######不在标签中\n",
      "Sentence 626 rankings: \n",
      "******在标签中\n",
      "Sentence 627 rankings: \n",
      "######不在标签中\n",
      "Sentence 628 rankings: \n",
      "######不在标签中\n",
      "Sentence 629 rankings: \n",
      "******在标签中\n",
      "Sentence 630 rankings: \n",
      "******在标签中\n",
      "Sentence 631 rankings: \n",
      "******在标签中\n",
      "Sentence 632 rankings: \n",
      "######不在标签中\n",
      "Sentence 633 rankings: \n",
      "******在标签中\n",
      "Sentence 634 rankings: \n",
      "******在标签中\n",
      "Sentence 635 rankings: \n",
      "######不在标签中\n",
      "Sentence 636 rankings: \n",
      "******在标签中\n",
      "Sentence 637 rankings: \n",
      "******在标签中\n",
      "Sentence 638 rankings: \n",
      "******在标签中\n",
      "Sentence 639 rankings: \n",
      "######不在标签中\n",
      "Sentence 640 rankings: \n",
      "******在标签中\n",
      "Sentence 641 rankings: \n",
      "######不在标签中\n",
      "Sentence 642 rankings: \n",
      "######不在标签中\n",
      "Sentence 643 rankings: \n",
      "******在标签中\n",
      "Sentence 644 rankings: \n",
      "******在标签中\n",
      "Sentence 645 rankings: \n",
      "######不在标签中\n",
      "Sentence 646 rankings: \n",
      "######不在标签中\n",
      "Sentence 647 rankings: \n",
      "******在标签中\n",
      "Sentence 648 rankings: \n",
      "######不在标签中\n",
      "Sentence 649 rankings: \n",
      "######不在标签中\n",
      "Sentence 650 rankings: \n",
      "######不在标签中\n",
      "Sentence 651 rankings: \n",
      "######不在标签中\n",
      "Sentence 652 rankings: \n",
      "######不在标签中\n",
      "Sentence 653 rankings: \n",
      "######不在标签中\n",
      "Sentence 654 rankings: \n",
      "######不在标签中\n",
      "Sentence 655 rankings: \n",
      "******在标签中\n",
      "Sentence 656 rankings: \n",
      "######不在标签中\n",
      "Sentence 657 rankings: \n",
      "######不在标签中\n",
      "Sentence 658 rankings: \n",
      "######不在标签中\n",
      "Sentence 659 rankings: \n",
      "******在标签中\n",
      "Sentence 660 rankings: \n",
      "######不在标签中\n",
      "Sentence 661 rankings: \n",
      "######不在标签中\n",
      "Sentence 662 rankings: \n",
      "######不在标签中\n",
      "Sentence 663 rankings: \n",
      "######不在标签中\n",
      "Sentence 664 rankings: \n",
      "######不在标签中\n",
      "Sentence 665 rankings: \n",
      "******在标签中\n",
      "Sentence 666 rankings: \n",
      "######不在标签中\n",
      "Sentence 667 rankings: \n",
      "******在标签中\n",
      "Sentence 668 rankings: \n",
      "******在标签中\n",
      "Sentence 669 rankings: \n",
      "******在标签中\n",
      "Sentence 670 rankings: \n",
      "******在标签中\n",
      "Sentence 671 rankings: \n",
      "******在标签中\n",
      "Sentence 672 rankings: \n",
      "******在标签中\n",
      "Sentence 673 rankings: \n",
      "******在标签中\n",
      "Sentence 674 rankings: \n",
      "******在标签中\n",
      "Sentence 675 rankings: \n",
      "******在标签中\n",
      "Sentence 676 rankings: \n",
      "******在标签中\n",
      "Sentence 677 rankings: \n",
      "######不在标签中\n",
      "Sentence 678 rankings: \n",
      "******在标签中\n",
      "Sentence 679 rankings: \n",
      "######不在标签中\n",
      "Sentence 680 rankings: \n",
      "######不在标签中\n",
      "Sentence 681 rankings: \n",
      "******在标签中\n",
      "Sentence 682 rankings: \n",
      "######不在标签中\n",
      "Sentence 683 rankings: \n",
      "******在标签中\n",
      "Sentence 684 rankings: \n",
      "######不在标签中\n",
      "Sentence 685 rankings: \n",
      "######不在标签中\n",
      "Sentence 686 rankings: \n",
      "******在标签中\n",
      "Sentence 687 rankings: \n",
      "######不在标签中\n",
      "Sentence 688 rankings: \n",
      "######不在标签中\n",
      "Sentence 689 rankings: \n",
      "######不在标签中\n",
      "Sentence 690 rankings: \n",
      "######不在标签中\n",
      "Sentence 691 rankings: \n",
      "######不在标签中\n",
      "Sentence 692 rankings: \n",
      "######不在标签中\n",
      "Sentence 693 rankings: \n",
      "######不在标签中\n",
      "Sentence 694 rankings: \n",
      "######不在标签中\n",
      "Sentence 695 rankings: \n",
      "######不在标签中\n",
      "Sentence 696 rankings: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######不在标签中\n",
      "Sentence 697 rankings: \n",
      "******在标签中\n",
      "Sentence 698 rankings: \n",
      "######不在标签中\n",
      "Sentence 699 rankings: \n",
      "######不在标签中\n",
      "Sentence 700 rankings: \n",
      "######不在标签中\n",
      "Sentence 701 rankings: \n",
      "******在标签中\n",
      "Sentence 702 rankings: \n",
      "******在标签中\n",
      "Sentence 703 rankings: \n",
      "******在标签中\n",
      "Sentence 704 rankings: \n",
      "******在标签中\n",
      "Sentence 705 rankings: \n",
      "######不在标签中\n",
      "Sentence 706 rankings: \n",
      "******在标签中\n",
      "Sentence 707 rankings: \n",
      "######不在标签中\n",
      "Sentence 708 rankings: \n",
      "######不在标签中\n",
      "Sentence 709 rankings: \n",
      "******在标签中\n",
      "Sentence 710 rankings: \n",
      "######不在标签中\n",
      "Sentence 711 rankings: \n",
      "######不在标签中\n",
      "Sentence 712 rankings: \n",
      "######不在标签中\n",
      "Sentence 713 rankings: \n",
      "######不在标签中\n",
      "Sentence 714 rankings: \n",
      "######不在标签中\n",
      "Sentence 715 rankings: \n",
      "******在标签中\n",
      "Sentence 716 rankings: \n",
      "######不在标签中\n",
      "Sentence 717 rankings: \n",
      "######不在标签中\n",
      "Sentence 718 rankings: \n",
      "######不在标签中\n",
      "Sentence 719 rankings: \n",
      "######不在标签中\n",
      "Sentence 720 rankings: \n",
      "######不在标签中\n",
      "Sentence 721 rankings: \n",
      "******在标签中\n",
      "Sentence 722 rankings: \n",
      "******在标签中\n",
      "Sentence 723 rankings: \n",
      "######不在标签中\n",
      "Sentence 724 rankings: \n",
      "######不在标签中\n",
      "Sentence 725 rankings: \n",
      "######不在标签中\n",
      "Sentence 726 rankings: \n",
      "******在标签中\n",
      "Sentence 727 rankings: \n",
      "******在标签中\n",
      "Sentence 728 rankings: \n",
      "******在标签中\n",
      "Sentence 729 rankings: \n",
      "******在标签中\n",
      "Sentence 730 rankings: \n",
      "######不在标签中\n",
      "Sentence 731 rankings: \n",
      "######不在标签中\n",
      "Sentence 732 rankings: \n",
      "######不在标签中\n",
      "Sentence 733 rankings: \n",
      "******在标签中\n",
      "Sentence 734 rankings: \n",
      "######不在标签中\n",
      "Sentence 735 rankings: \n",
      "******在标签中\n",
      "Sentence 736 rankings: \n",
      "******在标签中\n",
      "Sentence 737 rankings: \n",
      "******在标签中\n",
      "Sentence 738 rankings: \n",
      "######不在标签中\n",
      "Sentence 739 rankings: \n",
      "######不在标签中\n",
      "Sentence 740 rankings: \n",
      "######不在标签中\n",
      "Sentence 741 rankings: \n",
      "######不在标签中\n",
      "Sentence 742 rankings: \n",
      "######不在标签中\n",
      "Sentence 743 rankings: \n",
      "******在标签中\n",
      "Sentence 744 rankings: \n",
      "######不在标签中\n",
      "Sentence 745 rankings: \n",
      "######不在标签中\n",
      "Sentence 746 rankings: \n",
      "######不在标签中\n",
      "Sentence 747 rankings: \n",
      "######不在标签中\n",
      "Sentence 748 rankings: \n",
      "######不在标签中\n",
      "Sentence 749 rankings: \n",
      "******在标签中\n",
      "Sentence 750 rankings: \n",
      "######不在标签中\n",
      "Sentence 751 rankings: \n",
      "******在标签中\n",
      "Sentence 752 rankings: \n",
      "******在标签中\n",
      "Sentence 753 rankings: \n",
      "******在标签中\n",
      "Sentence 754 rankings: \n",
      "******在标签中\n",
      "Sentence 755 rankings: \n",
      "######不在标签中\n",
      "Sentence 756 rankings: \n",
      "######不在标签中\n",
      "Sentence 757 rankings: \n",
      "******在标签中\n",
      "Sentence 758 rankings: \n",
      "******在标签中\n",
      "Sentence 759 rankings: \n",
      "######不在标签中\n",
      "Sentence 760 rankings: \n",
      "######不在标签中\n",
      "Sentence 761 rankings: \n",
      "######不在标签中\n",
      "Sentence 762 rankings: \n",
      "######不在标签中\n",
      "Sentence 763 rankings: \n",
      "******在标签中\n",
      "Sentence 764 rankings: \n",
      "######不在标签中\n",
      "Sentence 765 rankings: \n",
      "******在标签中\n",
      "Sentence 766 rankings: \n",
      "######不在标签中\n",
      "Sentence 767 rankings: \n",
      "######不在标签中\n",
      "Sentence 768 rankings: \n",
      "######不在标签中\n",
      "Sentence 769 rankings: \n",
      "######不在标签中\n",
      "Sentence 770 rankings: \n",
      "######不在标签中\n",
      "Sentence 771 rankings: \n",
      "******在标签中\n",
      "Sentence 772 rankings: \n",
      "######不在标签中\n",
      "Sentence 773 rankings: \n",
      "######不在标签中\n",
      "Sentence 774 rankings: \n",
      "######不在标签中\n",
      "Sentence 775 rankings: \n",
      "######不在标签中\n",
      "Sentence 776 rankings: \n",
      "######不在标签中\n",
      "Sentence 777 rankings: \n",
      "******在标签中\n",
      "Sentence 778 rankings: \n",
      "******在标签中\n",
      "Sentence 779 rankings: \n",
      "######不在标签中\n",
      "Sentence 780 rankings: \n",
      "******在标签中\n",
      "Sentence 781 rankings: \n",
      "******在标签中\n",
      "Sentence 782 rankings: \n",
      "######不在标签中\n",
      "Sentence 783 rankings: \n",
      "******在标签中\n",
      "Sentence 784 rankings: \n",
      "******在标签中\n",
      "Sentence 785 rankings: \n",
      "******在标签中\n",
      "Sentence 786 rankings: \n",
      "******在标签中\n",
      "Sentence 787 rankings: \n",
      "******在标签中\n",
      "Sentence 788 rankings: \n",
      "******在标签中\n",
      "Sentence 789 rankings: \n",
      "######不在标签中\n",
      "Sentence 790 rankings: \n",
      "######不在标签中\n",
      "Sentence 791 rankings: \n",
      "######不在标签中\n",
      "Sentence 792 rankings: \n",
      "######不在标签中\n",
      "Sentence 793 rankings: \n",
      "******在标签中\n",
      "Sentence 794 rankings: \n",
      "######不在标签中\n",
      "Sentence 795 rankings: \n",
      "######不在标签中\n",
      "Sentence 796 rankings: \n",
      "######不在标签中\n",
      "Sentence 797 rankings: \n",
      "######不在标签中\n",
      "Sentence 798 rankings: \n",
      "######不在标签中\n",
      "Sentence 799 rankings: \n",
      "######不在标签中\n",
      "Sentence 800 rankings: \n",
      "######不在标签中\n",
      "Sentence 801 rankings: \n",
      "######不在标签中\n",
      "Sentence 802 rankings: \n",
      "######不在标签中\n",
      "Sentence 803 rankings: \n",
      "******在标签中\n",
      "Sentence 804 rankings: \n",
      "######不在标签中\n",
      "Sentence 805 rankings: \n",
      "######不在标签中\n",
      "Sentence 806 rankings: \n",
      "######不在标签中\n",
      "Sentence 807 rankings: \n",
      "######不在标签中\n",
      "Sentence 808 rankings: \n",
      "######不在标签中\n",
      "Sentence 809 rankings: \n",
      "######不在标签中\n",
      "Sentence 810 rankings: \n",
      "######不在标签中\n",
      "Sentence 811 rankings: \n",
      "######不在标签中\n",
      "Sentence 812 rankings: \n",
      "######不在标签中\n",
      "Sentence 813 rankings: \n",
      "######不在标签中\n",
      "Sentence 814 rankings: \n",
      "######不在标签中\n",
      "Sentence 815 rankings: \n",
      "######不在标签中\n",
      "Sentence 816 rankings: \n",
      "######不在标签中\n",
      "Sentence 817 rankings: \n",
      "######不在标签中\n",
      "Sentence 818 rankings: \n",
      "######不在标签中\n",
      "Sentence 819 rankings: \n",
      "******在标签中\n",
      "Sentence 820 rankings: \n",
      "******在标签中\n",
      "Sentence 821 rankings: \n",
      "######不在标签中\n",
      "Sentence 822 rankings: \n",
      "******在标签中\n",
      "Sentence 823 rankings: \n",
      "******在标签中\n",
      "Sentence 824 rankings: \n",
      "******在标签中\n",
      "Sentence 825 rankings: \n",
      "******在标签中\n",
      "Sentence 826 rankings: \n",
      "######不在标签中\n",
      "Sentence 827 rankings: \n",
      "******在标签中\n",
      "Sentence 828 rankings: \n",
      "******在标签中\n",
      "Sentence 829 rankings: \n",
      "######不在标签中\n",
      "Sentence 830 rankings: \n",
      "######不在标签中\n",
      "Sentence 831 rankings: \n",
      "######不在标签中\n",
      "Sentence 832 rankings: \n",
      "******在标签中\n",
      "Sentence 833 rankings: \n",
      "######不在标签中\n",
      "Sentence 834 rankings: \n",
      "######不在标签中\n",
      "Sentence 835 rankings: \n",
      "######不在标签中\n",
      "Sentence 836 rankings: \n",
      "######不在标签中\n",
      "Sentence 837 rankings: \n",
      "######不在标签中\n",
      "Sentence 838 rankings: \n",
      "######不在标签中\n",
      "Sentence 839 rankings: \n",
      "******在标签中\n",
      "Sentence 840 rankings: \n",
      "######不在标签中\n",
      "Sentence 841 rankings: \n",
      "******在标签中\n",
      "Sentence 842 rankings: \n",
      "******在标签中\n",
      "Sentence 843 rankings: \n",
      "******在标签中\n",
      "Sentence 844 rankings: \n",
      "######不在标签中\n",
      "Sentence 845 rankings: \n",
      "######不在标签中\n",
      "Sentence 846 rankings: \n",
      "******在标签中\n",
      "Sentence 847 rankings: \n",
      "######不在标签中\n",
      "Sentence 848 rankings: \n",
      "******在标签中\n",
      "Sentence 849 rankings: \n",
      "######不在标签中\n",
      "Sentence 850 rankings: \n",
      "******在标签中\n",
      "Sentence 851 rankings: \n",
      "######不在标签中\n",
      "Sentence 852 rankings: \n",
      "######不在标签中\n",
      "Sentence 853 rankings: \n",
      "######不在标签中\n",
      "Sentence 854 rankings: \n",
      "######不在标签中\n",
      "Sentence 855 rankings: \n",
      "######不在标签中\n",
      "Sentence 856 rankings: \n",
      "######不在标签中\n",
      "Sentence 857 rankings: \n",
      "######不在标签中\n",
      "Sentence 858 rankings: \n",
      "######不在标签中\n",
      "Sentence 859 rankings: \n",
      "******在标签中\n",
      "Sentence 860 rankings: \n",
      "******在标签中\n",
      "Sentence 861 rankings: \n",
      "******在标签中\n",
      "Sentence 862 rankings: \n",
      "******在标签中\n",
      "Sentence 863 rankings: \n",
      "******在标签中\n",
      "Sentence 864 rankings: \n",
      "######不在标签中\n",
      "Sentence 865 rankings: \n",
      "******在标签中\n",
      "Sentence 866 rankings: \n",
      "######不在标签中\n",
      "Sentence 867 rankings: \n",
      "######不在标签中\n",
      "Sentence 868 rankings: \n",
      "######不在标签中\n",
      "Sentence 869 rankings: \n",
      "******在标签中\n",
      "Sentence 870 rankings: \n",
      "******在标签中\n",
      "Sentence 871 rankings: \n",
      "******在标签中\n",
      "Sentence 872 rankings: \n",
      "******在标签中\n",
      "Sentence 873 rankings: \n",
      "******在标签中\n",
      "Sentence 874 rankings: \n",
      "######不在标签中\n",
      "Sentence 875 rankings: \n",
      "******在标签中\n",
      "Sentence 876 rankings: \n",
      "******在标签中\n",
      "Sentence 877 rankings: \n",
      "######不在标签中\n",
      "Sentence 878 rankings: \n",
      "######不在标签中\n",
      "Sentence 879 rankings: \n",
      "######不在标签中\n",
      "Sentence 880 rankings: \n",
      "******在标签中\n",
      "Sentence 881 rankings: \n",
      "######不在标签中\n",
      "Sentence 882 rankings: \n",
      "######不在标签中\n",
      "Sentence 883 rankings: \n",
      "######不在标签中\n",
      "Sentence 884 rankings: \n",
      "######不在标签中\n",
      "Sentence 885 rankings: \n",
      "******在标签中\n",
      "Sentence 886 rankings: \n",
      "******在标签中\n",
      "Sentence 887 rankings: \n",
      "######不在标签中\n",
      "Sentence 888 rankings: \n",
      "######不在标签中\n",
      "Sentence 889 rankings: \n",
      "######不在标签中\n",
      "Sentence 890 rankings: \n",
      "******在标签中\n",
      "Sentence 891 rankings: \n",
      "******在标签中\n",
      "Sentence 892 rankings: \n",
      "******在标签中\n",
      "Sentence 893 rankings: \n",
      "######不在标签中\n",
      "Sentence 894 rankings: \n",
      "######不在标签中\n",
      "Sentence 895 rankings: \n",
      "******在标签中\n",
      "Sentence 896 rankings: \n",
      "######不在标签中\n",
      "Sentence 897 rankings: \n",
      "******在标签中\n",
      "Sentence 898 rankings: \n",
      "######不在标签中\n",
      "Sentence 899 rankings: \n",
      "******在标签中\n",
      "Sentence 900 rankings: \n",
      "******在标签中\n",
      "Sentence 901 rankings: \n",
      "******在标签中\n",
      "Sentence 902 rankings: \n",
      "******在标签中\n",
      "Sentence 903 rankings: \n",
      "******在标签中\n",
      "Sentence 904 rankings: \n",
      "******在标签中\n",
      "Sentence 905 rankings: \n",
      "******在标签中\n",
      "Sentence 906 rankings: \n",
      "******在标签中\n",
      "Sentence 907 rankings: \n",
      "******在标签中\n",
      "Sentence 908 rankings: \n",
      "******在标签中\n",
      "Sentence 909 rankings: \n",
      "******在标签中\n",
      "Sentence 910 rankings: \n",
      "######不在标签中\n",
      "Sentence 911 rankings: \n",
      "######不在标签中\n",
      "Sentence 912 rankings: \n",
      "######不在标签中\n",
      "Sentence 913 rankings: \n",
      "******在标签中\n",
      "Sentence 914 rankings: \n",
      "******在标签中\n",
      "Sentence 915 rankings: \n",
      "######不在标签中\n",
      "Sentence 916 rankings: \n",
      "######不在标签中\n",
      "Sentence 917 rankings: \n",
      "******在标签中\n",
      "Sentence 918 rankings: \n",
      "******在标签中\n",
      "Sentence 919 rankings: \n",
      "######不在标签中\n",
      "Sentence 920 rankings: \n",
      "######不在标签中\n",
      "Sentence 921 rankings: \n",
      "######不在标签中\n",
      "Sentence 922 rankings: \n",
      "######不在标签中\n",
      "Sentence 923 rankings: \n",
      "######不在标签中\n",
      "Sentence 924 rankings: \n",
      "******在标签中\n",
      "Sentence 925 rankings: \n",
      "######不在标签中\n",
      "Sentence 926 rankings: \n",
      "######不在标签中\n",
      "Sentence 927 rankings: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******在标签中\n",
      "Sentence 928 rankings: \n",
      "######不在标签中\n",
      "The score of evaluation for BERT candidate generation\n",
      "0.9246501614639397 0.2442411194833154 0.33143441425650016 0.28123450669310857\n",
      "The score of evaluation for full LS pipeline\n",
      "0.721205597416577 0.5435952637244349 0.8223896663078579\n"
     ]
    }
   ],
   "source": [
    "for num in range(1):\n",
    "    args.seed=42\n",
    "    print(args.seed)\n",
    "    print('第 ',num,'次循环')\n",
    "    print(args.bert_model)\n",
    "    \n",
    "    CGBERT = []\n",
    "    CSBERT = []\n",
    "    SS = []\n",
    "    substitution_words = []\n",
    "   \n",
    "    num_selection = args.num_selections\n",
    "\n",
    "    bre_i=0\n",
    "\n",
    "    window_context = 11\n",
    "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        \n",
    "     \n",
    "        fileName = args.eval_dir.split('/')[-1][:-4]\n",
    "        if fileName=='lex.mturk':\n",
    "            eval_examples, mask_words, mask_labels = read_eval_dataset(args.eval_dir)\n",
    "        else:\n",
    "            eval_examples, mask_words, mask_labels = read_eval_index_dataset(args.eval_dir)\n",
    "\n",
    "       \n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "            #logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "      \n",
    "        eval_size = len(eval_examples)\n",
    "\n",
    "        print(\"prob_mask:\",args.prob_mask)\n",
    "    \n",
    "    \n",
    "    for i in range(eval_size):\n",
    "        print('Sentence {} rankings: '.format(i))\n",
    "        #output_sr_file.write(str(i))\n",
    "        #output_sr_file.write(' sentence: ')\n",
    "        #output_sr_file.write('\\n')\n",
    "        #print('sentence  ',eval_examples[i])\n",
    "        #print('mask word  ',mask_words[i])\n",
    "        tokens, words, position = convert_sentence_to_token(eval_examples[i], args.max_seq_length, tokenizer)\n",
    "\n",
    "        assert len(words)==len(position)\n",
    "\n",
    "        mask_index = words.index(mask_words[i])\n",
    "\n",
    "        mask_context = extract_context(words,mask_index,window_context)\n",
    "\n",
    "        len_tokens = len(tokens)\n",
    "\n",
    "        mask_position = position[mask_index]\n",
    " \n",
    "        if isinstance(mask_position,list):\n",
    "            feature = convert_whole_word_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "        else:\n",
    "            feature = convert_token_to_feature(tokens, mask_position, args.max_seq_length, tokenizer, args.prob_mask)\n",
    "\n",
    "        tokens_tensor = torch.tensor([feature.input_ids])\n",
    "\n",
    "        token_type_ids = torch.tensor([feature.input_type_ids])\n",
    "\n",
    "        attention_mask = torch.tensor([feature.input_mask])\n",
    "    \n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        token_type_ids = token_type_ids.to('cuda')\n",
    "        attention_mask = attention_mask.to('cuda')\n",
    "\n",
    "            # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            all_attentions,prediction_scores = model(tokens_tensor, token_type_ids,attention_mask)\n",
    "\n",
    "\n",
    "            \n",
    "        if isinstance(mask_position,list):\n",
    "            predicted_top = prediction_scores[0, mask_position[0]].topk(80)\n",
    "        else:\n",
    "            predicted_top = prediction_scores[0, mask_position].topk(80)\n",
    "            #print(predicted_top[0].cpu().numpy())\n",
    "        pre_tokens = tokenizer.convert_ids_to_tokens(predicted_top[1].cpu().numpy())\n",
    "            \n",
    "        #print(predicted_top[0].cpu().numpy())\n",
    "\n",
    "        sentence = eval_examples[i].lower()\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        words_tag = nltk.pos_tag(words)\n",
    "\n",
    "        complex_word_index = words.index(mask_words[i])\n",
    "\n",
    "        complex_word_tag = words_tag[complex_word_index][1]\n",
    "\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        complex_word_tag = preprocess_tag(complex_word_tag)\n",
    "        #print(complex_word_tag)\n",
    "\n",
    "        #mask_context = extract_context(words,complex_word_index,window_context)\n",
    "\n",
    "        #break\n",
    "        #print(mask_words[i])\n",
    "        #cgPPDB,csPPDB = PPDB_candidate_generation(mask_words[i],complex_word_tag, ppdb_model, ps, word_count, 30)\n",
    "        cgPPDB = ppdb_model.predict(mask_words[i],complex_word_tag)\n",
    "\n",
    "        #if(len(cgPPDB)>30):\n",
    "        #cgPPDB=cgPPDB[:30]\n",
    "\n",
    "        #break\n",
    "        cgBERT = BERT_candidate_generation(mask_words[i], pre_tokens, predicted_top[0].cpu().numpy(), ps, args.num_selections)\n",
    "            \n",
    "        CGBERT.append(cgBERT)\n",
    "\n",
    "\n",
    "        pre_word = substitution_ranking(mask_words[i], mask_context, cgBERT, fasttext_dico, fasttext_emb,word_count,cgPPDB,tokenizer,model,mask_labels[i])\n",
    "        \n",
    "        #print('best candidation ** ：  ',pre_word)\n",
    "        if pre_word in mask_labels[i]:\n",
    "            print('******在标签中')\n",
    "        else:\n",
    "            print('######不在标签中')\n",
    "        substitution_words.append(pre_word)\n",
    "    potential,precision,recall,F_score=evaulation_SS_scores(CGBERT, mask_labels)\n",
    "    print(\"The score of evaluation for BERT candidate generation\")\n",
    "    print(potential,precision,recall,F_score)\n",
    "\n",
    "        \n",
    "\n",
    "    precision,accuracy,changed_proportion=evaulation_pipeline_scores(substitution_words, mask_words, mask_labels)\n",
    "    print(\"The score of evaluation for full LS pipeline\")\n",
    "    print(precision,accuracy,changed_proportion)\n",
    "\n",
    "\n",
    "        #output_sr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "# 音频初始化\n",
    "pygame.mixer.init()\n",
    "# 加载音频文件路径 (路径必须真实存在，音频文件格式支持mp3/ogg等格式)\n",
    "pygame.mixer.music.load(r'D:/1.mp3')\n",
    "pygame.mixer.music.play()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
