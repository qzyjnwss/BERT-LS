{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertForMaskedLM\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "\n",
    "\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def convert_sentence_to_token(sentence, complex_word, seq_length, tokenizer):\n",
    "\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    #replace_sentence = sentence.lower().replace(complex_word,'[MASK]')\n",
    "\n",
    "    #print(replace_sentence)\n",
    "\n",
    "    index_complex = tokens.index(complex_word)\n",
    "\n",
    "    tokens[index_complex] = '[MASK]'\n",
    "\n",
    "\n",
    "\n",
    "    #print(complex_word,index_complex)\n",
    "\n",
    "  \n",
    "    new_sentence = '[CLS] '\n",
    "\n",
    "    \n",
    "\n",
    "   # new_sentence += complex_word\n",
    "    #new_sentence += ' [SEP] '\n",
    "\n",
    "    for word in tokens:\n",
    "        new_sentence += word\n",
    "        new_sentence += ' '\n",
    "\n",
    "    #new_sentence += replace_sentence\n",
    "\n",
    "    new_sentence += ' [SEP]'\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(new_sentence)\n",
    "\n",
    "    print(tokenized_text)\n",
    "\n",
    "\n",
    "    index_mask = tokenized_text.index('[MASK]')\n",
    "\n",
    "    #print(tokenized_text.index('[SEP]'))\n",
    "\n",
    "    return tokenized_text, index_mask\n",
    "\n",
    "def convert_token_to_feature(tokens, seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
    "\n",
    "    #tokens_a = tokenizer.tokenize(sentence)\n",
    "    #print(mask_position)\n",
    "\n",
    "   \n",
    "    input_type_ids = []\n",
    "\n",
    "    sep_index = tokens.index('[SEP]')\n",
    "\n",
    "    input_type_ids = [0] * (sep_index+1)\n",
    "\n",
    "    input_type_ids += [1]*(len(tokens)-sep_index-1)\n",
    "\n",
    "    #print(tokens)\n",
    "\n",
    "    #print(input_type_ids)\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "      \n",
    "    return InputFeatures(unique_id=0,  tokens=tokens, input_ids=input_ids,input_mask=input_mask,input_type_ids=input_type_ids)\n",
    "    \n",
    "\n",
    "def getWordmap(wordVecPath):\n",
    "    words=[]\n",
    "    We = []\n",
    "    f = open(wordVecPath,'r',encoding=\"utf-8\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for (n,line) in enumerate(lines):\n",
    "        if (n == 0) :\n",
    "            print(line)\n",
    "            continue\n",
    "        word, vect = line.rstrip().split(' ', 1)\n",
    "                    \n",
    "        vect = np.fromstring(vect, sep=' ')\n",
    "                \n",
    "        We.append(vect)\n",
    "\n",
    "        words.append(word)\n",
    "\n",
    "        #if(n==200000):\n",
    "        #    break\n",
    "    f.close()       \n",
    "    return (words, We)\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "\n",
    "def getWordCount(word_count_path):\n",
    "    word2count = {}\n",
    "    with open(word_count_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        N = 0\n",
    "        for i in lines:\n",
    "            i=i.strip()\n",
    "            if(len(i) > 0):\n",
    "                i=i.split()\n",
    "                if(len(i) == 2):\n",
    "                    word2count[i[0]] = float(i[1])\n",
    "                else:\n",
    "                    print(i)\n",
    "        \n",
    "    return word2count\n",
    "\n",
    "def read_eval_index_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            \n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "            mask_word,labels = words.strip().split('\\t',1)\n",
    "            label = labels.split('\\t')\n",
    "                \n",
    "            sentences.append(sentence)\n",
    "            mask_words.append(mask_word)\n",
    "                \n",
    "            one_labels = []\n",
    "            for la in label[1:]:\n",
    "                if la not in one_labels:\n",
    "                    la_id,la_word = la.split(':')\n",
    "                    one_labels.append(la_word)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "            mask_labels.append(one_labels)\n",
    "            \n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "def read_eval_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "    id = 0\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if is_label:\n",
    "                id += 1\n",
    "                if id==1:\n",
    "                    continue\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "                mask_word,labels = words.strip().split('\\t',1)\n",
    "                label = labels.split('\\t')\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "                \n",
    "                one_labels = []\n",
    "                for la in label:\n",
    "                    if la not in one_labels:\n",
    "                        one_labels.append(la)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "                mask_labels.append(one_labels)\n",
    "            else:\n",
    "                if not line:\n",
    "                    break\n",
    "                #print(line)\n",
    "                sentence,mask_word = line.strip().split('\\t')\n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "\n",
    "def candidate_generation(source_word, pre_tokens, pre_scores, ps, num_selection=10):\n",
    "\n",
    "    cur_tokens=[]\n",
    "   \n",
    "    source_stem = ps.stem(source_word)\n",
    "\n",
    "    assert num_selection<=len(pre_tokens)\n",
    "\n",
    "    for i in range(len(pre_tokens)):\n",
    "        token = pre_tokens[i]\n",
    "     \n",
    "        if token[0:2]==\"##\":\n",
    "            continue\n",
    "\n",
    "        if(token==source_word):\n",
    "            continue\n",
    "\n",
    "        token_stem = ps.stem(token)\n",
    "\n",
    "        if(token_stem == source_stem):\n",
    "            continue\n",
    "\n",
    "        if (len(token_stem)>=3) and (token_stem[:3]==source_stem[:3]):\n",
    "            continue\n",
    "\n",
    "        cur_tokens.append(token)\n",
    "        \n",
    "\n",
    "        if(len(cur_tokens)==num_selection):\n",
    "            break\n",
    "    \n",
    "    if(len(cur_tokens)==0):\n",
    "        cur_tokens = pre_tokens[0:num_selection+1]\n",
    "        \n",
    "\n",
    "    assert len(cur_tokens)>0       \n",
    "\n",
    "    return cur_tokens\n",
    "\n",
    "def get_score(sentence,tokenizer,maskedLM):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    tensor_input = tensor_input.to('cuda')\n",
    "    sentence_loss = 0\n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(tokenize_input):\n",
    "        orignial_word = tokenize_input[i]\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        #print(tokenize_input)\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        mask_input = mask_input.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            word_loss=maskedLM(mask_input,masked_lm_labels=tensor_input).data.cpu().numpy()\n",
    "        \n",
    "        print(word_loss)\n",
    "        sentence_loss += word_loss\n",
    "        tokenize_input[i] = orignial_word\n",
    "        \n",
    "    \n",
    "    \n",
    "    return np.exp(sentence_loss/len(tokenize_input))\n",
    "\n",
    "\n",
    "def LM_score(source_word,source_context,substitution_selection,tokenizer,maskedLM):\n",
    "    #source_index = source_context.index(source_word)\n",
    "\n",
    "    source_sentence = ''\n",
    "\n",
    "    for context in source_context:\n",
    "        source_sentence += context + \" \"\n",
    "    \n",
    "    source_sentence = source_sentence.strip()\n",
    "    #print(source_sentence)\n",
    "    LM = []\n",
    "\n",
    "    for substibution in substitution_selection:\n",
    "        \n",
    "        sub_sentence = source_sentence.replace(source_word,substibution)\n",
    "\n",
    "        \n",
    "        #print(sub_sentence)\n",
    "        score = get_score(sub_sentence,tokenizer,maskedLM)\n",
    "        LM.append(score)\n",
    "\n",
    "    return LM\n",
    "\n",
    "\n",
    "def preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count):\n",
    "    ss = []\n",
    "    ##ss_score=[]\n",
    "    sis_scores=[]\n",
    "    count_scores=[]\n",
    "    source_count = 10\n",
    "    if source_word in word_count:\n",
    "        source_count = word_count[source_word]\n",
    "\n",
    "    isFast = True\n",
    "\n",
    "    if(source_word not in fasttext_dico):\n",
    "        isFast = False\n",
    "    else:\n",
    "        source_emb = fasttext_emb[fasttext_dico.index(source_word)].reshape(1,-1)\n",
    "\n",
    "    #ss.append(source_word)\n",
    "\n",
    "    for sub in substitution_selection:\n",
    "\n",
    "        if sub not in word_count:\n",
    "            continue\n",
    "        else:\n",
    "            sub_count = word_count[sub]\n",
    "\n",
    "        #if sub_count<source_count:\n",
    "         #   continue\n",
    "        if isFast:\n",
    "            if sub not in fasttext_dico:\n",
    "                continue\n",
    "\n",
    "            token_index_fast = fasttext_dico.index(sub)\n",
    "            sis = cosine(source_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            #if sis<0.35:\n",
    "            #    continue\n",
    "            sis_scores.append(sis)\n",
    "\n",
    "        ss.append(sub)\n",
    "        count_scores.append(sub_count)\n",
    "\n",
    "    return ss,sis_scores,count_scores\n",
    "\n",
    "def compute_context_sis_score(source_word, sis_context, substitution_selection, fasttext_dico, fasttext_emb):\n",
    "    context_sis = []\n",
    "\n",
    "    word_context = []\n",
    "\n",
    "    \n",
    "\n",
    "    for con in sis_context:\n",
    "        if con==source_word or (con not in fasttext_dico):\n",
    "            continue\n",
    "\n",
    "        word_context.append(con)\n",
    "\n",
    "    if len(word_context)!=0:\n",
    "        for sub in substitution_selection:\n",
    "            sub_emb = fasttext_emb[fasttext_dico.index(sub)].reshape(1,-1)\n",
    "            all_sis = 0\n",
    "            for con in word_context:\n",
    "                token_index_fast = fasttext_dico.index(con)\n",
    "                all_sis += cosine(sub_emb, fasttext_emb[token_index_fast].reshape(1,-1))\n",
    "\n",
    "            context_sis.append(all_sis/len(word_context))\n",
    "    else:\n",
    "        for i in range(len(substitution_selection)):\n",
    "            context_sis.append(len(substitution_selection)-i)\n",
    "\n",
    "            \n",
    "    return context_sis\n",
    "\n",
    "\n",
    "def substitution_ranking(source_word, source_context, substitution_selection, fasttext_dico, fasttext_emb, word_count, tokenizer, maskedLM, lables):\n",
    "\n",
    "    ss,sis_scores,count_scores=preprocess_SR(source_word, substitution_selection, fasttext_dico, fasttext_emb, word_count)\n",
    "\n",
    "    #print(ss)\n",
    "    if len(ss)==0:\n",
    "        return source_word\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        seq = sorted(sis_scores,reverse = True )\n",
    "        sis_rank = [seq.index(v)+1 for v in sis_scores]\n",
    "    \n",
    "    rank_count = sorted(count_scores,reverse = True )\n",
    "    \n",
    "    count_rank = [rank_count.index(v)+1 for v in count_scores]\n",
    "  \n",
    "    lm_score = LM_score(source_word,source_context,ss,tokenizer,maskedLM)\n",
    "\n",
    "    rank_lm = sorted(lm_score)\n",
    "    lm_rank = [rank_lm.index(v)+1 for v in lm_score]\n",
    "    \n",
    "\n",
    "    bert_rank = []\n",
    "    for i in range(len(ss)):\n",
    "        bert_rank.append(i+1)\n",
    "\n",
    "    if len(sis_scores)>0:\n",
    "        all_ranks = [bert+sis+count+LM  for bert,sis,count,LM in zip(bert_rank,sis_rank,count_rank,lm_rank)]\n",
    "    else:\n",
    "        all_ranks = [bert+count+LM  for bert,count,LM in zip(bert_rank,count_rank,lm_rank)]\n",
    "    #all_ranks = [con for con in zip(context_rank)]\n",
    "\n",
    "\n",
    "    pre_index = all_ranks.index(min(all_ranks))\n",
    "    pre_word = ss[pre_index]\n",
    "\n",
    "    \n",
    "\n",
    "    return pre_word\n",
    "\n",
    "\n",
    "def evaulation_SS_scores(ss,labels):\n",
    "    assert len(ss)==len(labels)\n",
    "\n",
    "    potential = 0\n",
    "    instances = len(ss)\n",
    "    precision = 0\n",
    "    precision_all = 0\n",
    "    recall = 0\n",
    "    recall_all = 0\n",
    "\n",
    "    for i in range(len(ss)):\n",
    "\n",
    "        one_prec = 0\n",
    "        \n",
    "        common = list(set(ss[i]).intersection(labels[i]))\n",
    "\n",
    "        if len(common)>=1:\n",
    "            potential +=1\n",
    "        precision += len(common)\n",
    "        recall += len(common)\n",
    "        precision_all += len(ss[i])\n",
    "        recall_all += len(labels[i])\n",
    "\n",
    "    potential /=  instances\n",
    "    precision /= precision_all\n",
    "    recall /= recall_all\n",
    "    F_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return potential,precision,recall,F_score\n",
    "\n",
    "\n",
    "def evaulation_pipeline_scores(substitution_words,source_words,gold_words):\n",
    "\n",
    "    instances = len(substitution_words)\n",
    "    precision = 0\n",
    "    accuracy = 0\n",
    "    changed_proportion = 0\n",
    "\n",
    "    for sub, source, gold in zip(substitution_words,source_words,gold_words):\n",
    "        if sub==source or (sub in gold):\n",
    "            precision += 1\n",
    "        if sub!=source and (sub in gold):\n",
    "            accuracy += 1\n",
    "        if sub!=source:\n",
    "            changed_proportion += 1\n",
    "\n",
    "    return precision/instances,accuracy/instances,changed_proportion/instances\n",
    "\n",
    "\n",
    "def read_file(input_file):\n",
    "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            \n",
    "            sentences.append(line)\n",
    "           \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def extract_context(words, mask_index, window):\n",
    "    #extract 7 words around the content word\n",
    "\n",
    "    length = len(words)\n",
    "\n",
    "    half = int(window/2)\n",
    "\n",
    "    assert mask_index>=0 and mask_index<length\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    if length<=window:\n",
    "        context = words\n",
    "    elif mask_index<length-half and mask_index>=half:\n",
    "        context = words[mask_index-half:mask_index+half+1]\n",
    "    elif mask_index<half:\n",
    "        context = words[0:window]\n",
    "    elif mask_index>=length-half:\n",
    "        context = words[length-window:length]\n",
    "    else:\n",
    "        print(\"Wrong!\")\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--eval_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The evaluation data dir.\")\n",
    "parser.add_argument(\"--bert_model\", default=None, type=str,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "\n",
    "parser.add_argument(\"--output_SR_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The output directory of writing substitution selection.\")\n",
    "parser.add_argument(\"--word_embeddings\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word embeddings\")\n",
    "parser.add_argument(\"--word_frequency\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "    \n",
    "parser.add_argument(\"--ppdb\",\n",
    "                        default=\"./ppdb-2.0-tldr\",\n",
    "                        type=str,\n",
    "                        help=\"The path of word frequency.\")\n",
    "\n",
    "parser.add_argument(\"--prob_mask\",\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of the masked words in first sentence. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "\n",
    "\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--num_selections\",\n",
    "                        default=20,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--num_eval_epochs\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_eval=True\n",
    "args.do_lower_case=True\n",
    "args.num_selections=10\n",
    "\n",
    "args.prob_mask=0.0\n",
    "\n",
    "args.eval_dir='D:/data/bert_ppdb/datasets/lex.mturk.txt'\n",
    "args.bert_model='bert-large-uncased-whole-word-masking'\n",
    "args.max_seq_length=250\n",
    "args.word_embeddings='D:/data/bert_ppdb/crawl-300d-2M-subword.vec'\n",
    "args.word_frequency='D:/data/bert_ppdb/SUBTLEX_frequency.xlsx'\n",
    "args.ppdb='D:/data/bert_ppdb/ppdb-2.0-tldr'\n",
    "args.output_SR_file='D:/data/bert_ppdb/results/NNSeval'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.server_ip and args.server_port:\n",
    "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if  not args.do_eval:\n",
    "    raise ValueError(\"At least `do_eval` must be True.\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "    \n",
    "\n",
    "# Prepare model\n",
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "model = BertForMaskedLM.from_pretrained(args.bert_model,cache_dir=cache_dir)\n",
    "if args.fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "\n",
    "output_sr_file = open(args.output_SR_file,\"a+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Loading embeddings ...\")\n",
    "\n",
    "#wordVecPath = args.word_embeddings\n",
    "#wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\"\n",
    "\n",
    "#fasttext_dico, fasttext_emb = getWordmap(wordVecPath)\n",
    "\n",
    "#stopword = set(stopwords.words('english'))\n",
    "#word_count_path = args.word_frequency\n",
    "#word_count_path = \"word_frequency_wiki.txt\"\n",
    "#word_count = getWordCount(word_count_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "SS = []\n",
    "substitution_words = []\n",
    "source_words = []\n",
    "\n",
    "num_selection = args.num_selections\n",
    "bre_i=0\n",
    "\n",
    "window_context = 11\n",
    "\n",
    "if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "\n",
    "    fileName = args.eval_dir.split('/')[-1][:-4]\n",
    "    if fileName=='lex.mturk':\n",
    "        eval_examples, mask_words, mask_labels = read_eval_dataset(args.eval_dir)\n",
    "    else:\n",
    "        eval_examples, mask_words, mask_labels = read_eval_index_dataset(args.eval_dir)\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    #logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_size = len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(eval_size):\n",
    "\n",
    "    print('Sentence {} rankings: '.format(i))\n",
    "    #output_sr_file.write(str(i))\n",
    "    #output_sr_file.write(' sentence: ')\n",
    "    #output_sr_file.write('\\n')\n",
    "    tokens, position = convert_sentence_to_token(eval_examples[i], mask_words[i], args.max_seq_length, tokenizer)\n",
    "\n",
    "    print(tokens)\n",
    "    print(mask_words[i])\n",
    "\n",
    "    feature = convert_token_to_feature(tokens, args.max_seq_length, tokenizer)\n",
    "\n",
    "    tokens_tensor = torch.tensor([feature.input_ids])\n",
    "\n",
    "    token_type_ids = torch.tensor([feature.input_type_ids])\n",
    "\n",
    "    attention_mask = torch.tensor([feature.input_mask])\n",
    "\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    token_type_ids = token_type_ids.to('cuda')\n",
    "    attention_mask = attention_mask.to('cuda')\n",
    "\n",
    "                # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        prediction_scores = model(tokens_tensor, token_type_ids,attention_mask)\n",
    "\n",
    "\n",
    "    predicted_top = prediction_scores[0, position].topk(50)\n",
    "        #print(predicted_top[0].cpu().numpy())\n",
    "    pre_tokens = tokenizer.convert_ids_to_tokens(predicted_top[1].cpu().numpy())\n",
    "\n",
    "    ss = candidate_generation(mask_words[i], pre_tokens, predicted_top[0].cpu().numpy(), ps, num_selection)\n",
    "\n",
    "    print('ssss------')\n",
    "    print(ss)\n",
    "\n",
    "    SS.append(ss)\n",
    "    #break\n",
    "\n",
    "    #print(mask_words[i], \":\", ss)\n",
    "            \n",
    "\n",
    "    #if(bre_i==5):\n",
    "    #    break\n",
    "    #bre_i += 1\n",
    "#output_sr_file.close()\n",
    "\n",
    "potential,precision,recall,F_score=evaulation_SS_scores(SS, mask_labels)\n",
    "print(\"The score of evaluation for substitution selection\")\n",
    "\n",
    "print(potential,precision,recall,F_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
