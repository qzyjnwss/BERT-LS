{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForPreTraining\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering\n",
    "Bert_path = \"/home/qiang/Desktop/pytorch-pretrained-BERT/uncased_L_12_H_768_A_12/bert-base-uncased\"\n",
    "#Bert_path = \"/home/qiang/Desktop/pytorch-pretrained-BERT/wwm_uncased_L-24_H-1024_A-16\"\n",
    "config = BertConfig.from_json_file(Bert_path+\"/bert_config.json\")\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/qiang/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', '[MASK]', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dbcca6344bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmasked_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'who'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'henson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[SEP]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'puppet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'##eer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[SEP]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Convert token to vocabulary indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson [MASK] a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "text2 = \"2001 february -- britain , us-carry out 1-800-bethany raids to try -gt disable iraq s air defence network .\"\n",
    "\n",
    "tokenized_text2 = tokenizer.tokenize(text2)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "index = tokenized_text.index('[SEP]')\n",
    "\n",
    "for i in range(index+1, len(tokenized_text)-1):\n",
    "    print(tokenized_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/qiang/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_pretrained_bert.modeling:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/qiang/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 30522])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "print(predictions.size())\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([18.1892,  7.1109,  6.3592,  5.8626,  5.7051], device='cuda:0'), tensor([27227,  2040,  2002,  2008,  2370], device='cuda:0'))\n",
      "['henson', 'who', 'he', 'that', 'himself']\n",
      "a great deal of popularity in many sectors of the Arab world\n",
      "12.19252962251336\n",
      "a great deal of popularity in many parts of the Arab world\n",
      "10.029331883479344\n",
      "a great deal of popularity in many areas of the Arab world\n",
      "8.400345044815866\n",
      "a great deal of popularity in many dwelling of the Arab world\n",
      "18.078830251573144\n",
      "a great deal of popularity in many place of the Arab world\n",
      "15.444976824853324\n",
      "a great deal of popularity in many important of the Arab world\n",
      "19.891476378864724\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "predicted_top = predictions[0, masked_index].topk(5)\n",
    "print(predicted_top)\n",
    "pre_tokens = tokenizer.convert_ids_to_tokens(predicted_top[1].cpu().numpy())\n",
    "print(pre_tokens)\n",
    "\n",
    "def get_score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    tensor_input = tensor_input.to('cuda')\n",
    "    sentence_loss = 0\n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(tokenize_input):\n",
    "        orignial_word = tokenize_input[i]\n",
    "        tokenize_input[i] = '[MASK]'\n",
    "        #print(tokenize_input)\n",
    "        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "        mask_input = mask_input.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            word_loss=model(mask_input,masked_lm_labels=tensor_input).data.cpu().numpy()\n",
    "        sentence_loss += word_loss\n",
    "        tokenize_input[i] = orignial_word\n",
    "        \n",
    "    \n",
    "    \n",
    "    return np.exp(sentence_loss/len(tokenize_input))\n",
    "\n",
    "sentence1 = \"a great deal of popularity in many sectors of the Arab world\"\n",
    "word = 'sectors'\n",
    "sentence2 = sentence1.replace(word,'parts')\n",
    "sentence3 = sentence1.replace(word,'areas')\n",
    "sentence4 = sentence1.replace(word,'dwelling')\n",
    "sentence5 = sentence1.replace(word,'place')\n",
    "sentence6 = sentence1.replace(word,'important')\n",
    "print(sentence1)\n",
    "print(get_score(sentence1))\n",
    "print(sentence2)\n",
    "print(get_score(sentence2))\n",
    "print(sentence3)\n",
    "print(get_score(sentence3))\n",
    "print(sentence4)\n",
    "print(get_score(sentence4))\n",
    "print(sentence5)\n",
    "print(get_score(sentence5))\n",
    "print(sentence6)\n",
    "print(get_score(sentence6))\n",
    "\n",
    "print(int(7/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies may also take out strike insurance \"prior\" to an anticipated strike , to help offset the losses which the strike would cause.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "s = \"Companies may also take out strike insurance \\\"prior\\\" to an anticipated strike , to help offset the losses which the strike would cause.\"\n",
    "s = st.lemmatize(s)\n",
    "\n",
    "print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n",
      "2009\n",
      "rankings:\n",
      "{work}\n",
      "{collaborate,\n",
      "act}\n",
      "{cooperate,\n",
      "operate}\n",
      "{engage}\n"
     ]
    }
   ],
   "source": [
    "aaa = \"Sentence 2009 rankings: {work} {collaborate, act} {cooperate, operate} {engage}\"\n",
    "\n",
    "bb = aaa.split(' ')\n",
    "for b in bb:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordmap(wordVecPath):\n",
    "\twords=[]\n",
    "\tWe = []\n",
    "\tf = open(wordVecPath,'r')\n",
    "\tlines = f.readlines()\n",
    "\n",
    "\tfor (n,line) in enumerate(lines):\n",
    "\t\tif (n == 0) :\n",
    "\t\t\tprint(line)\n",
    "\t\t\tcontinue\n",
    "\t\tword, vect = line.rstrip().split(' ', 1)\n",
    "\t                \n",
    "\t\tvect = np.fromstring(vect, sep=' ')\n",
    "\t            \n",
    "\t\tWe.append(vect)\n",
    "\n",
    "\t\twords.append(word)\n",
    "\n",
    "\t\tif(n==max_vocab):\n",
    "\t\t\tbreak\n",
    "\tf.close()       \n",
    "\treturn (words, We)\n",
    "\n",
    "def getWordCount(word_count_path):\n",
    "\tword2count = {}\n",
    "\twith open(word_count_path, 'r') as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\tN = 0\n",
    "\t\tfor i in lines:\n",
    "\t\t\ti=i.strip()\n",
    "\t\t\tif(len(i) > 0):\n",
    "\t\t\t\ti=i.split()\n",
    "\t\t\t\tif(len(i) == 2):\n",
    "\t\t\t\t\tword2count[i[0]] = float(i[1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(i)\n",
    "\t    \n",
    "\treturn word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings ...\n",
      "the -0.071549 0.093459 0.023738 -0.090339 0.056123 0.32547 -0.39796 -0.092139 0.061181 -0.1895 0.13061 0.14349 0.011479 0.38158 0.5403 -0.14088 0.24315 0.23036 -0.55339 0.048154 0.45662 3.2338 0.020199 0.049019 -0.014132 0.076017 -0.11527 0.2006 -0.077657 0.24328 0.16368 -0.34118 -0.06607 0.10152 0.038232 -0.17668 -0.88153 -0.33895 -0.035481 -0.55095 -0.016899 -0.43982 0.039004 0.40447 -0.2588 0.64594 0.26641 0.28009 -0.024625 0.63302 -0.317 0.10271 0.30886 0.097792 -0.38227 0.086552 0.047075 0.23511 -0.32127 -0.28538 0.1667 -0.0049707 -0.62714 -0.24904 0.29713 0.14379 -0.12325 -0.058178 -0.001029 -0.082126 0.36935 -0.00058442 0.34286 0.28426 -0.068599 0.65747 -0.029087 0.16184 0.073672 -0.30343 0.095733 -0.5286 -0.22898 0.064079 0.015218 0.34921 -0.4396 -0.43983 0.77515 -0.87767 -0.087504 0.39598 0.62362 -0.26211 -0.30539 -0.022964 0.30567 0.06766 0.15383 -0.11211 -0.09154 0.082562 0.16897 -0.032952 -0.28775 -0.2232 -0.090426 1.2407 -0.18244 -0.0075219 -0.041388 -0.011083 0.078186 0.38511 0.23334 0.14414 -0.0009107 -0.26388 -0.20481 0.10099 0.14076 0.28834 -0.045429 0.37247 0.13645 -0.67457 0.22786 0.12599 0.029091 0.030428 -0.13028 0.19408 0.49014 -0.39121 -0.075952 0.074731 0.18902 -0.16922 -0.26019 -0.039771 -0.24153 0.10875 0.30434 0.036009 1.4264 0.12759 -0.073811 -0.20418 0.0080016 0.15381 0.20223 0.28274 0.096206 -0.33634 0.50983 0.32625 -0.26535 0.374 -0.30388 -0.40033 -0.04291 -0.067897 -0.29332 0.10978 -0.045365 0.23222 -0.31134 -0.28983 -0.66687 0.53097 0.19461 0.3667 0.26185 -0.65187 0.10266 0.11363 -0.12953 -0.68246 -0.18751 0.1476 1.0765 -0.22908 -0.0093435 -0.20651 -0.35225 -0.2672 -0.0034307 0.25906 0.21759 0.66158 0.1218 0.19957 -0.20303 0.34474 -0.24328 0.13139 -0.0088767 0.33617 0.030591 0.25577\n",
      "\n",
      "the size of voab in word2vec 399999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# read embeddings\n",
    "print(\"Loading embeddings ...\")\n",
    "#dico, emb = getWordmap(\"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/wikipedia/fastText/new.fasttext.vec\")\n",
    "#dico, emb = getWordmap(\"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\")\n",
    "\n",
    "#wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.840B.300d.txt\"\n",
    "wordVecPath = \"/media/qiang/ee63f41d-4004-44fe-bcfd-522df9f2eee8/glove.6B.200d.txt\"\n",
    "\n",
    "dico=[]\n",
    "emb = []\n",
    "\n",
    "f = open(wordVecPath,'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "for (n,line) in enumerate(lines):\n",
    "    if n==0:\n",
    "        print(line)\n",
    "        continue\n",
    "    word, vect = line.rstrip().split(' ', 1)\n",
    "    vect = np.fromstring(vect, sep=' ')\n",
    "    emb.append(vect)\n",
    "    dico.append(word)\n",
    "\n",
    "vocab = len(emb)\n",
    "\n",
    "print('the size of voab in word2vec ' +  str(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37879787]]\n",
      "[[0.53993423]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sis = 1-sp.distance.cdist(emb[index1].reshape(1,-1), emb[index2].reshape(1,-1), 'cosine')\n",
    "print(sis)\n",
    "\n",
    "sis = 1-sp.distance.cdist(emb[index1].reshape(1,-1), emb[index3].reshape(1,-1), 'cosine')\n",
    "print(sis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "here\n",
      "sits [[0.59903949]]\n",
      "atop [[0.6829604]]\n",
      "overlooking [[0.63796319]]\n",
      "perched [[1.]]\n",
      "nestled [[0.58518976]]\n",
      "precariously [[0.62324375]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "import scipy.spatial as sp\n",
    "\n",
    "word1 = \"perched\"\n",
    "\n",
    "\n",
    "index1 = dico.index(word1)\n",
    "\n",
    "res = []\n",
    "words = []\n",
    "id = 0\n",
    "for word2 in dico:\n",
    "    index2 = dico.index(word2)\n",
    "    #sis = cosine(emb[index1].reshape(1,-1), emb[index2].reshape(1,-1))\n",
    "    sis = 1-sp.distance.cdist(emb[index1].reshape(1,-1), emb[index2].reshape(1,-1), 'cosine')\n",
    "    \n",
    "    id += 1\n",
    "    if id%10000==0:\n",
    "        print(id)\n",
    "    if(sis>0.5):\n",
    "        res.append(sis)\n",
    "        words.append(word2)\n",
    "\n",
    "print(\"here\")\n",
    "seq = sorted(res,reverse = True )\n",
    "sis_rank = [seq.index(v)+1 for v in res]\n",
    "\n",
    "for ind in range(len(sis_rank)):\n",
    "    if sis_rank[ind]<=6:\n",
    "        print(words[ind],res[ind])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('storm', 'NN'), ('``', '``'), ('continued', 'VBN'), (\"''\", \"''\"), (',', ','), ('crossing', 'VBG'), ('the', 'DT'), ('outer', 'JJ'), ('banks', 'NNS'), ('of', 'IN'), ('north', 'JJ'), ('carolina', 'NN'), ('2015', 'CD'), ('nineteenth', 'JJ'), ('19th', 'CD'), (',', ','), ('and', 'CC'), ('retained', 'VBD'), ('its', 'PRP$'), ('strength', 'NN'), ('until', 'IN'), ('june', 'NN'), ('20', 'CD'), ('when', 'WRB'), ('it', 'PRP'), ('became', 'VBD'), ('extratropical', 'JJ'), ('near', 'IN'), ('newfoundland', 'NN'), ('.', '.')]\n",
      "['the', 'storm', '``', 'continued', \"''\", ',', 'crossing', 'the', 'outer', 'banks', 'of', 'north', 'carolina', '2015', 'nineteenth', '19th', ',', 'and', 'retained', 'its', 'strength', 'until', 'june', '20', 'when', 'it', 'became', 'extratropical', 'near', 'newfoundland', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(\"the storm \\\"continued\\\", crossing the outer banks of north carolina 2015 nineteenth 19th, and retained its strength until june 20 when it became extratropical near newfoundland.\t\")\n",
    "\n",
    "print(nltk.pos_tag(tokens))\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(word_count_path):\n",
    "\tword2count = {}\n",
    "\twith open(word_count_path, 'r') as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\tN = 0\n",
    "\t\tfor i in lines:\n",
    "\t\t\ti=i.strip()\n",
    "\t\t\tif(len(i) > 0):\n",
    "\t\t\t\ti=i.split()\n",
    "\t\t\t\tif(len(i) == 2):\n",
    "\t\t\t\t\tword2count[i[0]] = float(i[1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(i)\n",
    "\t    \n",
    "\treturn word2count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238.0\n",
      "1028.0\n",
      "no keepership\n",
      "no mahallas\n",
      "548.0\n"
     ]
    }
   ],
   "source": [
    "word_count_path = \"word_frequency_num.txt\"\n",
    "\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "\n",
    "if 'ether' in word_count:\n",
    "    print(word_count['ether'])\n",
    "else:\n",
    "    print(\"no ether\")\n",
    "    \n",
    "if 'hounds' in word_count:\n",
    "    print(word_count['hounds'])\n",
    "else:\n",
    "    print(\"no hounds\")\n",
    "    \n",
    "if 'keepership' in word_count:\n",
    "    print(word_count['keepership'])\n",
    "else:\n",
    "    print(\"no keepership\")\n",
    "    \n",
    "if 'mahallas' in word_count:\n",
    "    print(word_count['mahallas'])\n",
    "else:\n",
    "    print(\"no mahallas\")\n",
    "    \n",
    "if 'delimit' in word_count:\n",
    "    print(word_count['delimit'])\n",
    "else:\n",
    "    print(\"no delimit\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_path = \"word_frequency_children.txt\"\n",
    "\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "word_count_path2 = \"word_frequency_wiki_sent1200.txt\"\n",
    "\n",
    "word_count2 = getWordCount(word_count_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2512.0 98.0 25.632653061224488\n",
      "4907.0 1126.0 4.357904085257549\n",
      "0.24775353016688062\n"
     ]
    }
   ],
   "source": [
    "a = 'perils'\n",
    "if a in word_count:\n",
    "    print(word_count[a],word_count2[a],word_count[a]/word_count2[a])\n",
    "\n",
    "b= 'dangers'\n",
    "if b in word_count:\n",
    "    print(word_count[b],word_count2[b],word_count[b]/word_count2[b])\n",
    "    \n",
    "c= 'holder'\n",
    "if c in word_count:\n",
    "    print(word_count[c]/word_count2[c])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486.0\n",
      "3271.0\n"
     ]
    }
   ],
   "source": [
    "word_count_path = \"frequency_merge_wiki_child.txt\"\n",
    "\n",
    "word_count = getWordCount(word_count_path)\n",
    "\n",
    "\n",
    "if 'ethereal' in word_count:\n",
    "    print(word_count['ethereal'])\n",
    "\n",
    "    \n",
    "if 'strengthen' in word_count:\n",
    "    print(word_count['strengthen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['becomes']\n",
      "VBD\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('becomes')\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(['disappeared'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'cc': 3, 'b': 4.0, 'c': 3, 'aa': 1}\n"
     ]
    }
   ],
   "source": [
    "dica={'a':1,'b':2.0,'c':3}\n",
    "dicb ={'aa':1,'b':6,'cc':3}\n",
    "dic={}\n",
    "for key in dica:\n",
    "    if dicb.get(key):\n",
    "        dic[key]=(dica[key]+dicb[key])/2\n",
    "    else:\n",
    "        dic[key]=dica[key]\n",
    "        \n",
    "for key in dicb:\n",
    "    if dica.get(key):\n",
    "        pass\n",
    "    else:\n",
    "        dic[key]=dicb[key]\n",
    "    \n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'who', 'here', 'such', 'most', 'itself', 'below', \"you've\", 'between', 'both', \"haven't\", 'in', 'mustn', 'are', 'now', 'ma', 'doing', 'more', 'we', \"it's\", \"mightn't\", 'doesn', 'from', 'am', 'but', \"isn't\", 'when', 'or', 'on', 'did', 'were', \"you're\", 'been', 'do', 'further', 'shan', 'each', 'under', 'so', 'shouldn', 'an', 'hadn', 'before', \"won't\", 'them', 'y', 'which', 'myself', 'too', \"shan't\", 'was', 'themselves', 'couldn', \"you'd\", 'through', 'weren', 'how', 'these', 'because', 'few', \"shouldn't\", 'theirs', 'our', 'once', 'don', 'have', 'to', \"hasn't\", 'mightn', 'yourselves', 'can', 'o', 'me', 'than', 'her', 'all', 'does', 'yourself', 'as', 'off', 'didn', \"needn't\", 'above', 'has', \"aren't\", 'd', 'he', \"you'll\", 'himself', 'own', 'for', 'be', 'by', \"don't\", 'those', 'had', 'you', 'herself', \"hadn't\", 'having', 'only', 'aren', 'm', 'will', 'his', 'the', 'a', 'my', 'needn', 'being', \"wasn't\", \"should've\", 'ourselves', 'no', 'wouldn', 'won', 'll', 'they', 'whom', 'while', 'him', 'about', 'down', \"doesn't\", 'why', 'isn', 'there', 'then', 've', 'ain', 'some', 'just', 're', 'should', 'nor', \"couldn't\", 'against', 'and', \"weren't\", \"that'll\", 'hasn', 's', 'ours', 'any', 'their', 'with', 'haven', 'during', 'yours', 'other', \"she's\", 'i', 'again', 'where', 'if', 'is', 'of', 'after', \"mustn't\", 'wasn', 'what', 'she', 'it', 'up', 'at', \"didn't\", 'your', 'until', 'that', 'into', \"wouldn't\", 'its', 'hers', 'out', 'same', 't', 'over', 'very', 'this'}\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "if 'released' in stopwords:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ether\n",
      "invit\n",
      "invit\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "a = 'ethereal'\n",
    "b = 'invited'\n",
    "c = 'invites'\n",
    "\n",
    "print(ps.stem(a))\n",
    "print(ps.stem(b))\n",
    "print(ps.stem(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_eval_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "    id = 0\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            if is_label:\n",
    "                id += 1\n",
    "                if id==1:\n",
    "                    continue\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "                mask_word,labels = words.strip().split('\\t',1)\n",
    "                label = labels.split('\\t')\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "                \n",
    "                one_labels = []\n",
    "                for la in label:\n",
    "                    if la not in one_labels:\n",
    "                        one_labels.append(la)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "                mask_labels.append(one_labels)\n",
    "            else:\n",
    "                if not line:\n",
    "                    break\n",
    "                #print(line)\n",
    "                sentence,mask_word = line.strip().split('\\t')\n",
    "                sentences.append(sentence)\n",
    "                mask_words.append(mask_word)\n",
    "    return sentences,mask_words,mask_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset ...\")\n",
    "\n",
    "dataPath = \"/home/qiang/Desktop/pytorch-pretrained-BERT/datasets/lex.mturk.txt\"\n",
    "\n",
    "sentences, mask_words, mask_labels = read_eval_dataset(dataPath)\n",
    "print(sentences[0],mask_words[0],mask_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiometric dating is a technique used to date materials , usually based on a comparison between the observed abundance of a naturally occurring radioactive isotope and its decay products , using known decay rates .\n",
      "abundance\n",
      "['amount', 'plenty', 'surplus', 'wealth', 'supply', 'bounty', 'excess', 'plenitude', 'profusion', 'outpouring', 'portion', 'growth', 'heap', 'availability', 'quantity']\n",
      "929\n",
      "929\n"
     ]
    }
   ],
   "source": [
    "def read_eval_index_dataset(data_path, is_label=True):\n",
    "    sentences=[]\n",
    "    mask_words = []\n",
    "    mask_labels = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding='ISO-8859-1') as reader:\n",
    "        while True:\n",
    "            line = reader.readline()\n",
    "            \n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            sentence,words = line.strip().split('\\t',1)\n",
    "                #print(sentence)\n",
    "            mask_word,labels = words.strip().split('\\t',1)\n",
    "            label = labels.split('\\t')\n",
    "                \n",
    "            sentences.append(sentence)\n",
    "            mask_words.append(mask_word)\n",
    "                \n",
    "            one_labels = []\n",
    "            for la in label[1:]:\n",
    "                if la not in one_labels:\n",
    "                    la_id,la_word = la.split(':')\n",
    "                    one_labels.append(la_word)\n",
    "                \n",
    "                #print(mask_word, \" ---\",one_labels)\n",
    "                    \n",
    "            mask_labels.append(one_labels)\n",
    "            \n",
    "    return sentences,mask_words,mask_labels\n",
    "\n",
    "dataPath = \"/home/qiang/Desktop/pytorch-pretrained-BERT/datasets/BenchLS.txt\"\n",
    "\n",
    "sentences, mask_words, mask_labels = read_eval_index_dataset(dataPath)\n",
    "print(sentences[10])\n",
    "print(mask_words[10])\n",
    "print(mask_labels[10])\n",
    "print(len(mask_words))\n",
    "print(len(mask_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 4, 6, 9, 10, 11, 11]\n",
      "[1, 7, 5, 6, 4, 7, 3, 1]\n",
      "[7, 29, 23, 26, 16, 29, 11, 7]\n",
      "[2.3333333333333335, 9.666666666666666, 7.666666666666667, 8.666666666666666, 5.333333333333333, 9.666666666666666, 3.6666666666666665, 2.3333333333333335]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x = [3,11,9,10,6,11,4,3]\n",
    "seq = sorted(x)\n",
    "print(seq)\n",
    "sis_rank = [seq.index(v)+1 for v in x]\n",
    "print(sis_rank)\n",
    "sum = [y+dd+z for y,dd,z in zip(x, sis_rank,x)]\n",
    "print(sum)\n",
    "sum = [s/3 for s in sum]\n",
    "print(sum)\n",
    "print(sum.index(min(sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "gold = ['aaa','bb','cc','dd','ee']\n",
    "\n",
    "source = 'ff'\n",
    "\n",
    "sub = 'bb'\n",
    "\n",
    "if sub==source or (sub in gold):\n",
    "    print('yes')\n",
    "    \n",
    "if sub!=source or (sub in gold):\n",
    "    print('yes')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.512875466720187\n",
      "9.210335372488682\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def information(count, all_count):\n",
    "    return -math.log((count+1)/(all_count+1))\n",
    "\n",
    "print(information(20000,2000000000))\n",
    "\n",
    "print(information(200000,2000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 22, column 27 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-363-326104650e1e>\"\u001b[0m, line \u001b[1;32m5\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    tree = ET.parse(path)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.5/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1184\u001b[0m, in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.5/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m596\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 22, column 27\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "tree = ET.parse(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "path = './datasets/test-data/contexts.xml'\n",
    "\n",
    "output_path = './datasets/test-data/contexts.txt'\n",
    "output_sr = open(output_path,\"w\")\n",
    "\n",
    "with open(path) as fp:\n",
    "    soup = BeautifulSoup(fp,'xml')\n",
    "    \n",
    "ss_sentences = soup.find_all('context')\n",
    "ss_mask = soup.find_all('head')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 300 sen: \n",
      "the 301 sen: \n",
      "the 302 sen: \n",
      "the 303 sen: \n",
      "the 304 sen: \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('the {} sen: '.format(301+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a= 'On Sunday at Craven Cottage , Jose Mourinho and his all stars exhibited all of the above symptoms and they were made to pay the price by a Fulham side that had in previous weeks woken up after matches with their heads kicked in .\\tside'\n",
    "b='www'\n",
    "print(len(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_ngram_downloader import readline_google_store\n",
    "\n",
    "fname, url, records = next(readline_google_store(ngram_len=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4838, 0.3299, 0.0679, 0.8502, 3.1353], [-0.4047, -0.4485, 0.03, -0.7441, 0.722], [0.0047, -0.9409, 0.1461, 0.3312, 0.7145]]\n",
      "tensor([0, 1, 1])\n",
      "tensor(1.7280, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "input1 = [[ 1.4838,  0.3299,  0.0679,  0.8502,  3.1353],\n",
    "        [-0.4047, -0.4485,  0.0300, -0.7441,  0.7220],\n",
    "        [ 0.0047, -0.9409,  0.1461,  0.3312,  0.7145]];\n",
    "\n",
    "input1 = torch.Tensor(input1)\n",
    "print(input1)\n",
    "target = [0,1,1]\n",
    "target = torch.Tensor(target)\n",
    "\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "\n",
    "print(output)\n",
    "\n",
    "output.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Record(ngram=\"0'9\", year=1797, match_count=1, volume_count=1)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding : utf-8 -*-\n",
    "# coding: utf-8\n",
    "import codecs\n",
    "\n",
    "filename = '/home/qiang/Downloads/data'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-521-56d131600eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_absolute_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'addicted to smoking'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-520-052ca346fa23>\u001b[0m in \u001b[0;36mprint_absolute_counts\u001b[0;34m(token, corpus, smoothing, start_year, end_year)\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mabsolute_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_absolute_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Absolute Counts for: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsolute_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-520-052ca346fa23>\u001b[0m in \u001b[0;36mretrieve_absolute_counts\u001b[0;34m(token, corpus, smoothing, start_year, end_year)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Load the data from the page.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Find the places in the html where the data starts and ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urllib2' is not defined"
     ]
    }
   ],
   "source": [
    "print_absolute_counts('addicted to smoking', 'english', smoothing=0, start_year=1900, end_year=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
